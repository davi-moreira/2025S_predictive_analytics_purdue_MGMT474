[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description",
    "text": "Course Description\nXXX This course is designed to introduce students to basic data analysis techniques. Coverage will include the application of probability distributions such as the normal, the \\(t\\), and the binomial; sampling distributions, basic statistical inference methods, analysis of variance, applied linear regression techniques, logistic regression time series analysis, statistical quality control, and decision analysis.\nCourse Website: https://davi-moreira.github.io/2025F_business_statistics_purdue_MGMT305\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#objective",
    "href": "index.html#objective",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Objective",
    "text": "Objective\nOur main goal is to instill the basic quantitative and data analysis skills needed by managers in modern business, where Business Analytics and Data Science have become important. These skills will help managers to understand and carry out data-based decision making, risk assessments, and policy making and to work effectively with data analysis teams to improve all aspects of business performance.\nModern data analysis is done using computers and various types of sophisticated software. Therefore, to most effectively introduce management candidates to its techniques, we will also emphasize software applications (with Microsoft Excel, R, or Python) in this course.\n\nCourse Materials\nThe following are learning materials:\n\nText book: Anderson, Sweeney, Williams, Camn, Cochran, Fry, and Ohlmann: Modern Business Statistics with Microsoft Excel, 7th edition, 2018, Cengage Learning, Inc. (Required) This text is also used in STAT 30301.\nHandouts: Lecture Slides and Supplementary Materials. (Posted in this page and on Brightspace: Brightspace -&gt; Content -&gt; Table of Contents -&gt; ….)\n\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.\nSoftware: Microsoft Excel will be used for in-class demonstrations and instruction. The main tool is Data Analysis under Tools. If you don’t see this tool, follow the following steps to add in:\n\nFile -&gt; Options -&gt; Add-ins -&gt; Select Analysis ToolPak and Analysis ToolPak-VBA (also select StatTools 7.5, if available) -&gt; Go -&gt; Data -&gt; Data Analysis (to conduct analyses)\n\nCourse Website: This class website will be used throughout the course, but it does not replace the Course Brightspace Page.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThe course is designed to introduce students to basic data analysis techniques. Coverage will include the application of probability distributions such as the normal, the \\(t\\), and the binomial; sampling distributions, basic statistical inference methods, analysis of variance, applied linear regression techniques, logistic regression time series analysis, statistical quality control, and decision analysis.\nCourse Website: https://davi-moreira.github.io/2025S_business_statistics_purdue_MGMT305",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives",
    "href": "syllabus.html#objectives",
    "title": "Syllabus",
    "section": "Objectives",
    "text": "Objectives\nOur main goal is to instill the basic quantitative and data analysis skills needed by managers in modern business, where Business Analytics and Data Science have become important. These skills will help managers to understand and carry out data-based decision making, risk assessments, and policy making and to work effectively with data analysis teams to improve all aspects of business performance.\nModern data analysis is done using computers and various types of sophisticated software. Therefore, to most effectively introduce management candidates to its techniques, we will also emphasize software applications (with Microsoft Excel, R, or Python) in this course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): [ISLP] James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-infrastructure",
    "href": "syllabus.html#course-infrastructure",
    "title": "Syllabus",
    "section": "Course Infrastructure",
    "text": "Course Infrastructure\n\nBrightspace: The Course Brightspace Page (https://purdue.brightspace.com/) should be checked on a regular basis for announcements and course material.\nSoftware: Microsoft Excel will be used for in-class demonstrations and instruction. The main tool is Data Analysis under Tools. If you don’t see this tool, follow these steps to add it in:\n\nFile &gt; Options &gt; Add-ins &gt; Select Analysis ToolPak and Analysis ToolPak-VBA (also select StatTools 7.5, if available) &gt; Go &gt; Data &gt; Data Analysis (to conduct analyses)\n\nCourse Website: This class website will be used throughout the course, but it does not replace the Course Brightspace Page.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n20%\n\n\nHomework\n30%\n\n\nFinal Project\n40%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (40%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPoster Presentation at the Purdue Undergraduate Research Conference (40%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#keys-to-success",
    "href": "syllabus.html#keys-to-success",
    "title": "Syllabus",
    "section": "Keys to Success",
    "text": "Keys to Success\n\nConsistent Effort: Follow the schedule diligently.\nPre-Class Preparation: Review materials and exercises beforehand.\nClass Materials: Ensure accessibility of readings and notes.\nEngagement: Regularly practice problems and seek clarifications.\nActive Learning: Actively engage with materials and presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#ai-policy",
    "href": "syllabus.html#ai-policy",
    "title": "Syllabus",
    "section": "AI Policy",
    "text": "AI Policy\nStudents are encouraged to use AI tools for learning enhancement but must adhere to the following:\n\nAI tools are prohibited during exams.\nRefine prompts for effective outputs.\nUse AI for learning, not solution generation.\nCite any use of AI tools in submissions.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#additional-information",
    "href": "syllabus.html#additional-information",
    "title": "Syllabus",
    "section": "Additional Information",
    "text": "Additional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "material.html",
    "href": "material.html",
    "title": "Materials",
    "section": "",
    "text": "Topic\nBook Sections*\nSlides**\nData\nSupplementary Materials\n\n\n\n\nBasic Stat. & Prob. Rvw. 01\n1.2, 1.4, 1.5\nslides\ndata\n- Video: The NBA Data Scientist- Video: Hans Rosling’s 200 Countries, 200 Years\n\n\nBasic Stat. & Prob. Rvw. 02\n1.8, 3.3 (omit Chebyshev), 3.4, 3.5\nslides\ndata\n- Video: Data Basics: Observations, Variable, and Data Matrices- Video: Summarizing and Graphing Numerical Data- Video: Exploring Categorical Data\n\n\nBasic Stat. & Prob. Rvw. 03\n3.5, 6.2, t Dist. Supplement\nslides\n.\n- Video: Normal Distribution\n\n\nInt. Est. Rvw.\n8.1, 8.2, 8.3, 8.4\nslides\n.\n- Video: Point Estimates and Sampling Variability- Video: Confidence Intervals- Video: Inferences for Proportions- Video: t-distribution- Video: Inference for one mean\n\n\nHyp. Testing Rvw. 01\n9.1, 9.2, 9.4, 9.5\nslides\n.\n- Video: Hypothesis Testing Fundamentals\n\n\nHyp. Testing Rvw. 02\n11.1, 11.2\nslides\ndata\n- Nature: Statisticians issue warning over misuse of P-values\n\n\nAnalysis of Variance\n13.1, 13.2\nslides\ndata\n- Video: ANOVA Introduction- Video: Conditions for ANOVA- Video: Multiple comparisons\n\n\nSimple Regression\n14.1, 14.2, 14.3, 14.4, 14.5, 14.6, 14.7, 14.8, 14.9\nslides\ndata\n- Video: Line Fitting, Residuals, and Correlation- Video: Fitting a Line with Least Squares Regression- Video: Least Squares Regression- Video: Types of Outliers in Linear Regression- Video: Inference for Linear Regression\n\n\nMultiple Regression\n15.1, 15.2, 15.3, 15.4, 15.5, 15.6, 15.7, 15.8\nslides\ndata\n- Video: Introduction to Multiple Regression- Video: Model Selection in Multiple Regression- Video: Checking Multiple Regression Diagnostics Using Graphs\n\n\nModel Building\n16.1, 16.2, 16.3, 16.4\nslides\ndata\n- Video: Model Selection in Multiple Regression\n\n\nLogistic Regression\nSupplement in Brightspace and Slides\nslides\n.\n- Video: Basic Ideas of Logistic Regression\n\n\nTime Series\n17.1, 17.2, 17.3, 17.4, 17.5, 17.6\nslides\ndata\n- Video: Time Series Forecast Using Forecast Sheet in Excel\n\n\nQuality Control\n19.1, 19.2, 19.3\nslides\n.\n.\n\n\nDecision Analysis\n20.1, 20.2, 20.3, 20.4\nslides\n.\n- Video: Bayes theorem, the geometry of changing beliefs\n\n\n\n\n\n* Section Numbers refer to sections in the course textbook.\n** Course material adapted from the textbook and previous course editions to better fit our curriculum. Thanks to Professor Jen Tang for guidance and for generously sharing the materials.",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings [ISLP]\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidesbook lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidesbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 8\nSupport Vector Machines\nCh. 9\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 09\nUnsupervised Learning\nCh. 12\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 10\nFinal Project\n.\n.\n.\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nDeep Learning\nCh. 10\nslidesbook lab\n- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP- Video: TBP\n\n\nWeek 14\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\n\n* The course slides and labs are based on the [ISLP] book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#overview",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\nUnderstand basic statistical concepts\nApply statistical methods to business problems\n\n\n\n\nKey Topics\n\nDescriptive Statistics\nInferential Statistics\nExcel4stats"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructor",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructor",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructors-passions",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructors-passions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructors-passions-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#instructors-passions-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#students",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#students",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Students",
    "text": "Students\n\n\nIt is your turn! - 5 minutes\n\n\n\nPresent yourself to your left/right colleague and tell her/him what are the current two main passions in your life."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#course-overview-and-logistics-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\nMaterials:\n\nBrightspace\nCourse Webpage\n\nSyllabus\nSchedule"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#survey-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#survey-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Survey",
    "text": "Survey\n\n\nAttendance and Participation10 min"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#what-is-statistics-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#what-is-statistics-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What is Statistics?",
    "text": "What is Statistics?\n\n\n\n\n\n\n\n“Without data, you’re just another person with an opinion.” – W. Edwards Deming\n\n\n\n\n\n\n\n\n\n\n\nW. Edwards Deming\nWiki"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#what-is-statistics-2",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#what-is-statistics-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What is Statistics?",
    "text": "What is Statistics?\n\nStatistics can be defined as the science of collecting, analyzing, interpreting, presenting, and organizing data to make informed decisions."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#where-is-statistics-applied-in-business-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#where-is-statistics-applied-in-business-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Where is statistics applied in Business?",
    "text": "Where is statistics applied in Business?\n\nAccounting\n\nTo compare actual financial performance with budgeted amounts, identifying areas of inefficiency or unexpected costs.\n\nEconomics\n\nTo analyze employment trends, wage distributions, and the impact of income on business opportunities, helping managers make informed decisions.\n\nFinance\n\nRisk analysts to assess the probability of default on loans or bonds, aiding in the pricing of financial instruments and risk management strategies."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-sources-and-scales-of-measurement",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-sources-and-scales-of-measurement",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Data Sources and Scales of Measurement",
    "text": "Data Sources and Scales of Measurement\n\n\nElements are the entities on which data are collected.\nA variable is a characteristic of interest for the elements.\nThe set of measurements obtained for a particular element is called an observation.\nA data set with \\(n\\) elements contains \\(n\\) observations.\nThe total number of data values in a complete data set is the number of elements multiplied by the number of variables."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-types",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-types",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Data Types",
    "text": "Data Types\n\n\n\nCategorical Data\n\nLabels or names used to identify an attribute of each element\nOften referred to as qualitative data\nUse either the nominal or ordinal scale of measurement\nCan be either numeric or nonnumeric\nAppropriate statistical analysis is rather limited\n\n\n\n\nQuantitative Data\n\nQuantitative data indicate how many or how much:\n-  discrete, if measuring how many\n- continuous, if measuring how much\nQuantitative data are always numeric.\nOrdinary arithmetic operations (+, -, ×, ÷) are meaningful for quantitative data"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-types-examples",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-types-examples",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Data Types: Examples",
    "text": "Data Types: Examples\n\n\n\n\n\n\n\n\n\n\nCategorical: Nominal\nCategorical: Ordinal\nQuantitative: Continuous or Discrete\n\n\n\n\nVehicle Type\nSatisfaction Level\nTemperature\n\n\nBeverage\nEducation Level\nNumber of Transactions\n\n\nMusic Genre\nCustomer Feedback\nRevenue\n\n\nNationality\nJob Position\nProduct Weight\n\n\nRelationship Status\nMilitary Rank\nDistance Traveled\n\n\nOperating System\nPriority Level\nMarket Share\n\n\n\n\n\n\nCategorical: Nominal refers to variables that categorize data without a specific order.\nCategorical: Ordinal refers to variables that categorize data with a meaningful order but without a consistent difference between categories.\nQuantitative: Continuous or Discrete refers to variables that are numerical, where discrete variables are countable and continuous variables can take any value within a range."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#scales-of-measurement",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#scales-of-measurement",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scales of Measurement",
    "text": "Scales of Measurement"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#scales-of-measurement-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#scales-of-measurement-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scales of Measurement",
    "text": "Scales of Measurement\n\n\n\n\n\nCategorical Data\n\nDefinition: Data that can be sorted into categories or groups.\nSubcategories:\n\nNumeric: Categorical data represented by numbers, but the numbers do not have inherent numerical value.\n\nNominal: Categories that have no inherent order. Examples include gender, type of car, and nationality.\nOrdinal: Categories that have a specific order or ranking. Examples include satisfaction levels (e.g., satisfied, neutral, dissatisfied) and education levels (e.g., high school, bachelor’s, master’s).\n\nNon-numeric: Categorical data not represented by numbers.\n\nNominal: Similar to numeric nominal data but represented with non-numeric labels. Examples include types of cuisine (e.g., Italian, Chinese, Indian).\nOrdinal: Similar to numeric ordinal data but represented with non-numeric labels. Examples include rankings such as job positions (e.g., intern, junior, senior).\n\n\n\n\n\n\n\n\n\nQuantitative Data\n\nDefinition: Data that can be measured and expressed numerically.\nSubcategories:\n\nNumeric: Quantitative data always represented by numbers.\n\nInterval: Numerical data with meaningful differences between values but no true zero point. Examples include temperature in Celsius or Fahrenheit.\nRatio: Numerical data with meaningful differences between values and a true zero point. Examples include height, weight, and sales figures."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#cross-sectional-data-time-series",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#cross-sectional-data-time-series",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Cross-Sectional Data: Time Series",
    "text": "Cross-Sectional Data: Time Series"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Study Design",
    "text": "Study Design\n\n\nObservational\nIn observational studies, no attempt is made to control or influence the variables of interest. A survey is a good example.\n  \n\nAn example of an observational study is researchers observing a randomly selected group of customers that enter a Walmart Supercenter to collect data on variables such as time spent in the store, gender of the customer, and the amount spent.\n\n\nExperimental (Chapter 13)\nIn experimental studies, the variable of interest is first identified. Then, the values or levels (categories) of the variable are identified and controlled so that data from the experimental units (subjects) can be obtained about how they influence the variable of interest. Can have multiple variables.\n\nThe largest experimental study ever conducted is believed to be the 1954 Public Health Service experiment for the Salk polio vaccine. Nearly two million U.S. children (grades 1 through 3) were selected."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-random-assignment-vs.-random-sampling",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-random-assignment-vs.-random-sampling",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Study Design: Random Assignment vs. Random Sampling",
    "text": "Study Design: Random Assignment vs. Random Sampling"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-2",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#study-design-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Study Design",
    "text": "Study Design"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summarizing and Presenting Data",
    "text": "Summarizing and Presenting Data\n\n\n\n\nJune 9th Apple CEO Steve Jobs - Post"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summarizing and Presenting Data",
    "text": "Summarizing and Presenting Data\n\n\n\nProblems with pie charts - Post"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data-2",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summarizing-and-presenting-data-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summarizing and Presenting Data",
    "text": "Summarizing and Presenting Data"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#example-hudson-auto-repair",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#example-hudson-auto-repair",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Hudson Auto Repair",
    "text": "Example: Hudson Auto Repair\nThe manager of Hudson Auto would like to have a better understanding of the cost of parts used in the engine tune-ups performed in her shop. She examines 50 customer invoices for tune-ups. The costs of parts, rounded to the nearest dollar, are listed on the next slide."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#tabular-summary-frequency-and-percent-frequency---example",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#tabular-summary-frequency-and-percent-frequency---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tabular Summary: Frequency and Percent Frequency - Example",
    "text": "Tabular Summary: Frequency and Percent Frequency - Example\n\n\n\n\nParts Cost ($)\nFrequency\nPercent Frequency\n\n\n\n\n50 to 59\n2\n4%\n\n\n60 to 69\n13\n26%\n\n\n70 to 79\n16\n32%\n\n\n80 to 89\n7\n14%\n\n\n90 to 99\n7\n14%\n\n\n100 to 109\n5\n10%\n\n\nTOTAL\n50\n100%"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Graphical Summary: Bar Chart or Histogram - Example",
    "text": "Graphical Summary: Bar Chart or Histogram - Example\n\n\n\n\n\n\nhudson_auto_repair.xlsx\n\n\nFile -&gt; Options -&gt; Add-ins -&gt; Select Analysis ToolPak and Analysis ToolPak-VBA (also select StatTools 7.5, if available) -&gt; Go -&gt; Data -&gt; Data Analysis (to conduct analyses)"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Graphical Summary: Bar Chart or Histogram - Example",
    "text": "Graphical Summary: Bar Chart or Histogram - Example\n \n\n\n\n\nOpen Data Analysis Add-in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelect Histogram"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example-2",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#graphical-summary-bar-chart-or-histogram---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Graphical Summary: Bar Chart or Histogram - Example",
    "text": "Graphical Summary: Bar Chart or Histogram - Example\n\n\n\n\nInput the ranges (data, bin limits, output cell)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResult"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Numerical Descriptive Statistics",
    "text": "Numerical Descriptive Statistics\n\n\nThe most common numerical descriptive statistic is the mean (or average).\nThe mean demonstrates a measure of the central tendency, central location, or center of mass of the data for a variable.\nHudson’s mean cost of parts, based on the 50 tune-ups studied, is $79 (found by summing up the 50 cost values and then dividing by 50).\nThere are other descriptive statistics (next chapter)."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics---example",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Numerical Descriptive Statistics - Example",
    "text": "Numerical Descriptive Statistics - Example\n\n\n\n\nOpen Data Analysis Add-in and select the Descriptive Statistics analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput the ranges (data, bin limits, output cell) and options"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics---example-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#numerical-descriptive-statistics---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Numerical Descriptive Statistics - Example",
    "text": "Numerical Descriptive Statistics - Example\n\n\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s check the result\n\n\n\n\n\n\n\n\n\n\n\nFor percentiles: = percentile.exc(Data Array, %)"
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#statistical-inference-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#statistical-inference-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation: the set of all elements of interest in a particular study.\nSample: a subset of the population.\nDescriptive Statistics: Tabular, graphical, and numerical summaries of data.\nInferential Statistics: The process of using data from the sample to make estimates or test hypotheses about the characteristics of a population\nEstimation: Using sample data to approximate population parameters.\nHypotheses Testing: Determining if there is enough evidence in a sample to support a claim about a population.\nPrediction: Forecasting future events based on historical data."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-science-big-data-and-data-mining---definitions",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#data-science-big-data-and-data-mining---definitions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Data Science, Big Data, and Data Mining - Definitions",
    "text": "Data Science, Big Data, and Data Mining - Definitions\n\n\nData Science:\n\nThe interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n\nBig Data:\n\nExtremely large datasets that may be analyzed computationally to reveal patterns, trends, and associations, especially relating to human behavior and interactions.\n\nData Mining:\n\nThe practice of examining large databases to generate new information, involving methods at the intersection of machine learning, statistics, and database systems."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#how-data-science-big-data-and-data-mining-are-used",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#how-data-science-big-data-and-data-mining-are-used",
    "title": " MGMT 30500: Business Statistics ",
    "section": "How Data Science, Big Data, and Data Mining are Used",
    "text": "How Data Science, Big Data, and Data Mining are Used\n\n\n\nData Science:\n\nPersonalizing marketing efforts by analyzing customer data to predict preferences and buying behavior.\nOptimizing supply chain management through predictive analytics.\n\nBig Data:\n\nAnalyzing customer feedback and social media interactions to improve customer service and develop new products.\nEnhancing risk management in financial institutions by monitoring transaction patterns and detecting fraudulent activities.\n\nData Mining:\n\nIdentifying potential leads and sales opportunities by analyzing past sales data and customer demographics.\nEnhancing customer retention by understanding churn patterns and developing targeted retention strategies."
  },
  {
    "objectID": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summary-1",
    "href": "lecture_slides/01_chapter_data_statistics/01_chapter_data_statistics.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nCourse Overview:\n\nMaterials and resources available on Brightspace and the course webpage.\nYou are welcome to join virtual office hours!\nIf you need an individual appointment, e-mail me!\n\nKey Concepts:\n\nImportance of statistics in business decision-making.\nStudy Design.\nData summary and visualization good practices.\nDistinction between Descriptive and Inferential Statistics.\nData Science, Big Data, and Data Mining"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#overview",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nDescriptive Statistics\nMeasures of Central Location and Variability\nDistribution Shape\n\nSkewness\nSymmetry\n\nRelative Location and z-Scores\n\nCalculation and Interpretation\nExamples\n\nEmpirical Rule\n\n68-95-99.7 Rule\nDetecting Outliers\n\n\n\n\nFive-Number Summaries and Boxplots\nMeasures of Association Between Two Variables\n\nCovariance\nCorrelation"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#central-location",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#central-location",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Central Location",
    "text": "Central Location\n\n\n\n\n\n\n\n\nStatistic\nDefinition\nFormula\n\n\n\n\nMean\nThe average of all values of a variable\n\\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n\nMode\nThe most frequently occurring value\n\n\n\nkth Percentile\nRoughly k% of the data is at or below this value\n\n\n\nQuartile\nThe first, second, and third quartiles are 25th, 50th, and 75th percentiles\n\\(Q1, Q2, Q3\\)\n\n\nMedian\nThe “middle” observation when the data are listed from smallest to largest\n\\(Q2\\)\n\n\nMaximum\nThe largest value\n\n\n\nMinimum\nThe smallest value\n\n\n\nMidrange\nThe middle of the maximum and minimum\n\\(\\frac{Max + Min}{2}\\)\n\n\nMidhinge\nThe middle of the first and third quartiles\n\\(\\frac{Q3 + Q1}{2}\\)"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#variability-sampling-variation",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#variability-sampling-variation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Variability (Sampling Variation)",
    "text": "Variability (Sampling Variation)\n\nSample Variance: “Average” squared deviation of observations from the mean of all observations (\\(n-1\\) is called the degrees of freedom, df):\n\\[\nS^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n - 1}\n\\]\n\nWhy do we compute the sample variance using \\(n-1\\) instead of \\(n\\)?\nTo not underestimate the True Population Variance \\(\\sigma^2\\)\nWhat is an unbiased estimator? Video\nVideo and Simulation\n\nSample Standard Deviation:\n\\[\nS = \\sqrt{S^2}\n\\]\nRange = Maximum – Minimum.\nInterquartile Range (IQR) = 3rd Quartile – 1st Quartile\n(Range of the middle 50% of data.)"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\nAn important measure of the shape of a distribution is called skewness.\nThe formula for the skewness of sample data is:\n\\[\n\\text{Skewness} = \\frac{n}{(n - 1)(n - 2)} \\sum \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3\n\\]\nSkewness can be easily computed using statistical software."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nSymmetric (not skewed)\n\nSkewness is zero.\nMean and median are equal."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nModerately Skewed Left\n\nSkewness is negative.\nMean will usually be less than the median."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-3",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nModerately Skewed Right\n\nSkewness is positive.\nMean will usually be more than the median."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-4",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#distribution-shape-skewness-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nHighly Skewed Right\n\nSkewness is positive (often above 1.0).\nMean will usually be more than the median."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores",
    "title": " MGMT 30500: Business Statistics ",
    "section": "z-Scores",
    "text": "z-Scores\n\nThe z-score is often called the standardized value.\nIt describes the relative location of a data value relative to the mean.\nIt denotes the number of standard deviations a data value \\(x_i\\) is from the mean.\n\\[\nZ_i = \\frac{x_i - \\bar{x}}{s}\n\\]\n=STANDARDIZE(x, mean, standard deviation)"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "z-Scores",
    "text": "z-Scores\n\nAn observation’s z-score is a measure of the relative location of the observation in a data set.\nA data value less than the sample mean will have a z-score less than zero.\nA data value greater than the sample mean will have a z-score greater than zero.\nA data value equal to the sample mean will have a z-score of zero."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#z-scores-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "z-Scores",
    "text": "z-Scores\n\nExample: class_size_data.xlsx\n\\[\nZ_i = \\frac{x_i - \\bar{x}}{s}\n\\]\n\n\n\n\n\n\n\n\nNumber of students in class\nDeviation about the Mean\nZ score\n\n\n\n\n46\n2\n\\(\\frac{2}{8} = 0.25\\)\n\n\n54\n10\n\\(\\frac{10}{8} = 1.25\\)\n\n\n42\n-2\n\\(\\frac{-2}{8} = -0.25\\)\n\n\n46\n2\n\\(\\frac{2}{8} = 0.25\\)\n\n\n32\n-12\n\\(\\frac{-12}{8} = -1.5\\)\n\n\n\n\n\n\nNote: \\(\\bar{x} = 44\\) and \\(s = 8\\) for the given data."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Empirical Rule - 68-95-99.7 Rule",
    "text": "Empirical Rule - 68-95-99.7 Rule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the data are believed to approximate a bell-shaped (normal) distribution:\n\nThe empirical rule can be used to determine the percentage of data values that must be within a specified number of standard deviations of the mean."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Empirical Rule - 68-95-99.7 Rule",
    "text": "Empirical Rule - 68-95-99.7 Rule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor nearly normally distributed data,\n\nabout 68% falls within 1 SD of the mean,\nabout 95% falls within 2 SD of the mean,\nabout 99.7% falls within 3 SD of the mean.\n\nIt is possible for observations to fall 4, 5, or more standard deviations away from the mean, but these occurrences are very rare if the data are nearly normal."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule---example",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#empirical-rule---68-95-99.7-rule---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Empirical Rule - 68-95-99.7 Rule - Example",
    "text": "Empirical Rule - 68-95-99.7 Rule - Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSAT scores are distributed nearly normally with mean 1500 and standard deviation 300.\n\n~68% of students score between 1200 and 1800 on the SAT.\n~95% of students score between 900 and 2100 on the SAT.\n~$99.7% of students score between 600 and 2400 on the SAT."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#detecting-outliers-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#detecting-outliers-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Outliers",
    "text": "Detecting Outliers\n\nAn outlier is an unusually small or unusually large value in a data set.\nA data value with a z-score less than −3 or greater than +3 might be considered an outlier.\nIt might be:\n\nan incorrectly recorded data value\na data value that was incorrectly included in the data set\na correctly recorded unusual data value that belongs in the data set"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#detecting-outliers---example",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#detecting-outliers---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Outliers - Example",
    "text": "Detecting Outliers - Example\nExample: class_size_data.xlsx\n\\[\nZ_i = \\frac{x_i - \\bar{x}}{s}\n\\]\n\n\n\n\n\n\n\n\nNumber of students in class\nDeviation about the Mean\nZ score\n\n\n\n\n46\n2\n\\(\\frac{2}{8} = 0.25\\)\n\n\n54\n10\n\\(\\frac{10}{8} = 1.25\\)\n\n\n42\n-2\n\\(\\frac{-2}{8} = -0.25\\)\n\n\n46\n2\n\\(\\frac{2}{8} = 0.25\\)\n\n\n32\n-12\n\\(\\frac{-12}{8} = -1.5\\)\n\n\n\n\nNote: \\(-1.5\\) shows the fifth class size is farthest from the mean.\nNo outliers are present as the z values are within the \\(\\pm 3\\) guideline."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#five-number-summaries",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#five-number-summaries",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Five-Number Summaries",
    "text": "Five-Number Summaries\n\n\nSmallest Value\nFirst Quartile (25th percentile)\nMedian (50th percentile)\nThird Quartile (75th percentile)\nLargest Value\n\n\n\n\nNote: \\(k-th\\) percentile = percentile.EXC(Data Array, k), where 0 ≤ k ≤ 1."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#five-number-summaries---example",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#five-number-summaries---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Five-Number Summaries - Example",
    "text": "Five-Number Summaries - Example\nExample: Monthly starting salary\n\n\n\n\n\nMonthly Starting Salary ($)\n\n\n\n\n5,710\n\n\n5,755\n\n\n5,850\n\n\n5,880\n\n\n5,890\n\n\n5,920\n\n\n5,940\n\n\n5,950\n\n\n6,050\n\n\n6,130\n\n\n6,325\n\n\n\n\n\n \n\nLowest Value = 5,710\nThird Quartile = 6,025\nMedian = 5,905\nFirst Quartile = 5,857.5\nLargest Value = 6,325"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Boxplot",
    "text": "Boxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichelson–Morley experiment - Wiki\n\n\n\n\nA boxplot is a graphical summary of data that is based on a five-number summary.\nA key to the development of a boxplot is the computation of the median and the quartiles, \\(Q_1\\) and \\(Q_3\\).\nBoxplots provide another way to identify outliers."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Boxplot",
    "text": "Boxplot\nExample: monthly_starting_salary.xlsx\n\n\n\n\nA box is drawn with its ends located at the first and third quartiles.\nA vertical line is drawn in the box at the location of the median (second quartile)."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nLimits are located using the interquartile range (IQR).\nData outside these limits are considered outliers.\nThe locations of each outlier are shown with the symbol.\nThe limits are not shown is a Boxplot."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-3",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Boxplot",
    "text": "Boxplot\n\nExample: monthly_starting_salary.xlsx\n\n\n\n\n\n\nThe lower limit is located 1.5(IQR) below \\(Q_1\\).\n\nLower Limit: \\(Q_1 - 1.5(\\text{IQR}) = 5,857.5 - 1.5(167.5) = 5,606.25\\)\n\nThe upper limit is located 1.5(IQR) above \\(Q_3\\).\n\nUpper Limit: \\(Q_3 + 1.5(\\text{IQR}) = 6,025 + 1.5(167.5) = 6,276.25\\)\n\nThere is one outlier: 6,325."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-4",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#boxplot-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nDistribution and Boxplot"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\nThe covariance is a measure of the linear association between two variables.\nPositive values indicate a positive relationship.\nNegative values indicate a negative relationship."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance",
    "text": "Covariance\n\n\nThe covariance is computed as follows:\nFor population:\n\\[\n\\sigma_{xy} = \\frac{\\sum (x_i - \\mu_x)(y_i - \\mu_y)}{N}\n\\]\nFor samples:\n\\[\ns_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\]\nEXCEL for Sample covariance: =COVARIANCE.S(array1, array2)"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\n\n\nCorrelation is a unit-free measure of linear association and not necessarily causation."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\n\nSpurious Correlations!\n\n\n\nJust because two variables are highly correlated, it does not mean that one variable is the cause of the other."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\nThe correlation coefficient is computed as follows:\nFor population:\n\\[\n\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\n\\]\nFor samples:\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y}\n\\]\nEXCEL: =correl(array1, array2)"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-3",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\nThe coefficient can take on values between −1 and +1.\n\nValues near −1 indicate a strong negative linear relationship.\nValues near +1 indicate a strong positive linear relationship.\n\nThe closer the correlation is to zero, the weaker the relationship.\n\n\n\n\nGuess the correlation!"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-4",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\nRules of thumb:\n\n( 0.0 &lt; |r| &lt; 0.3 ) — weak correlation\n( 0.3 &lt; |r| &lt; 0.7 ) — moderate correlation\n( 0.7 &lt; |r| &lt; 1.0 ) — strong correlation"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-5",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\n\n\n\n\n\nThe correlation reflects the strength and direction of a linear relationship (top row)\nThe correlation does not reflect the slope of that relationship (middle)\nNor many aspects of nonlinear relationships (bottom).\nN.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance and Correlation Coefficient - Example",
    "text": "Covariance and Correlation Coefficient - Example\n\n\nExample: san_francisco_electronics_store.xlsx\n\n\n\n\n\n\n\nWeek\nNumber of Commercials\nSales ($100s)\n\n\n\n\n1\n2\n50\n\n\n2\n5\n57\n\n\n3\n1\n41\n\n\n4\n3\n54\n\n\n5\n4\n54\n\n\n6\n1\n38\n\n\n7\n5\n63\n\n\n8\n3\n48\n\n\n9\n4\n59\n\n\n10\n2\n46\n\n\n\n\n\n\n\n\nThe store’s manager wants to determine the relationship between the number of weekend television commercials shown and the sales at the store during the following week."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance and Correlation Coefficient - Example",
    "text": "Covariance and Correlation Coefficient - Example\n\n\nExample: san_francisco_electronics_store.xlsx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(y_i\\)\n\\(x_i - \\bar{x}\\)\n\\(y_i - \\bar{y}\\)\n\\((x_i - \\bar{x})(y_i - \\bar{y})\\)\n\n\n\n\n2\n50\n-1\n-1\n1\n\n\n5\n57\n2\n6\n12\n\n\n1\n41\n-2\n-10\n20\n\n\n3\n54\n0\n3\n0\n\n\n4\n54\n1\n3\n3\n\n\n1\n38\n-2\n-13\n26\n\n\n5\n63\n2\n12\n24\n\n\n3\n48\n0\n-3\n0\n\n\n4\n59\n1\n8\n8\n\n\n2\n46\n-1\n-5\n5\n\n\nTotals\n30\n510\n0\n99\n\n\n\n\n\n\n\n\\[\ns_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1} = \\frac{99}{10 - 1} = 11\n\\]"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example-2",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#covariance-and-correlation-coefficient---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance and Correlation Coefficient - Example",
    "text": "Covariance and Correlation Coefficient - Example\n\n\nExample: san_francisco_electronics_store.xlsx\n\n\nSample Covariance\n\\[\ns_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1} = \\frac{99}{10 - 1} = 11\n\\]\nSample Correlation Coefficient\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y} = \\frac{11}{1.49 \\times 7.93} = 0.93\n\\]"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-6",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#correlation-coefficient-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\\[\nr_{xy} = \\frac{s_{xy}}{s_x s_y} = \\frac{1}{n-1} \\sum \\left(\\frac{x_i - \\bar{x}}{s_x}\\right) \\left(\\frac{y_i - \\bar{y}}{s_y}\\right)\n\\] \n\nCorrelation is a unit-free measure of linear relationship.\nCorrelation is unchanged if one or both variables are linearly transformed."
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#using-excel-to-compute-covariance-and-correlation-coefficient-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#using-excel-to-compute-covariance-and-correlation-coefficient-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel to Compute Covariance and Correlation coefficient",
    "text": "Using Excel to Compute Covariance and Correlation coefficient\nExample: San Francisco Electronics Store\nExcel Formula and Value Worksheets"
  },
  {
    "objectID": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#summary-1",
    "href": "lecture_slides/03_chapter_descriptive_statistics/03_chapter_descriptive_statistics.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nDescriptive Statistics: help us to understand the data we have.\nMeasures of Central Location and Variability: Central location and variability metrics help us to summarize the data.\nDistribution Shape: skewness can be used to understand the shape of a distribution.\nRelative Location and z-Scores: z-scores to determine the relative position of data points within a distribution.\nEmpirical Rule: The 68-95-99.7 Rule for understanding data distribution in relation to the mean and standard deviation. Good for symetric distrubutions!\nCovariance and Correlation: to understand the relationship between two numerical variables."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#overview",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nContinuous Probability Distributions\n\nDefinition\nArea as a Measure of Probability\n\nNormal Probability Distribution\n\nCharacteristics\nApplications\n\n\n\n\nStandard Normal Probability Distribution\n\nStandardization\nExcel Functions\n\n\\(t\\)-distribution\n\nUse and comparison\nExcel Functions"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#continuous-probability-distributions-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#continuous-probability-distributions-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\n\n\nA continuous random variable can assume any value in an interval on the real line or in a collection of intervals.\nWe don’t normally talk about the probability of a continuous random variable assuming a particular value, because it is always 0 \\(P(X=x)=0\\).\nInstead, we talk about the probability of the random variable assuming a value within a given interval \\(P(X \\in (a,b))\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#continuous-probability-distributions-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#continuous-probability-distributions-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nThe probability of the random variable assuming a value within some given interval from \\(x_1\\) to \\(x_2\\) is defined to be the area under the graph of the probability density function between \\(x_1\\) and \\(x_2\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#area-as-a-measure-of-probability",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#area-as-a-measure-of-probability",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Area as a Measure of Probability",
    "text": "Area as a Measure of Probability\n\n\nThe area under the graph of \\(f(x)\\) and probability are identical.\nThis is valid for all continuous random variables.\nThe probability that \\(x\\) takes on a value between some lower value \\(x_1\\) and some higher value \\(x_2\\) can be found by computing the area under the graph of \\(f(x)\\) over the interval from \\(x_1\\) to \\(x_2\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\n\nThe normal probability distribution is the most common distribution for describing a continuous random variable.\nIt is widely used in statistical inference, especially because of the Central Limit Theorem (CLT).\nIt has been used in a wide variety of applications including:\n\nHeights of people\nTest scores\nRainfall amounts\nScientific measurements\n\nAbraham de Moivre, a French mathematician, published The Doctrine of Chances in 1733. He derived the normal distribution."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\n\nNormal Probability Density Function\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\n\\]\nwhere:\n\n\\(\\mu\\)= population mean\n\\(\\sigma\\)= population standard deviation\n\\(\\pi = 3.14159\\)\n\\(e = 2.71828\\)\n\nThe distribution is symmetric with respect to mean; its skewness measure is zero.\nThe graph of the distribution is a bell-shaped curve."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-3",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\n\nCharacteristics\n\nThe entire family of normal probability distributions is defined by its mean $$and its standard deviation \\(\\sigma\\). Denoted by \\(N(\\mu, \\sigma)\\).\nThe highest point on the normal curve is at the mean, which is also the median and mode.\nThe mean can be any numerical value: negative, zero, or positive. The mean determines the location of the distribution."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-4",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\n\nCharacteristics\n\nThe standard deviation determines the width of the curve: larger values result in wider, flatter curves.\nProbabilities for the normal random variable are given by areas under the curve. The total area under the curve is 1 (.5 to the left of the mean and .5 to the right)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-5",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#normal-probability-distribution-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\n\nCharacteristics\n\nEmpirical Rule"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probability-distribution-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probability-distribution-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probability Distribution",
    "text": "Standard Normal Probability Distribution\n\n\nCharacteristics\n\nA random variable having a normal distribution with a mean of 0 and a standard deviation of 1 is said to have a standard normal probability distribution.\nAlso called a z-distribution; Denoted by \\(Z\\) or \\(N(0,1)\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probability-distribution-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probability-distribution-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probability Distribution",
    "text": "Standard Normal Probability Distribution\n\n\n\n\nStandardization: Standardize \\(X \\sim N(\\mu, \\sigma)\\) to standard Normal Distribution, \\(Z\\)\n\\[\n  Z = \\frac{X - \\mu}{\\sigma}\n  \\]\n\nWe can think of \\(Z\\) as a measure of the number of standard deviations \\(X\\) is from \\(\\mu\\). (Cf. Standardization in Chapter 1.)\n\nConversely, \\(X = \\mu + z \\cdot \\sigma\\). If \\(X\\) can be written as \\(X = \\mu + z \\cdot \\sigma\\) where \\(Z\\) is standard normal, then \\(X \\sim N(\\mu, \\sigma^2)\\)\n\nThe letter z is used to designate the standard normal random variable."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel to Compute Standard Normal Probabilities",
    "text": "Excel to Compute Standard Normal Probabilities\n\n\nExcel functions for standard normal probability distributions.\n\n=NORM.S.DIST(z, TRUE) function computes the cumulative probability for a given \\(z\\) value of the standard normal distribution.\n=NORM.S.INV(cumulative probability) function computes the \\(z\\) value for a given cumulative probability of the standard normal distribution.\n“S” in the function names reminds us that these functions relate to the standard normal probability distribution.\n“TRUE” or “1” indicates a cumulative probability is requested."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel to Compute Standard Normal Probabilities",
    "text": "Excel to Compute Standard Normal Probabilities\n\n\nThe standard normal distribution and the cumulative area up to a given \\(z\\) value using the NORM.S.DIST(z, TRUE) function in Excel. The shaded area represents the cumulative probability up to the specified \\(z\\) value."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-3",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel to Compute Standard Normal Probabilities",
    "text": "Excel to Compute Standard Normal Probabilities\n\n\n\n \nFormula\n\n\n\n\n\n\n\n\n\n\n  Results"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-4",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#excel-to-compute-standard-normal-probabilities-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel to Compute Standard Normal Probabilities",
    "text": "Excel to Compute Standard Normal Probabilities\n\n\n \nFormula\n\n\n\n\n\n\n\n\n\n\n  Results"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\nGrear Tire company has developed a new steel-belted radial tire to be sold through a chain of discount stores. But before finalizing the tire mileage guarantee policy, Grear’s managers want probability information about the number of miles of tires will last.\nIt was estimated from the historical data that the (population) mean tire mileage is 36,500 miles with a (population) standard deviation of 5000.\nQuestion 1: The manager now wants to know the probability that the tire mileage \\(x\\) will exceed 40,000."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\n\nP(x &gt; 40,000) = ?\n\n\nStep 1: Convert x to standard normal distribution.\n\\[\nz = \\frac{(x - \\mu)}{\\sigma} \\\\\nz = \\frac{(40,000 - 36,500)}{5,000} \\\\\nz = 0.7\n\\]"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\n\nStep 2: Compute the area under the standard normal curve to the right of \\(z = 0.7\\)\n\n\n\n\\[\nP(z &gt; 0.7) = 1 - P(z \\leq 0.7) \\\\\n= 1 - \\text{=NORM.S.DIST}(0.7, \\text{TRUE}) \\\\\n= 1 - 0.7580 \\\\\n= 0.2420\n\\]"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-3",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\n\nQuestion 2: What should be the guaranteed mileage if Grear wants no more than 10% of tires to be eligible for the discount guarantee?\n\n\n(Hint: Given a probability, we can use the standard normal table in an inverse fashion to find the corresponding z-value.)"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-4",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\n\nStep 2: Convert \\(z_{0.1}\\) to the corresponding value of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-5",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#standard-normal-probabilities---example-grear-tire-company-problem-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standard Normal Probabilities - Example: Grear Tire Company Problem",
    "text": "Standard Normal Probabilities - Example: Grear Tire Company Problem\n\nStep 2: Convert \\(z_{0.1}\\) to the corresponding value of \\(x\\).\n\n\n\n\\[\nx = \\mu + z_{0.1} \\cdot \\sigma \\\\\n= 36,500 + \\text{=NORM.S.INV}(0.1) \\cdot (5000) \\\\\n= 36,500 + (-1.28155) \\cdot (5000) \\\\\n= 30,092.24\n\\]\nThus, a guarantee of 30,100 miles will meet the requirement that approximately 10% of the tires will be eligible for the guarantee."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#why-use-the-t-distribution",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#why-use-the-t-distribution",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Why use the \\(t\\)-distribution?",
    "text": "Why use the \\(t\\)-distribution?\n\nWhat can we do when dealing with small sample sizes (\\(n &lt; 30\\))?\nWhat can we do when the population standard deviation (\\(\\sigma\\)) is unknown and must be estimated from the sample?\nHow to obtain more accurate confidence intervals for the mean when dealing with small samples?\n\n\n\n\n\\(t\\)-distribution"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#why-use-the-t-distribution-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#why-use-the-t-distribution-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Why use the \\(t\\)-distribution?",
    "text": "Why use the \\(t\\)-distribution?\n\n\nSample Size Considerations\n\nWhen the sample size is small \\((n &lt; 30)\\), the \\(t\\)-distribution provides a better estimate than the normal distribution.\nWith small samples, the sample standard deviation (\\(S\\)) may not be a reliable estimate of the population standard deviation (\\(\\sigma\\)).\n\nUnknown Population Standard Deviation\n\nWhen the population standard deviation (\\(\\sigma\\)) is unknown and must be estimated from the sample, the \\(t\\)-distribution is appropriate.\nThe \\(t\\)-distribution adjusts for the additional variability introduced by using \\(S\\) instead of \\(\\sigma\\).\n\nMore Accurate Confidence Intervals\n\nProvides more accurate confidence intervals for the mean when dealing with small samples.\nReflects the increased uncertainty in estimates due to small sample sizes and unknown \\(\\sigma\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(t\\) Probability Distribution",
    "text": "\\(t\\) Probability Distribution\n\n\nStandardize \\(N(\\mu, \\sigma)\\) using sample mean and sample standard deviation:\n\n\\[\nt = \\frac{X - \\bar{X}}{S}\n\\]\n\nThis t-statistic has a \\(t\\)-distribution with degrees of freedom (df) = \\(n-1\\), which is the degrees of freedom of the sample standard deviation \\(S\\).\nWhen using sample mean and sample standard deviation to standardize a normal distribution, we will always have a \\(t\\)-distribution, approximately."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-2",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(t\\) Probability Distribution",
    "text": "\\(t\\) Probability Distribution\n\nSimilar to the standard normal.\nSymmetric with mean 0 and a shape parameter (the degrees of freedom, df).\nThe standard deviation is \\(\\sqrt{\\frac{df}{(df - 2)}}\\) and is always larger than 1 (Recall: 1 is the standard deviation of Z-distribution):\n\n\\(t(df)\\) has more variability than Z.\n\\(t(df)\\) has heavier tails than Z: More probability (area) in the tails than Z. (Heavier tails.)\n\\(t(df)\\) approaches Z if \\(df\\) increases. Empirical rules can be used when \\(df \\geq 5\\)."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-3",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(t\\) Probability Distribution",
    "text": "\\(t\\) Probability Distribution\n\n\nThe standard normal distribution and two \\(t\\) distributions with different degrees of freedom."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#comparing-normal-and-t-distributions",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#comparing-normal-and-t-distributions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Comparing Normal and \\(t\\)-Distributions",
    "text": "Comparing Normal and \\(t\\)-Distributions\n\nShape and Symmetry\n\nBoth distributions are symmetric and bell-shaped.\nThe \\(t\\)-distribution has heavier tails compared to the normal distribution, reflecting more variability.\n\nDependence on Sample Size\n\nThe normal distribution does not depend on sample size.\nThe \\(t\\)-distribution varies with degrees of freedom (\\(df = n - 1\\)).\nAs the degrees of freedom increase, the \\(t\\)-distribution approaches the normal distribution."
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution---example-grear-tire-company-problem",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution---example-grear-tire-company-problem",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(t\\) Probability Distribution - Example: Grear Tire Company Problem",
    "text": "\\(t\\) Probability Distribution - Example: Grear Tire Company Problem\nGrear Tire company has developed a new steel-belted radial tire to be sold through a chain of discount stores. But before finalizing the tire mileage guarantee policy, Grear’s managers want probability information about the number of miles tires will last.\nIt was estimated from a sample of size 28 that the sample mean tire mileage is 36,500 miles with a sample standard deviation of 5000.\nThe manager now wants to know:\n\nQuestion 1: What is the probability that the tire mileage \\(X\\) will exceed 40,000 miles?\nQuestion 2: What is the 10th percentile tire mileage for the new tires?"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution---example-grear-tire-company-problem-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#t-probability-distribution---example-grear-tire-company-problem-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(t\\) Probability Distribution - Example: Grear Tire Company Problem",
    "text": "\\(t\\) Probability Distribution - Example: Grear Tire Company Problem\nSolution\n\nQuestion 1: What is the probability that the tire mileage \\(X\\) will exceed 40,000 miles?\n\\(P(X &gt; 40,000) = P(t_{27} &gt; 0.7) = 1 - P(t_{27} \\leq 0.7)\\)\n\\[\n= 1 - \\text{T.DIST}(0.7, 27, \\text{TRUE}) \\\\\n= 1 - 0.7550 \\\\\n= 0.250\n\\]\nQuestion 2: What is the 10th percentile tire mileage for the new tires?\n\\[\nx_{0.1} = \\bar{X} + t_{0.1,27} \\cdot s \\\\\n= 36,500 + \\text{T.INV}(0.1, 27) \\cdot (5000) \\\\\n= 36,500 + (-1.314) \\cdot (5000) \\\\\n= 29,903 \\text{ miles}\n\\]"
  },
  {
    "objectID": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#summary-1",
    "href": "lecture_slides/06_chapter_continuous_probability_distributions/06_chapter_continuous_probability_distributions.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\n\nSome key takeaways from this session:\n\nContinuous Probability Distributions:\n\nContinuous random variables can assume any value in an interval.\nProbability is defined as the area under the probability density function.\n\nNormal Probability Distribution:\n\nMost common distribution for describing a continuous random variable.\nDefined by its mean and standard deviation.\nWidely used due to the Central Limit Theorem.\n\n\n\n\n\nStandard Normal Probability Distribution:\n\nA normal distribution with a mean of 0 and a standard deviation of 1.\nStandardization helps in comparing different normal distributions.\n\n\\(t\\)-distribution\n\nUse when the sample size is small \\((n &lt; 30)\\)\nUse when the population standard deviation is unknown\n\nExcel Functions:\n\n=NORM.S.DIST(z, TRUE) computes the cumulative probability for a given z-value.\n=NORM.S.INV(cumulative probability) computes the z-value for a given cumulative probability.\n=T.DIST(X, DF, TRUE) computes the cumulative probability for a given t-value.\n\n=T.INV(cumulative probability) computes the t-value for a given cumulative probability."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#overview",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nStatistical Inference\nCentral Limit Theorem\nPopulation Mean: \\(\\sigma\\) Known\n\n\n\nPopulation Mean: \\(\\sigma\\) Unknown\nPopulation Proportion\nSample Size Determination"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#statistical-inference-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#statistical-inference-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\nInterval Estimation\n\n\nConfidence level: 95% \\((1-\\alpha)\\) (Middle area; Confidence level)\nConfidence multipliers\nUpper/lower tail areas\nOne- or 2-sided intervals\nSampling errors and Margin of Error (MOE)\nFind range of all reasonable parameter values\n\n\n\n \n\n\n\n\nHypothesis Testing\n\n\nSignificance level: 5% \\((\\alpha)\\) (Tail area(s); Risk)\nCritical values\np-values\nWhich side \\((H_a)\\) depends mostly on data\nStrength of sample evidence against the hypothesized value (via p-value)\nTest a specific hypothesized parameter value"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate and Margin of Error",
    "text": "Interval Estimate and Margin of Error\n\nParameter Estimation\n\n\nWe are often interested in population parameters.\nSince complete populations are difficult (or impossible) to collect data on, we use sample statistics as point estimates for the unknown population parameters of interest.\nSample statistics vary from sample to sample.\nQuantifying how sample statistics vary provides a way to estimate the margin of error associated with our point estimate.\n\n\n\nSuppose we randomly sample 1,000 adults from each state in the US. Would you expect the sample means of their heights to be the same, somewhat different, or very different?\n\nNot the same, but only somewhat different."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate and Margin of Error",
    "text": "Interval Estimate and Margin of Error\n\n\n\n\nA plausible range of values for the population parameter is called a confidence interval.\nUsing only a sample statistic to estimate a parameter is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#central-limit-theorem",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#central-limit-theorem",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\nSeeing Theory\nhttps://seeing-theory.brown.edu/\n\n\nBunnies, Dragons and the ‘Normal’ World: Central Limit Theorem | The New York Times\n(video)"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate and Margin of Error",
    "text": "Interval Estimate and Margin of Error\n\nA point estimator cannot be expected to provide the exact value of the population parameter with a given level of confidence; but it is a good starting point to construct an interval estimate.\nAn interval estimate for mean can be computed by adding and subtracting a margin of error (MOE) to the point estimate.\n\\[\n\\text{Point Estimate} \\pm \\text{Margin of Error}\n\\]\nThe purpose of an interval estimate is to provide information about how close the point estimate is to the true value of the unknown population parameter with a certain level of confidence."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-3",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-and-margin-of-error-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate and Margin of Error",
    "text": "Interval Estimate and Margin of Error\n\n\n\n\n\n\n\n\n\n\n\nSimulation\nhttps://rpsychologist.com/d3/ci/"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean",
    "text": "Interval Estimate of a Population Mean\n\nIn order to develop an interval estimate of a population mean, the margin of error (the maximum error due to sampling in order to control the confidence level) must be computed using either:\n\nthe population standard deviation \\(\\sigma\\) (if known), or\nthe sample standard deviation \\(s\\)\n\n\\(\\sigma\\) is rarely known exactly, but often a good estimate can be obtained based on historical data or other information.\nWe refer to such cases as the \\(\\sigma\\) known case."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#margin-of-error-and-the-interval-estimate",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#margin-of-error-and-the-interval-estimate",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Margin of Error and the Interval Estimate",
    "text": "Margin of Error and the Interval Estimate\n\nThe general form of an interval estimate of a population mean is\n\\[\n\\bar{X} \\pm \\text{Margin of Error}\n\\]\nThe Margin of Error (MOE) depends on the sampling distribution (sampling variation) of the point estimate \\(\\bar{X}\\)."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sampling-distribution-of-barx",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sampling-distribution-of-barx",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sampling Distribution of \\(\\bar{X}\\)",
    "text": "Sampling Distribution of \\(\\bar{X}\\)\n\n\nIt will be symmetric if:\n\n\\(n\\) is large (Central Limit Teorem) or\nPopulation of \\(X\\) is normal"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-known-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-known-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Known",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Known\n\n\nWhen the population standard deviation (\\(\\sigma\\)) is known, we can compute a 95% confidence that the population parameter \\((\\mu)\\) will fall within \\(1.96 \\times \\sigma_{\\bar{X}}\\) from the \\(\\bar{X}\\); i.e., \\(\\mu\\) is in the following interval\n\n\\[\n\\bar{X} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\nWhen the quantities are replaced by numbers, the interval is called a 95% confidence interval for \\(\\mu\\).\n95% is called the confidence level (\\(= 1- \\alpha\\)). What means that \\(\\alpha = 0.05\\).\n1.96 is referred to as the 95% z-confidence multiplier, denoted by \\(z_{0.025}\\) (or \\(z_{\\alpha/2}\\)). This value represents the number of standard deviations one must move from the mean in both directions to capture the central 95% of the standard normal distribution, thereby defining the 95% confidence interval.\n\\(\\frac{\\sigma}{\\sqrt{n}}\\) Standard Error of the Mean"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#confidence-multipliers",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#confidence-multipliers",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Multipliers",
    "text": "Confidence Multipliers\n\n\nValues of \\(Z_{\\alpha/2}\\) for the Most Commonly Used Confidence Levels\n\n\n\n\nConfidence level\n\\(\\alpha\\)\n\\(\\alpha/2\\)\n\\(Z_{\\alpha/2}\\)\n\n\n\n\n90%\n0.1\n0.05\n1.645\n\n\n95%\n0.05\n0.025\n1.960\n\n\n99%\n0.01\n0.005\n2.576\n\n\n\n\nExcel: \\(= \\text{norm.s.inv}(1 - \\alpha/2)\\)"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown\n\nWhen \\(\\sigma\\) of a normal population is unknown, it is replaced by the sample standard deviation \\(S\\) and the distribution involved will be a \\(t\\)-distribution.\nThe degrees of freedom of the \\(t\\)-distribution is the sample size minus 1 (i.e., \\(n-1\\)).\nThe \\(1 - \\alpha\\) confidence multiplier is \\(t_{\\alpha/2, n-1}\\)\nExcel: = T.INV(1-\\(\\alpha\\)/2, \\(n-1\\))."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown\nThis formula is used to construct a confidence interval for the population mean when the population standard deviation is unknown and the sample size is relatively small.\n\\[\n\\bar{X} \\pm t_{\\alpha/2, n-1} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\n\\(\\bar{X}\\): The sample mean, which serves as the point estimate of the population mean.\n\\(t_{\\alpha/2, n-1}\\): The t-value for the chosen confidence level with \\(n-1\\) degrees of freedom. It represents the number of standard deviations (from the t-distribution) needed to capture the central \\((1-\\alpha)\\%\\) of the data.\n\\(s\\): The sample standard deviation, which measures the amount of variation or dispersion in the sample.\n\\(n\\): The sample size, representing the number of observations in the sample.\n\\(\\frac{s}{\\sqrt{n}}\\): The standard error of the mean, which estimates the standard deviation of the sampling distribution of the sample mean."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example\n\nExample: Credit card debt for the population of US households\n\nThe credit card balances of a sample of 70 households provided a mean credit card debt of $9,312 with a sample standard deviation of $4,007.\n\nLet us provide a 95% confidence interval estimate of the mean credit card debt for the population of US households. We will assume this population to be normally distributed."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example\n\\[\n\\bar{x} \\pm t_{.025, 69} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\\[\n\\bar{x} \\pm =T.INV(1-0.025,69) \\times \\frac{s}{\\sqrt{n}}\n\\]\n\\[\n9,312 \\pm 1.995 \\times \\frac{4007}{\\sqrt{70}} = 9,312 \\pm 955 = (8,357,10,267)\n\\]\nWe are 95% confident that the mean credit card debt for the entire population of U.S. households is between $8,357 and $10,267. The margin of error (maximum error) of the sample mean (as an estimate of the unknown population mean) is $955."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-mean-sigma-unknown---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "Interval Estimate of a Population Mean: \\(\\sigma\\) Unknown - Example\nAdequate Sample Size\nUsually, a sample size of \\(n \\geq 30\\) is adequate when using the expression\n\\[\n\\bar{x} \\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}\n\\]\nto develop an interval estimate of a population mean \\(\\mu\\).\n\nIf the population distribution is highly skewed or contains outliers, a sample size of 50 or more is recommended.\nIf the population is not normally distributed but is roughly symmetric, a sample size as small as 15 will suffice.\nIf the population is believed to be at least approximately normal, a sample size of less than 15 can be used."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion",
    "text": "Interval Estimate of a Population Proportion\n\nThe general form of an interval estimate of a population proportion is:\n\n\\[\n\\bar{p} \\pm \\text{Margin of Error}\n\\]\n\nwhere \\(\\bar{p}\\) is the sample proportion of the event of interest."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion",
    "text": "Interval Estimate of a Population Proportion\n\nThe sampling distribution of \\(\\bar{p}\\) plays a key role in computing the margin of error for this interval estimate.\nThe sampling distribution of \\(\\bar{p}\\) can be approximated by a normal distribution whenever \\(np \\geq 5\\) and \\(n(1-p) \\geq 5\\).This is known as success failure condition.\nUnlike the population mean, there is no \\(t\\)-distribution in this case."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-3",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion",
    "text": "Interval Estimate of a Population Proportion"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-4",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion",
    "text": "Interval Estimate of a Population Proportion\nThis formula is used to construct a confidence interval for the population proportion.\n\\[\n\\bar{p} \\pm z_{\\alpha/2} \\times \\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\n\\]\n\n\\(\\bar{p}\\): The sample proportion, which serves as the point estimate of the population proportion.\n\\(z_{\\alpha/2}\\): The z-value for the chosen confidence level. It represents the number of standard deviations (from the standard normal distribution) needed to capture the central \\((1-\\alpha)\\%\\) of the data.\n\\(\\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\\): The standard error of the proportion, which estimates the standard deviation of the sampling distribution of the sample proportion.\n\n\\(\\bar{p}(1 - \\bar{p})\\): The variance of the sample proportion.\n\\(n\\): The sample size, representing the number of observations in the sample."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---example",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion - Example",
    "text": "Interval Estimate of a Population Proportion - Example\nExample: Survey of women golfers\n\nA national survey of 900 women golfers was conducted to learn how women golfers view their treatment at golf courses in United States. The survey found that 396 of the women golfers were satisfied with the availability of tee times.\n\nSuppose one wants to develop a 95% confidence interval estimate for the proportion of the population of women golfers satisfied with the availability of tee times."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---example-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion - Example",
    "text": "Interval Estimate of a Population Proportion - Example\n\n\\[\n\\bar{p} \\pm z_{\\alpha/2} \\times \\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\n\\]\nwhere:\n\n\\(n = 900\\),\n\\(\\bar{p} = \\frac{396}{900} = 0.44\\),\n\\(z_{\\alpha/2} = 1.96\\)\n\n\\[\n0.44 \\pm 1.96 \\times \\sqrt{\\frac{0.44(1 - 0.44)}{900}} = 0.44 \\pm 0.0324\n\\]\nSurvey results enable us to state with 95% confidence that between 40.76% and 47.24% of all women golfers are satisfied with the availability of tee times."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---excel-example",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#interval-estimate-of-a-population-proportion---excel-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimate of a Population Proportion - Excel Example",
    "text": "Interval Estimate of a Population Proportion - Excel Example"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Size for an Interval Estimate of a Population Proportion",
    "text": "Sample Size for an Interval Estimate of a Population Proportion\n\nMargin of Error\n\\[\nE = z_{\\alpha/2} \\times \\sqrt{\\frac{\\bar{p}(1-\\bar{p})}{n}}\n\\]\nSolving for the necessary sample size \\(n\\), we get:\n\\[\nn = \\left( \\frac{z_{\\alpha/2}^2 \\times \\bar{p}(1-\\bar{p})}{E^2} \\right)\n\\]\nHowever, \\(\\bar{p}\\) will not be known until after we have selected the sample. We can use the planning value \\(p^*\\) for \\(\\bar{p}\\)."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Size for an Interval Estimate of a Population Proportion",
    "text": "Sample Size for an Interval Estimate of a Population Proportion\n\nNecessary Sample Size:\n\\[\nn = \\left( \\frac{z_{\\alpha/2}^2 \\times p^*(1-p^*)}{E^2} \\right)\n\\]\nThe planning value \\(p^*\\) can be chosen by one of the following procedures:\n\n\nPrevious Studies: Use the sample proportion from a previous study or similar survey as \\(p^*\\).\nPilot Study: Conduct a preliminary survey and use the sample proportion from that study as \\(p^*\\).\nBest Guess: Use a judgment or “best guess” based on available information or expert opinion.\nConservative Approach: If no prior information is available, use \\(p^* = 0.5\\) as it maximizes the sample size and provides the most conservative estimate. This is because the product \\(p^*(1 - p^*)\\) is maximized when \\(p^* = 0.5\\)."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Size for an Interval Estimate of a Population Proportion - Example",
    "text": "Sample Size for an Interval Estimate of a Population Proportion - Example\nExample: Survey of women golfers\n\nA national survey of 900 women golfers was conducted to learn how women golfers view their treatment at golf courses in United States. The survey found that 396 of the women golfers were satisfied with the availability of tee times.\n\nLet us provide a 95% confidence interval estimate of the mean credit card debt for the population of US households. We will assume this population to be normally distributed."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Size for an Interval Estimate of a Population Proportion - Example",
    "text": "Sample Size for an Interval Estimate of a Population Proportion - Example\nCalculations:\n\\[\n  E = z_{\\alpha/2} \\times \\sqrt{\\frac{p^*(1-p^*)}{n}} = .025\n  \\]\nAt 95% confidence \\(z_{0.0125} = 1.96\\). Assume \\(p^* = 0.44\\).\n\\[\n  n = \\left( \\frac{z_{\\alpha/2}^2 \\times p^*(1-p^*)}{E^2} \\right) = \\left( \\frac{(1.96)^2 (0.44)(0.56)}{(0.025)^2} \\right) = 1,514.5\n  \\]\nConclusion: A sample of size 1,515 is needed to reach a desired precision of \\(\\pm\\) 0.025 at 95% confidence."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#sample-size-for-an-interval-estimate-of-a-population-proportion---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Size for an Interval Estimate of a Population Proportion - Example",
    "text": "Sample Size for an Interval Estimate of a Population Proportion - Example\n\nWe used 0.44 as the best estimate of \\(p\\) in the preceding expression.\nIf no information is available about \\(p\\), then 0.5 is often assumed because it provides the highest possible (conservative) sample size.\nIf we had used \\(p = 0.5\\), the recommended \\(n\\) would have been 1537."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#concepts-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#concepts-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Concepts",
    "text": "Concepts\n\n\n\n\n\n\n\n\n\nConcept\nDefinition\nPurpose\n\n\n\n\nStandard Deviation\nA measure of the amount of variation or dispersion in a set of values.\nTo quantify the dispersion of data points around the mean.\n\n\nStandard Error\nThe standard deviation of the sample distribution of a statistic (e.g. the sample mean).\nTo estimate how much sample statistics will vary from the true population parameter.\n\n\nMargin of Error\nA measure of the range within which the true population parameter is expected to lie, with a given level of confidence.\nTo quantify the uncertainty in estimates of population parameters.\n\n\nConfidence Level\nThe probability that if a random sample were taken and a confidence interval calculated, that interval would contain the true population parameter.\nTo indicate the degree of confidence that the interval contains the true parameter.\n\n\nConfidence Interval\nAn interval estimate of a population parameter that provides an estimated range of values which is likely to include an unknown population parameter.\nTo provide a range in which we are fairly confident the true population parameter lies."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-1",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nCI for \\(\\bar{p}\\)\n\n\nCheck assumptions.\n\nRandomization\nIndependence\nThe sampling distribution of \\(\\bar{p}\\) is approximately normal. (\n\nCheck \\(n\\bar{p} \\geq 5\\) and \\(n(1 - \\bar{p}) \\geq 5\\).\n\n\nCalculate the standard error of \\(\\bar{p}\\): \\[\nse = \\sqrt{\\frac{\\bar{p}(1 - \\bar{p})}{n}}\n\\]\nIdentify \\(z\\) for your specified level of confidence.\nCalculate the interval: \\[\n\\bar{p} \\pm z \\times se\n\\]\n\n\n\nCI for \\(\\mu\\)\n\n\nCheck assumptions.\n\nRandomization\nIndependence\nThe sampling distribution of \\(\\bar{x}\\) is approximately normal.\n\nCheck \\(n \\geq 30\\) or underlying population distribution normal.\n\n\nCalculate the standard error of \\(\\bar{x}\\): \\[\nse = \\frac{s}{\\sqrt{n}}\n\\]\nIdentify \\(t\\) for your specified level of confidence \\[\n(df = n - 1)\n\\]\nCalculate the interval: \\[\n\\bar{x} \\pm t \\times se\n\\]"
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-2",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nStatistical Inference:\nInvolves using sample data to make generalizations about a population. It includes methods like interval estimation and hypothesis testing to draw conclusions with a certain level of confidence.\nCentral Limit Theorem:\nStates that the distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population’s distribution. This theorem underpins many inferential statistics techniques.\nInterval Estimation:\nProvides a range of values (confidence interval) for an unknown population parameter, giving an estimate along with an associated margin of error to quantify the uncertainty."
  },
  {
    "objectID": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-3",
    "href": "lecture_slides/08_chapter_interval_estimation/08_chapter_interval_estimation.html#summary-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nPopulation Mean (\\(\\sigma\\) Known):\nWhen the population standard deviation is known, a z-distribution is used to construct confidence intervals for the population mean.\nPopulation Mean (\\(\\sigma\\) Unknown):\nWhen the population standard deviation is unknown, a t-distribution is used. This is common in practical applications where the sample standard deviation serves as an estimate for the population standard deviation.\nPopulation Proportion:\nInvolves estimating the proportion of a population that possesses a certain characteristic. Confidence intervals for population proportions can be calculated using the sample proportion and its standard error.\nSample Size Determination:\nImportant for ensuring that estimates are accurate and reliable. The required sample size can be calculated based on desired margin of error, confidence level, and variability in the population."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#overview",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Hypothesis Testing\nFramework of Hypothesis Testing\nType I and Type II Errors\n\n\n\nHypothesis Testing for Means\nHypothesis Testing for Proportions\nUsing Excel for Hypothesis Testing"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#statistical-inference-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#statistical-inference-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\nInterval Estimation\n\n\nConfidence level: 95% \\((1-\\alpha)\\) (Middle area; Confidence level)\nConfidence multipliers\nUpper/lower tail areas\nOne- or 2-sided intervals\nSampling errors and Margin of Error (MOE)\nFind range of all reasonable parameter values\n\n\n\n\n\n\nHypothesis Testing\n\n\nSignificance level: 5% \\((\\alpha)\\) (Tail area(s); Risk)\nCritical values\np-values\nWhich side \\((H_a)\\) depends mostly on data\nTest a specific hypothesized parameter value\nStrength of sample evidence against the hypothesized value of the null hypothesis"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#hypothesis-testing-framework-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#hypothesis-testing-framework-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nHypothesis testing can be used to determine whether a statement about the population or a hypothesized value of the population parameter should or should not be rejected.\nWe start with a null hypothesis (\\(H_0\\)) that represents the status quo.\nWe also have an alternative hypothesis (\\(H_A\\)) that represents our research question, i.e. what we’re testing for.\nWe conduct a hypothesis test under the assumption that the null hypothesis is true.\nIf the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative.\nWe will never know which hypothesis is true, unless we sample the entire population."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#null-and-alternative-hypotheses-about-a-population-mean",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#null-and-alternative-hypotheses-about-a-population-mean",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Null and Alternative Hypotheses about a Population Mean",
    "text": "Null and Alternative Hypotheses about a Population Mean\nThe equality part of the hypotheses always appears in the null hypothesis.\nIn general, a hypothesis test about the value of a population mean \\(\\mu\\) takes one of the following three forms (where \\(\\mu_0\\) is the hypothesized value of the population mean).\n\n\n\n\n\n\n\n\n\nTest Type\nNull Hypothesis (\\(H_0\\))\nAlternative Hypothesis (\\(H_A\\))\n\n\n\n\nOne-tailed (lower-tail)\n\\(H_0\\): \\(\\mu \\geq \\mu_0\\)\n\\(H_A\\): \\(\\mu &lt; \\mu_0\\)\n\n\nOne-tailed (upper-tail)\n\\(H_0\\): \\(\\mu \\leq \\mu_0\\)\n\\(H_A\\): \\(\\mu &gt; \\mu_0\\)\n\n\nTwo-tailed\n\\(H_0\\): \\(\\mu = \\mu_0\\)\n\\(H_A\\): \\(\\mu \\neq \\mu_0\\)"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#building-a-null-hypothesis",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#building-a-null-hypothesis",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Building a Null Hypothesis",
    "text": "Building a Null Hypothesis\n\nFormulate the null hypothesis by stating that there is no effect, no difference, or that the effect is equal to a specific value. The null hypothesis is often a statement of equality (e.g., no difference or no association).\nKey Phrases for Null Hypothesis:\n\n“There is no difference…”\n“There is no effect…”\n“The population mean is equal to…”\n“The proportion is equal to…”\n\nExamples:\n\nSingle-Sample Test (Mean):\n\\(H_0: \\mu = \\mu_0\\)\n(The mean blood pressure reduction with the new drug is equal to a standard value.)\nProportion Test:\n\\(H_0: p = p_0\\)\n(The proportion of patients responding to the treatment is equal to 50%.)"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-i-error",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-i-error",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Type I Error",
    "text": "Type I Error\n\nBecause hypothesis tests are based on sample data, we must allow for the possibility of errors.\nA Type I error is rejecting \\(H_0\\) when \\(H_0\\) is true (but in the one-tailed tests, we still don’t know exactly the value of \\(\\mu\\)).\nThe probability of making a Type I error when the null hypothesis is true as an equality is called the level of significance \\((\\alpha)\\), which is (likely) the maximum value of the Type I probability.\nLevel of significance \\((\\alpha)\\) is predetermined by the user (normally, 1%, 5%, or 10%) to control the probability of making a Type I error: Significance tests."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-ii-error",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-ii-error",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Type II Error",
    "text": "Type II Error\n\nA Type II error is an error of accepting \\(H_0\\) when \\(H_0\\) is false. (Or, rejecting \\(H_A\\) when \\(H_A\\) is true.)\nNormally, we don’t control the probability of making a Type II error.\nStatisticians avoid the risk of making a Type II error by using “do not reject \\(H_0\\)” instead of “accept \\(H_0\\)”."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-i-and-type-ii-errors-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#type-i-and-type-ii-errors-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\nExample: A researcher sets out to test whether a new teaching method increases students’ average scores above the previous benchmark of 12 (\\(H_A\\)), challenging the long-held belief that the average score remains at or below 12 (\\(H_0\\)).\nHypotheses:\n\n\\(H_0\\): \\(\\mu \\leq 12\\)\n\\(H_A\\): \\(\\mu &gt; 12\\)"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-values",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-values",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\)-values",
    "text": "\\(p\\)-values\n\nThe \\(p-value\\) is the probability computed using the test statistic, that provides the probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is true.\nIf the \\(p-value\\) is low (lower than the significance level, \\(\\alpha\\), which is usually 5%) we say that it would be very unlikely to observe the data if the null hypothesis were true, and hence reject \\(H_0\\).\nIf the \\(p-value\\) is high (higher than \\(\\alpha\\)) we say that it is likely to observe the data even if the null hypothesis were true, and hence do not reject \\(H_0\\)."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests about a Population Mean: \\(\\sigma\\) Unknown",
    "text": "Tests about a Population Mean: \\(\\sigma\\) Unknown\n\nTwo approaches to decide about the Null Hypothesis (\\(H_0\\)).\n\nCritical Value Approach:\n\nIt uses critical values from the \\(t-distribution\\) to determine whether to reject the null hypothesis (\\(H_0\\)). Depending on the direction of the test (one-tailed or two-tailed), the critical value is compared to the calculated t-statistic.\nDecision Rule:\n\\[\n\\begin{aligned}\nH_0 &: \\mu \\geq \\mu_0; \\quad H_a: \\mu &lt; \\mu_0 \\quad \\text{(left/lower-tail)} \\quad &\\text{Reject } H_0 \\text{ if } t \\leq -t_{\\alpha, n-1} \\\\\nH_0 &: \\mu \\leq \\mu_0; \\quad H_a: \\mu &gt; \\mu_0 \\quad \\text{(right/upper-tail)} \\quad &\\text{Reject } H_0 \\text{ if } t \\geq t_{\\alpha, n-1} \\\\\nH_0 &: \\mu = \\mu_0; \\quad H_a: \\mu \\neq \\mu_0 \\quad \\text{(two-tail)} \\quad &\\text{Reject } H_0 \\text{ if } t \\leq -t_{\\alpha/2, n-1} \\text{ or } t \\geq t_{\\alpha/2, n-1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown-2",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests about a Population Mean: \\(\\sigma\\) Unknown",
    "text": "Tests about a Population Mean: \\(\\sigma\\) Unknown\n\n\n\\(p-value\\) Approach:\n\n\nCalculate the p-value, which is the probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true.\n\nDecision Rule:\n\\[\n\\text{Reject } H_0 \\text{ if } p\\text{-value} \\leq \\alpha\n\\]\n\nIf the p-value is less than or equal to the significance level (\\(\\alpha\\)), reject the null hypothesis."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-values-and-critical-value---excel",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-values-and-critical-value---excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p-values\\) and Critical Value - Excel",
    "text": "\\(p-values\\) and Critical Value - Excel\n\nEXCEL functions:\n\np-value is a probability: =T.DIST\nCritical value is a point or value on the horizontal or \\(t\\) axis: =T.INV"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown---example",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-mean-sigma-unknown---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests about a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "Tests about a Population Mean: \\(\\sigma\\) Unknown - Example\nA business travel magazine wants to classify transatlantic gateway airports according to the mean rating for the population of business travelers. A rating scale with a low score of 0 and a high score of 10 will be used, and airports with a population mean rating greater than 7 will be designated as superior service airports.\nA sample of 60 business travelers were surveyed at London’s Heathrow Airport, which provided a sample mean rating of \\(\\bar{x} = 7.25\\) and a sample standard deviation of \\(s = 1.052\\).\nQuestion: Do the data indicate that Heathrow should be designated as a superior service airport?"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-tailed-test-about-a-population-mean-sigma-unknown---example",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-tailed-test-about-a-population-mean-sigma-unknown---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-Tailed Test about a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "One-Tailed Test about a Population Mean: \\(\\sigma\\) Unknown - Example\n\n\nDetermine the hypotheses.\n\n\\[\n\\begin{aligned}\nH_0 &: \\mu \\leq 7 \\\\\nH_a &: \\mu &gt; 7 \\quad \\text{(upper tail)}\n\\end{aligned}\n\\]\n\nSpecify the level of significance.\n\n\\[\n\\alpha = 0.05\n\\]\n\nCompute the value of the test statistic.\n\n\\[\nt = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}} = \\frac{7.25 - 7}{1.052 / \\sqrt{60}} = 1.84\n\\]"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#critical-value-approach-visualization-upper-tailed---example",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#critical-value-approach-visualization-upper-tailed---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Critical Value Approach Visualization (Upper-tailed) - Example",
    "text": "Critical Value Approach Visualization (Upper-tailed) - Example"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-value-approach-visualization-upper-tailed---example",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#p-value-approach-visualization-upper-tailed---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p-value\\) Approach Visualization (Upper-tailed) - Example",
    "text": "\\(p-value\\) Approach Visualization (Upper-tailed) - Example"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-tailed-test-about-a-population-mean-sigma-unknown---example-2",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-tailed-test-about-a-population-mean-sigma-unknown---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-Tailed Test about a Population Mean: \\(\\sigma\\) Unknown - Example",
    "text": "One-Tailed Test about a Population Mean: \\(\\sigma\\) Unknown - Example\n\np-value Approach\n\nCompute the p-value = 0.0354.\nBecause p-value \\(\\leq \\alpha = 0.05\\), we reject \\(H_0\\).\n\nBased on the current sample, we have significant statistical evidence to reject the null hypothesis and conclude that Heathrow should be classified as a superior service airport. The significance level is 5%. The sample evidence against \\(H_0\\) is strong."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#null-and-alternative-hypotheses-about-a-population-proportion",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#null-and-alternative-hypotheses-about-a-population-proportion",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Null and Alternative Hypotheses about a Population Proportion",
    "text": "Null and Alternative Hypotheses about a Population Proportion\n\n\nThe equality part of the hypotheses always appears in the null hypothesis.\nIn general, a hypothesis test about the value of a population proportion \\(p\\) must take one of the following three forms (where \\(p_0\\) is the hypothesized value of the population proportion).\n\n\\[\n\\begin{aligned}\n\\text{One-tailed (lower tail)} & \\quad H_0: p \\geq p_0 \\quad H_a: p &lt; p_0 \\\\\n\\text{One-tailed (upper tail)} & \\quad H_0: p \\leq p_0 \\quad H_a: p &gt; p_0 \\\\\n\\text{Two-tailed} & \\quad H_0: p = p_0 \\quad H_a: p \\neq p_0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests about a Population Proportion",
    "text": "Tests about a Population Proportion\n\n\nTest Statistic:\nThe Z-test for a population proportion is used to determine whether the observed sample proportion (\\(\\bar{p}\\)) is significantly different from the hypothesized population proportion (\\(p_0\\)). By calculating the z-value, we can compare it to critical values from the standard normal distribution to decide whether to reject the null hypothesis.\n\\[\nz = \\frac{\\bar{p} - p_0}{\\sigma_{\\bar{p}}}\n\\]\nwhere:\n\\[\n\\sigma_{\\bar{p}} = \\sqrt{\\frac{p_0 (1 - p_0)}{n}}\n\\]\nassuming \\(np_0 \\geq 5\\) and \\(n(1 - p_0) \\geq 5\\).\n\n\\(\\bar{p}\\): Sample proportion\n\\(p_0\\): Hypothesized population proportion\n\\(\\sigma_{\\bar{p}}\\): Standard error of the sample proportion\n\\(p_0\\): Hypothesized population proportion\n\\(n\\): Sample size"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests About a Population Proportion - Example",
    "text": "Tests About a Population Proportion - Example\n\nExample: Pine Creek Golf Course\n\nOver the past year, 20% of the players at Pine Creek were women. In an effort to increase the proportion of women players, Pine Creek implemented a special promotion design to attract women golfers. The manager now wants to determine if the proportion of women players has increased.\n\nA random sample of 400 players were selected and 100 of the players were women. The level of significance is \\(\\alpha = .05\\)."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests About a Population Proportion - Example",
    "text": "Tests About a Population Proportion - Example\n\n\nDetermine the hypotheses\n\n\\(H_0: p \\le .20\\) and \\(H_a: p &gt; .20\\) (upper tail)\n\n\n\n\nSpecify the level of significance. \\(\\alpha = .05\\)\n\n\n\nCompute the value of the z test statistic.\n\n\\(\\sigma_{\\bar{p}} = \\sqrt{\\frac{p_0(1-p_0)}{n}} = \\sqrt{\\frac{.2(1-.2)}{400}} = .02\\)\n\\(z = \\frac{\\bar{p} - p_0}{\\sigma_{\\bar{p}}} = \\frac{(.25) - .20}{.02} = 2.50\\)"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-2",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests About a Population Proportion - Example",
    "text": "Tests About a Population Proportion - Example\n\nCritical Value Approach\n\nDetermine the critical values and rejection rule.\n\n\\(z_{0.05} = \\text{NORM.S.INV}(0.95) = 1.645\\)\nDecision Rule: Reject \\(H_0\\) if \\(z \\ge 1.645\\)\n\n\n\n\nDetermine whether to reject \\(H_0\\) because \\(z = 2.5 &gt; 1.645\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-3",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#tests-about-a-population-proportion---example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tests About a Population Proportion - Example",
    "text": "Tests About a Population Proportion - Example\n\np-value Approach\n\nCompute the p-value for \\(z = 2.50\\) (an upper tail problem).\n\n\\(p\\)-value \\(= 1 - \\text{NORM.S.DIST}(2.50, \\text{TRUE}) = 1 - .9938 = .0062\\)\n\n\n\n\nBecause \\(p\\)-value \\(= .0062 &lt; \\alpha = .05\\), we reject \\(H_0\\).\n\nBased on the current sample, we have significant statistical evidence to conclude that the proportion of women players has increased from 20%. The significance level is 5%. The evidence against \\(H_0\\) is overwhelming."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-sided-and-two-sided-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#one-sided-and-two-sided-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-sided and Two-sided",
    "text": "One-sided and Two-sided\n\nFor a left/lower-tailed test, we calculate the tail area and critical value on the left side.\nFor a two-tailed test,\n\nDouble the tail area to obtain the \\(p\\)-value (Note: p-value is a probability and, hence, must be between 0 and 1, inclusively.)\nDivide the significance level \\(\\alpha\\) and assign to each side to obtain two critical values."
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#using-excel-to-determine-p-values-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#using-excel-to-determine-p-values-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel to Determine \\(p-values\\)",
    "text": "Using Excel to Determine \\(p-values\\)"
  },
  {
    "objectID": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#summary-1",
    "href": "lecture_slides/09_chapter_hypothesis_tests/09_chapter_hypothesis_tests.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nDeveloping a null hypothesis involves stating the parameter of interest, its hypothesized value, and forming \\(H_0\\) to reflect no change or effect.\nThe critical value approach and p-value approach are two fundamental methods in hypothesis testing.\nCritical values are determined based on the significance level and the test statistic’s distribution.\nThe p-value approach provides a probability measure to assess the evidence against \\(H_0\\).\nBoth approaches lead to the same conclusion but offer different perspectives on the data."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#overview",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nInference about a Population Variance\nChi-Square Distribution\nInterval Estimation of \\(\\sigma^2\\)\n\n\n\nHypothesis Testing about a Population Variance\nInferences about Two Population Variances\nF-Test Two-Sample for Variances - Example Excel"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#statistical-inference-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#statistical-inference-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\nInterval Estimation\n\n\nConfidence level: 95% \\((1-\\alpha)\\) (Middle area; Confidence level)\nConfidence multipliers\nUpper/lower tail areas\nOne- or 2-sided intervals\nSampling errors and Margin of Error (MOE)\nFind range of all reasonable parameter values\n\n\n\n\n\n\nHypothesis Testing\n\n\nSignificance level: 5% \\((\\alpha)\\) (Tail area(s); Risk)\nCritical values\np-values\nWhich side \\((H_a)\\) depends mostly on data\nStrength of sample evidence against the hypothesized value (via p-value)\nTest a specific hypothesized parameter value"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-framework-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-framework-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nHypothesis testing can be used to determine whether a statement about the population or a hypothesized value of the population parameter should or should not be rejected.\nWe start with a null hypothesis (\\(H_0\\)) that represents the status quo.\nWe also have an alternative hypothesis (\\(H_A\\)) that represents our research question, i.e. what we’re testing for.\nWe conduct a hypothesis test under the assumption that the null hypothesis is true.\nIf the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative.\nWe will never know which hypothesis is true, unless we sample the entire population."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#inference-about-a-population-variance-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#inference-about-a-population-variance-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Inference about a Population Variance",
    "text": "Inference about a Population Variance\n\nA variance can provide important decision-making information.\nConsider the production process of filling containers with a liquid detergent product.\n\nThe mean filling weight is important, but so too is the variance of the filling weights.\nBy selecting a sample of containers, we can compute a sample variance for the amount of detergent placed in a container.\nIf the sample variance is excessive, overfilling and underfilling may be occurring even though the mean is correct."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Chi-Square Distribution",
    "text": "Chi-Square Distribution\n\nWe can use the chi-square distribution to develop interval estimates and conduct hypothesis tests about a population variance \\(\\sigma^2\\).\nThe chi-square (\\(\\chi^2\\)) distribution is the sum of squared standardized normal random variables (a variable that follows a normal distribution) such as \\((z_1)^2 + (z_2)^2 + (z_3)^2\\) and so on.\nThe chi-square distribution is based on sampling from a normal population.\nThe sampling distribution of \\((n-1)S^2/\\sigma^2\\) has a chi-square distribution with \\(n-1\\) degrees of freedom whenever a simple random sample of size \\(n\\) is selected from a normal population with population variance \\(\\sigma^2\\). \\(S^2\\) is the sample variance."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distributions-with-different-degrees-of-freedom",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distributions-with-different-degrees-of-freedom",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Chi-Square Distributions with different degrees of Freedom",
    "text": "Chi-Square Distributions with different degrees of Freedom"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Chi-Square Distribution",
    "text": "Chi-Square Distribution\nWe will use the notation \\(\\chi^2_{\\alpha}\\) to denote the value for the chi-square distribution that provides an area of \\(\\alpha\\) to the right of the stated \\(\\chi^2_{\\alpha}\\) value."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#chi-square-distribution-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Chi-Square Distribution",
    "text": "Chi-Square Distribution"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimation of \\(\\sigma^2\\)",
    "text": "Interval Estimation of \\(\\sigma^2\\)\n\nThere is a \\((1-\\alpha)\\) probability of obtaining a \\((n-1)S^2/\\sigma^2\\) value such that\n\\[\n\\chi^2_{(1-\\alpha/2)} \\leq \\frac{(n-1)S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2}\n\\]\nPerforming algebraic manipulation, we obtain an \\(1-\\alpha\\) interval estimate of the population variance \\[\n\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{(1-\\alpha/2)}}\n\\]\nTaking the square root of the upper and lower limits of the variance interval provides the confidence interval for the population standard deviation. \\[\n\\sqrt{\\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}}} \\leq \\sigma \\leq \\sqrt{\\frac{(n-1)S^2}{\\chi^2_{(1-\\alpha/2)}}}\n\\]"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimation of \\(\\sigma^2\\) - Example",
    "text": "Interval Estimation of \\(\\sigma^2\\) - Example\nExample: Liquid detergent product filling process\nThe filling mechanism for the process of filling liquid detergent product in a container is adjusted such that mean filling weight is 16 ounces per container. A sample of 20 containers was taken to test this. If the sample variance is modest, the production process will be continued. If the sample variance is excessive, the filling mechanism will be readjusted to reduce the filling variance.\nThe sample variance for the filling quantities is found to be \\(s^2 = .0025\\).\nChoose \\(\\alpha = .05\\)."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimation of \\(\\sigma^2\\) - Example",
    "text": "Interval Estimation of \\(\\sigma^2\\) - Example\n\nTwo confidence multipliers: (df = n-1 = 20-1 = 19) \\[\n\\chi^2_{0.025} = \\text{CHISQ.INV}(0.975, 19) = 32.852\n\\] \\[\n\\chi^2_{0.975} = \\text{CHISQ.INV}(0.025, 19) = 8.907\n\\]"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimation of \\(\\sigma^2\\) - Example",
    "text": "Interval Estimation of \\(\\sigma^2\\) - Example\n\nA 95% confidence interval for the population variance is given by: \\[\n\\frac{(20-1)0.0025}{32.852} \\leq \\sigma^2 \\leq \\frac{(20-1)0.0025}{8.907}\n\\]\n\\[\n0.0014 \\leq \\sigma^2 \\leq 0.0053\n\\]\nTaking the square root of these values provides a 95% confidence interval for the population standard deviation.\n\\[\n0.0380 \\leq \\sigma \\leq 0.0730\n\\]\nWe are 95% confident that the process standard deviation of the filling weight per container is between 0.0380 ounces and 0.0730 ounces."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-excel",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#interval-estimation-of-sigma2---example-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interval Estimation of \\(\\sigma^2\\) - Example Excel",
    "text": "Interval Estimation of \\(\\sigma^2\\) - Example Excel"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance",
    "text": "Hypothesis Testing about a Population Variance\n\n\n\nHypotheses\n\n\\(H_0: \\sigma^2 \\geq \\sigma_0^2\\)\n\\(H_a: \\sigma^2 &lt; \\sigma_0^2\\) (Left-tailed)\n\nwhere \\(\\sigma_0^2\\) is the hypothesized value for the population variance.\n\nTest Statistic\n\\[\n\\chi^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\n\\]\n\n\n\n\n\nRejection Rule (Left-tailed test)\n\nCritical-value approach:\n\nReject \\(H_0\\) if \\(\\chi^2 \\leq \\chi^2_{(1-\\alpha)}\\)\n\n\n\n\n\n\np-value approach:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\)\n\n\nwhere \\(\\chi^2_{(1-\\alpha)}\\) is based on a chi-square distribution with \\(n-1\\) df."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance",
    "text": "Hypothesis Testing about a Population Variance\n\n\n\nHypotheses\n\\[\n\\begin{aligned}\nH_0 &: \\sigma^2 \\leq \\sigma^2_0 \\\\\nH_a &: \\sigma^2 &gt; \\sigma^2_0 \\quad \\text{(Right-tailed)}\n\\end{aligned}\n\\]\nwhere \\(\\sigma^2_0\\) is the hypothesized value for the population variance.\n\nTest Statistic\n\\[\n\\chi^2 = \\frac{(n-1)S^2}{\\sigma^2_0}\n\\]\n\n\n\n\n\nRejection Rule (Right-tailed test)\n\nCritical-value approach:\nReject \\(H_0\\) if \\(\\chi^2 \\geq \\chi^2_\\alpha\\)\n\n\n\n\n\np-value approach:\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\)\n\nwhere \\(\\chi^2_\\alpha\\) is based on a chi-square distribution with \\(n-1\\) df."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance",
    "text": "Hypothesis Testing about a Population Variance\n\n\n\nHypotheses\n\\[\n\\begin{aligned}\nH_0 &: \\sigma^2 = \\sigma^2_0 \\\\\nH_a &: \\sigma^2 \\neq \\sigma^2_0 \\quad \\text{(Two-tailed)}\n\\end{aligned}\n\\]\nwhere \\(\\sigma^2_0\\) is the hypothesized value for the population variance.\n\nTest Statistic\n\\[\n\\chi^2 = \\frac{(n-1)S^2}{\\sigma^2_0}\n\\]\n\n\n\n\n\nRejection Rule (Two-tailed test)\n\nCritical-value approach:\nReject \\(H_0\\) if \\(\\chi^2 \\leq \\chi^2_{(1-\\alpha/2)}\\) or \\(\\chi^2 \\geq \\chi^2_{\\alpha/2}\\)\n\n\n\n\n\np-value approach:\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\)\n\nwhere \\(\\chi^2_{(1-\\alpha/2)}\\) and \\(\\chi^2_{\\alpha/2}\\) are based on a chi-square distribution with \\(n-1\\) df."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance - Example",
    "text": "Hypothesis Testing about a Population Variance - Example\nExample: St. Louis Metro Bus Company\nSt. Louis Metro Bus Company wants to promote an image of reliability by encouraging its drivers to maintain consistent schedules. The company specifies an arrival time variance of 4 or less when arrival times are measured in minutes.\nWe will conduct a hypothesis test (with \\(\\alpha = 0.05\\)) to determine whether the arrival time population variance is excessive."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance - Example",
    "text": "Hypothesis Testing about a Population Variance - Example\nHypotheses: \\[\nH_0: \\sigma^2 \\leq 4\n\\] \\[\nH_a: \\sigma^2 &gt; 4\n\\]\nA random sample of 24 bus arrivals taken at a downtown intersection provided a sample variance of \\(s^2 = 4.9\\).\nTest Statistic\n\\[\n\\chi^2 = \\frac{(n-1)S^2}{\\sigma_0^2} = \\frac{(24-1)(4.9)}{4} = 28.18\n\\]"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance - Example",
    "text": "Hypothesis Testing about a Population Variance - Example\n\n\\(p\\)-value Approach:\n\n\n\n\\(p\\)-value = 1-CHISQ.DIST(28.18, 23, TRUE) = 0.209 &gt; 5%. Fail to reject \\(H_0\\)."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance - Example",
    "text": "Hypothesis Testing about a Population Variance - Example\n\n\n5% Critical Value = CHI.INV(0.95, 23) = 35.172\n\nRejection Rule:\nReject \\(H_0\\) if \\(\\chi^2 \\geq 35.172\\)\n\n\nBecause \\(\\chi^2 = 28.18\\), we fail to reject the null hypothesis.\n\nConclusion: Based on the current sample, we do not have significant statistical evidence to conclude that the arrival time variance is greater than 4. The significance level is 5%."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-excel",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-a-population-variance---example-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about a Population Variance - Example Excel",
    "text": "Hypothesis Testing about a Population Variance - Example Excel"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#inferences-about-two-population-variances-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#inferences-about-two-population-variances-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Inferences about Two Population Variances",
    "text": "Inferences about Two Population Variances\n\nWe may want to compare the population variances (variation) in:\n\nproduct quality resulting from two different production processes,\ntemperatures for two heating devices, or\nassembly times for two assembly methods.\n\nWe use sample data collected from two independent normal populations.\nThe two sample variances will be the basis for making inferences about the two normal population variances."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations",
    "text": "Hypothesis Testing about the Variances of Two Populations\nOne-Tailed Test: (Right tail)\nHypotheses: \\[\n  H_0: \\sigma_1^2 \\leq \\sigma_2^2 \\quad \\left( \\frac{\\sigma_1^2}{\\sigma_2^2} \\leq 1 \\right)\n  \\] \\[\n  H_a: \\sigma_1^2 &gt; \\sigma_2^2 \\quad \\left( \\frac{\\sigma_1^2}{\\sigma_2^2} &gt; 1 \\right)\n  \\]\n\nThe population providing the larger sample variance is designated as Population 1.\nThe Null Hypothesis is based on the assumption that the variances are equal.\nTest Statistic:\n\\[\nF = \\frac{S_1^2}{S_2^2} \\quad \\text{follows F-distribution } (n_1-1, n_2-1), \\text{ if } H_0(=) \\text{ is true}.\n\\]"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations",
    "text": "Hypothesis Testing about the Variances of Two Populations\nRejection Rule:\n\nCritical-value approach:\n\nReject \\(H_0\\) if \\(F \\geq F_{\\alpha, n_1-1, n_2-1}\\).\nwhere the value of \\(F_{\\alpha, n_1-1, n_2-1}\\) is based on an \\(F\\) distribution with \\(n_1 - 1\\) (numerator) and \\(n_2 - 1\\) (denominator) d.f., such that the upper tail area is \\(\\alpha\\).\n\np-value approach:\n\nReject \\(H_0\\) if p-value \\(\\leq\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations",
    "text": "Hypothesis Testing about the Variances of Two Populations\n\nTwo-Tailed Test\nHypotheses:\n\\[ H_0 : \\sigma^2_1 = \\sigma^2_2 \\] \\[ H_a : \\sigma^2_1 \\neq \\sigma^2_2 \\]\n\nDenote the population providing the larger sample variance as population 1.\nThe Null Hypothesis is based on the assumption that the variances are equal.\n\nTest Statistic:\n\\[ F = \\frac{S^2_1}{S^2_2} \\]\nfollows F-distribution \\((n_1-1, n_2-1)\\), if \\(H_0 ( =)\\) is true."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations",
    "text": "Hypothesis Testing about the Variances of Two Populations\nDecision Rule:\n\nCritical-value approach:\n\nReject \\(H_0\\) if \\(F \\geq F_{\\alpha/2, n_1-1, n_2-1}\\).\nwhere the value of \\(F_{\\alpha/2, n_1-1, n_2-1}\\) is based on an \\(F\\) distribution with \\(n_1 - 1\\) (numerator) and \\(n_2 - 1\\) (denominator) d.f.\n\np-value approach:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\)."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations - Example",
    "text": "Hypothesis Testing about the Variances of Two Populations - Example\nExample: Dullus County School\nDullus County school wants to select one among the Milbank Company and Gulf Park Company for the bus service contract for the coming year. The variance of arrival/pickup time will be used as a primary measure of the quality of service.\nWe will conduct a hypothesis test with \\(\\alpha = 10\\%\\) to see if the variances are equal for Milbank Company and Gulf Park Company."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations - Example",
    "text": "Hypothesis Testing about the Variances of Two Populations - Example\n\nHypotheses:\n\n\\(H_0\\): Milbank and Gulf Park have same arrival time variance\n\\(H_a\\): Their variances are not equal- Two-Tailed\n\n\\[ H_0 : \\sigma^2_1 = \\sigma^2_2\\]\n\\[ H_a : \\sigma^2_1 \\neq \\sigma^2_2 \\]\nA sample of 26 arrival times for the Milbank service provided a sample variance of 48 and a sample of 16 arrival times for the Gulf Park service provided a sample variance of 20.\nBecause the Milbank sample provided a larger sample variance, we will denote Milbank as Population 1.\nHence, \\(n_1 = 26, n_2 = 16\\).\nRecall \\(\\alpha = 10\\%\\)."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations - Example",
    "text": "Hypothesis Testing about the Variances of Two Populations - Example\nTest statistic \\(F\\) has 25 d.f. (numerator), and 15 d.f. (denominator)\n\\[ F = \\frac{S^2_1}{S^2_2} = \\frac{48}{20} = 2.40 \\]\nCritical Value"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations - Example",
    "text": "Hypothesis Testing about the Variances of Two Populations - Example\nCritical-value approach:\n\\[\\alpha / 2 = 0.05\\]\n\nUpper-tailed critical value = F.INV(0.95, 25, 15) = 2.28\nDecision rule:\n\nReject \\(H_0\\) if \\(F \\geq 2.28\\).\n\nBecause \\(F = 2.40\\) is greater than 2.28, we reject null hypothesis."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-4",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#hypothesis-testing-about-the-variances-of-two-populations---example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Hypothesis Testing about the Variances of Two Populations - Example",
    "text": "Hypothesis Testing about the Variances of Two Populations - Example\np-value approach:\n\n\n25 d.f. (numerator), and 15 d.f. (denominator),\np-value = 2*(1- F.DIST(2.40, 25,15, TRUE)) = 2*0.041 = 0.082.\nBecause \\(\\alpha = 0.10\\), we have p-value \\(&lt; \\alpha\\) and therefore, null hypothesis is rejected."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#f-test-two-sample-for-variances---example-excel-2",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#f-test-two-sample-for-variances---example-excel-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "F-Test Two-Sample for Variances - Example Excel",
    "text": "F-Test Two-Sample for Variances - Example Excel\n\n\n\nExample: Dullus County School\n\nEnter A1: A27 in the Variable 1\nRange box.\nEnter B1:B17 in the Variable 2\nRange box.\nSelect Labels.\nEnter .05 in the Alpha box.\nSelect Output Range and enter\nD1 in the box.\nClick OK."
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#f-test-two-sample-for-variances---example-excel-3",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#f-test-two-sample-for-variances---example-excel-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "F-Test Two-Sample for Variances - Example Excel",
    "text": "F-Test Two-Sample for Variances - Example Excel\nResult:"
  },
  {
    "objectID": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#summary-1",
    "href": "lecture_slides/11_chapter_inference_pop_variance/11_chapter_inference_pop_variance.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nChi-Square Distribution\n\nThe chi-square distribution is used for interval estimation and hypothesis testing about a population variance.\nIt is the sum of squared standardized normal random variables.\nUnderstanding its dependence on sampling from a normal population and degrees of freedom.\n\nInterval Estimation of \\(\\sigma^2\\)\n\nIt is easy to convert the interval estimate of variance to an interval estimate of standard deviation.\n\nHypothesis Testing about a Population Variance\n\nWe use the chi-square distribution for the test statistic.\nWe can interpret results using both the critical value approach and the p-value approach."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#overview",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroducion to Experimental Design\nAnalysis of Variance (ANOVA)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design",
    "text": "Experimental Design\n\n\nStatistical studies can be classified as being either experimental or observational.\nIn an experimental study, the levels of one or more factors are controlled so that data can be obtained about how the factors influence the response variables of interest.\n\nCause-and-effect relationships are easier to establish in experimental studies.\nAnalysis of variance (ANOVA) can be used to analyze the data obtained from experimental studies.\n\nIn an observational study, no attempt is made to control levels of the factors."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\nChemitech developed a new filtration system for municipal water supplies. There are different methods that can be used to assemble the system.\nChemitech has narrowed down to three methods: A, B, and C and wants to determine which assembly method can produce the greatest mean number of filtration systems per week."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\nRandomization and Replications\nFor the purpose, the company randomly selected 15 workers and randomly divided them into 3 groups, and then randomly assigned one of the three treatments to each group 5 of the workers.\n\n\n\nFactor\nAssembly method\n\n\n\n\nTreatments/Levels/Values\nA, B, and C\n\n\nResponse\nNumber of units produced\n\n\nExperimental units\nEmployees"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-2",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\nTerminology\n\nA factor is a variable that the experimenter has selected for investigation.\nA treatment is a level (value) of a factor.\nResponse or performance variable is a dependent/outcome variable affected by the factor.\nExperimental units are the objects of interest in the experiment.\nA completely randomized design is an experimental design in which the treatments are randomly assigned to the experimental units."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-3",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\n\n\n\nObservation\nA\nB\nC\n\n\n\n\n1\n58\n58\n48\n\n\n2\n64\n69\n57\n\n\n3\n55\n71\n59\n\n\n4\n66\n64\n47\n\n\n5\n67\n68\n49\n\n\nSample mean \\(\\bar{x}_j\\)\n62\n66\n52\n\n\nSample variance \\(s_j^2\\)\n27.5\n26.5\n31.0\n\n\nSample standard deviation \\(s_j\\)\n5.244\n5.148\n5.568\n\n\n\n\n\nGrand sample mean \\(\\bar{x}\\) = 60"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-4",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\n\n\nOne-way Analysis of Variance (ANOVA) can be used to test for the equality of means (mean responses) of two or more normal populations.\n\n\n\nThe hypotheses:\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = ... = \\mu_k\\)\n\\(H_a:\\) Not all population means are equal.\n\n where,\n\\((\\mu_1, \\mu_2, \\mu_3, ..., \\mu_k\\) are mean responses for the \\(k\\) populations.)\n\\((k = 3\\) for the motivating example.)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-5",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#experimental-design---example-chemitech-experiment-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Experimental Design - Example: Chemitech Experiment",
    "text": "Experimental Design - Example: Chemitech Experiment\n\n\nIf \\(H_0\\) is rejected, we cannot conclude that all population means are different.\n\n\n\nRejecting \\(H_0\\) means that at least two population means have different values, or not all means are equal."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-model-assumptions",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-model-assumptions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Model Assumptions",
    "text": "One-way ANOVA: Model Assumptions\n\nFor each population:\n\nThe response variable is normally distributed.\nThe mean of \\(i\\)-th population is \\(\\mu_i\\).\nThe variance of the response variable is the same for all of the populations; denoted \\(\\sigma^2\\) (unknown).\nThe observations must be independent."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-analysis-method",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-analysis-method",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Analysis Method",
    "text": "One-way ANOVA: Analysis Method\n\n\n\nIf the variation of the sample means is large, then not all population means are equal."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-analysis-method-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-analysis-method-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Analysis Method",
    "text": "One-way ANOVA: Analysis Method\n\nThere are two types of variation:\n\nBetween-treatments variation of sample means (\\(\\bar{x}_j\\)): Variation due to different treatments. (Explained variation)\nWithin-treatments variation of observations from their respective treatment means (Unexplained variation).\n\n\n\nThe F Test for \\(H_0\\) vs \\(H_a\\):\n\nIf Between-treatments variation is significantly larger than the Within-treatment variation, reject \\(H_0\\)."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-explained-between-treatments-variation-mstr",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-explained-between-treatments-variation-mstr",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Explained Between-Treatments Variation (MSTR)",
    "text": "One-way ANOVA: Explained Between-Treatments Variation (MSTR)\n\n\n\nThe variation in the sample means is measured by the mean square due to treatments and is denoted by MSTR.\n\n\\[\n\\text{MSTR} = \\frac{\\text{Sum of squares due to treatments (SSTR)}}{\\text{Degrees of freedom of SSTR}} = \\frac{\\sum_{j=1}^k n_j(\\bar{x}_j - \\bar{x})^2}{k - 1}\n\\]\n\\[\n= \\frac{5(62 - 60)^2 + 5(66 - 60)^2 + 5(52 - 60)^2}{3 - 1} = \\frac{520}{2} = 260\n\\]"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-unexplained-within-treatments-variation-mse",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-unexplained-within-treatments-variation-mse",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Unexplained Within-Treatments Variation (MSE)",
    "text": "One-way ANOVA: Unexplained Within-Treatments Variation (MSE)\n\n\nThe variation of the sample observations within each sample is called the mean square error (MSE).\nA measure of sampling variation.\n\n\\[\n\\text{MSE} = \\frac{\\text{Sum of squares due to unexplained error (SSE)}}{\\text{Degrees of freedom of SSE}} = \\frac{\\sum_{j=1}^k (n_j - 1) s_j^2}{n_T - k}\n\\]\n\\[\n= \\frac{(5-1)(27.5) + (5-1)(26.5) + (5-1)(31)}{15-3} = \\frac{340}{12} = 28.33\n\\]\n\\((n_T = 15 \\text{ is the total sample size.})\\)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-test-statistic-and-decision-rule",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-test-statistic-and-decision-rule",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Test Statistic and Decision Rule",
    "text": "One-way ANOVA: Test Statistic and Decision Rule\n\n\nTest statistic:\n\nIf MSTR is large, we will reject \\(H_0\\); but we need to take the sampling variation (MSE) into account.\nF statistic: \\(F = \\frac{MSTR}{MSE}\\)\n\nDecision rule:\n\nMSE is always a good estimate of the common population variance \\(\\sigma^2\\).\nIf \\(H_0\\) is false (not all means are equal), MSTR overestimates \\(\\sigma^2\\) and is expected to be much larger than MSE.\nReject \\(H_0\\) if \\(F = \\frac{MSTR}{MSE}\\) is too large."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-the-f-test",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-the-f-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: The F Test",
    "text": "One-way ANOVA: The F Test\n\n\nThe distribution of the test statistic:\n\n\\[\nF = \\frac{MSTR}{MSE}\n\\]\n\nIf the null hypothesis is true and the ANOVA assumptions are valid, the sampling distribution of the test statistic is an F distribution with \\(k-1\\) (numerator) and \\(n_T - k\\) (denominator) degrees of freedom.\n\n\n\nDecision Rule: Reject \\(H_0\\) if the observed \\(F &gt; F_{\\alpha, k-1, n_T-k}\\) (right-tail),\n\nwhere \\(F_{\\alpha, k-1, n_T-k} = \\text{F.INV}(1-\\alpha, k-1, n_T-k)\\)."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-the-f-test-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-the-f-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: The F Test",
    "text": "One-way ANOVA: The F Test\n\n\n\n\n\n\n\n\n\n\n\n\nObserved F-ratio = 9.18\np-value = 1–F.DIST(9.18,2,12,TRUE) \\(\\approx\\) 0.004"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-decision-rules",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-decision-rules",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Decision Rules",
    "text": "One-way ANOVA: Decision Rules\n\n\n\np-value approach: Reject \\(H_0\\) if \\(p\\)-value \\(\\leq \\alpha\\)\n\n\n\nCritical-value approach: Reject \\(H_0\\) if \\(F \\geq F_{\\alpha}\\)\n\nwhere the critical value \\(F_{\\alpha}\\) is based on an F distribution with \\(k-1\\) numerator d.f. and \\(n_T - k\\) denominator d.f.\n\nIn our example, \\(F_{0.05,1,12} = \\text{F.inv}(0.95,2,12) = 3.89\\), and \\(9.18 &gt; 3.89\\).\n\nConclusion: We have statistically significant evidence that at least one pair of the assembly methods produced different mean numbers of filtration systems per week. The significance level is 5%."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: ANOVA Table",
    "text": "One-way ANOVA: ANOVA Table\n\n\nANOVA can be viewed as the process of partitioning the total sum of squares and the degrees of freedom into their corresponding sources: treatments and error.\nDividing the sum of squares by the appropriate degrees of freedom provides the variance estimates, the \\(F\\) value, and the \\(p\\)-value used to test the hypothesis of equal population means."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: ANOVA Table",
    "text": "One-way ANOVA: ANOVA Table\n\n\n\nAssuming \\(H_0\\) is true, the entire data set is basically one sample from one population, the formula for computing the total sum of squares (SST) is:\n\n\\[\n\\text{SST} = \\sum_{j=1}^k \\sum_{i=1}^{n_j} (x_{ij} - \\bar{x})^2 = (n_T - 1) s_x^2 = 860\n\\]\n\\[\n= \\text{SSTR} + \\text{SSE} = 520 + 340\n\\]\n\nSST has \\(n_T - 1 = 15 - 1 = 14 (= 2 + 12)\\) degrees of freedom."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-2",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: ANOVA Table",
    "text": "One-way ANOVA: ANOVA Table\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(H_a:\\) Not all the means are equal\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of Freedom\nMean Square\n\\(F\\)\np-Value\n\n\n\n\nTreatments\n\\(SSTR\\)\n\\(df_1\\)\n\\(\\text{MSTR}=\\frac{\\text{SSTR}}{df_1}\\)\n\\(\\frac{MSTR}{MSE}\\)\n\\(1-\\text{F.DIST}(F, k-1, n_T-k, \\text{TRUE})\\)\n\n\nError\n\\(SSE\\)\n\\(df_2\\)\n\\(\\text{MSE}=\\frac{\\text{SSE}}{df_2}\\)\n\n\n\n\nTotal\n\\(SST=SSTR+SSE\\)\n\\(df_3=df_1+df_2\\)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-3",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: ANOVA Table",
    "text": "One-way ANOVA: ANOVA Table\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(H_a:\\) Not all the means are equal\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of Freedom\nMean Square\n\\(F\\)\np-Value\n\n\n\n\nTreatments\n\\(\\sum_{j=1}^k n_j (\\bar{x}_j - \\bar{x})^2\\)\n\\(k - 1\\)\n\\(\\frac{\\text{SSTR}}{k - 1}\\)\n\\(\\frac{\\text{MSTR}}{\\text{MSE}}\\)\n\\(1-\\text{F.DIST}(F, k-1, n_T-k, \\text{TRUE})\\)\n\n\nError\n\\(\\sum_{j=1}^k (n_j - 1) s_j^2\\)\n\\(n_T - k\\)\n\\(\\frac{\\text{SSE}}{n_T - k}\\)\n\n\n\n\nTotal\n\\(\\sum_{j=1}^k \\sum_{i=1}^{n_j} (x_{ij} - \\bar{x})^2\\)\n\\(n_T - 1\\)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-4",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-anova-table-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: ANOVA Table",
    "text": "One-way ANOVA: ANOVA Table\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(H_a:\\) Not all the means are equal\n\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares\nDegrees of Freedom\nMean Square\n\\(F\\)\np-Value\n\n\n\n\nTreatments\n520\n2\n260.00\n9.18\n.004\n\n\nError\n340\n12\n28.33\n\n\n\n\nTotal\n860\n14"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-other-applications",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#one-way-anova-other-applications",
    "title": " MGMT 30500: Business Statistics ",
    "section": "One-way ANOVA: Other Applications",
    "text": "One-way ANOVA: Other Applications\n\n\nMean sales of companies using four (4) different advertising methods (spot TV, newspapers, flyers, magazines).\nMean quality levels of products from several vendors.\nYields of several processes (machines).\nMean completion times of different methods.\nMean waiting times of several lines in a bank.\nMean selling price of houses in different school districts.\nMean response time of a service request for several companies.\nMean ages between floor workers, middle managers, and executives.\nMean improvement or learning outcome measures between several training/teaching methods (in-person, online, hybrid, weekend program)."
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel’s ANOVA: Single Factor Tool",
    "text": "Excel’s ANOVA: Single Factor Tool\n\n\nStep 1: Open Assembly.xlsx, and, on Sheet 2, click the Data tab on the Ribbon\nStep 2: In the Analysis group, click Data Analysis\nStep 3: Choose Anova: Single Factor from the list of Analysis Tools\nStep 4: When the Anova: Single Factor dialog box appears: (see details on next slide)"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel’s ANOVA: Single Factor Tool",
    "text": "Excel’s ANOVA: Single Factor Tool"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool-2",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#excels-anova-single-factor-tool-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel’s ANOVA: Single Factor Tool",
    "text": "Excel’s ANOVA: Single Factor Tool\nSummary and Output data"
  },
  {
    "objectID": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#summary-1",
    "href": "lecture_slides/13_chapter_one_way_anova/13_chapter_one_way_anova.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nExperimental Design: allow for control over variables, enabling the establishment of cause-and-effect relationships.\n\nObservational studies do not control variables, limiting conclusions about causality.\n\nANOVA: Is a statistical method used to test for differences in means across multiple groups.\n\nIt partitions the variance into components due to treatments and error, facilitating hypothesis testing.\n\nANOVA Assumptions and Decision Making:\n\nAssumptions include normal distribution, equal variances, and independence of observations.\nDecision rules are based on comparing the calculated F-statistic to a critical value or p-value to accept or reject the null hypothesis."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#overview",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nSimple Linear Regression Model\nLeast Squares Method\nCoefficient of Determination\nModel Assumptions\nTesting for Significance\n\n\n\nExcel’s Regression Tool\nUsing the Estimated Regression Equation for Estimation and Prediction\nResidual Analysis: Validating Model Assumptions\nOutliers and Influential Observations"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#covariance",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#covariance",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\n\nThe Covariance is a measure of the linear association between two variables.\nPositive values indicate a positive relationship.\nNegative values indicate a negative relationship."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#correlation-coefficient",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#correlation-coefficient",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\n\n\n\nCorrelation is a unit-free measure of linear association and not necessarily causation.\nThe coefficient can take on values between −1 and +1.\n\nValues near −1 indicate a strong negative linear relationship.\nValues near +1 indicate a strong positive linear relationship.\n\nThe closer the correlation is to zero, the weaker the linear relationship."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-is-a-model",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-is-a-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What is a model?",
    "text": "What is a model?\n\n\n\n\n\n\n\nAll models are wrong, but some are useful.\n— George Box\n\n\n\n\n\n\n\n\n\n\n\n\nNY City Subway Map"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-does-it-mean-to-model-data",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-does-it-mean-to-model-data",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What does it mean to “model” data?",
    "text": "What does it mean to “model” data?\n\n\nLet’s start with a very simple premise:\n\nto model, we need to make explicit the conditions under which a variable \\(X\\) is related to a variable \\(Y\\).\n\n\n\n\nLet’s begin by giving specific names to these variables:\n\nDependent Variable (DV): This is our phenomenon of interest, usually denoted as \\(Y\\).\nIndependent Variable (IV): This is the phenomenon that explains/describe our dependent variable, generally denoted as \\(X\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-does-it-mean-to-model-data-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#what-does-it-mean-to-model-data-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What does it mean to “model” data?",
    "text": "What does it mean to “model” data?\n\nMathematically, we model \\(Y\\) as a function of \\(X\\). Statistically, modeling can serve two main purposes:\n\nPrediction: The possibility of using the values of \\(X\\) to predict the value of \\(Y\\). There must be a substantive connection between these two variables for one to generate reliable predictions about the values of the other.\nExplanation: Used to understand the connection and significance (both substantive and statistical) of the relationship between two variables. In this case, we aim to accurately estimate the impact of one variable on the other, preferably excluding any potential omitted variables."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Walmart Carton-Mix Optimization Study",
    "text": "Motivation: Walmart Carton-Mix Optimization Study\n\n\nWalmart is the world’s largest retailer with over 245 million customers weekly.\nIn 2000, Walmart launched its online shopping site, Walmart.com.\nA network of distribution centers was created in the US to manage packaging for online orders."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Walmart Carton-Mix Optimization Study",
    "text": "Motivation: Walmart Carton-Mix Optimization Study\n\nCarton-Mix Optimization Study\n\nLocation: Carrollton, Georgia distribution center.\nObjective: Minimize material, labor, and shipping costs by optimizing the size and number of cartons.\nConstraints:\n\nMinimum and maximum carton sizes.\nOne-size carton limit for automatically constructed cartons.\n\nPeak Season: November to December, with over 100,000 packages shipped per day."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Walmart Carton-Mix Optimization Study",
    "text": "Motivation: Walmart Carton-Mix Optimization Study\n\n\nData was collected to develop a cost model for optimizing carton mix.\nDV: material cost of a carton in dollars per carton.\nIV: volume of the carton measured in cubic inches per carton.\nCost Model Formula:\n\n\\[\ny = -0.11 + 0.0014x\n\\]\n\nExample Calculation:\n\nCarton volume: 2800 cubic inches\nEstimated material cost: \\(y = -0.11 + 0.0014 \\times 2800 = 3.81\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-4",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#motivation-walmart-carton-mix-optimization-study-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Walmart Carton-Mix Optimization Study",
    "text": "Motivation: Walmart Carton-Mix Optimization Study\n\nImplementation and Results\n\nThe regression model was used in an optimization algorithm in Microsoft Excel to provide Walmart managers a recommendation on the optimal mix of carton sizes to carry at the distribution center.\n\nResults:\n\nFirst-year savings: $600,000\nEstimated annual savings across all distribution centers: $2 million"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\nManagerial decisions often are based on the relationship between two or more variables.\nRegression analysis can be used to develop an equation (as a conjecture) for the relationship between the variables.\n\nThe variable being predicted is called the dependent variable and is denoted by y.\nThe variables being used to predict the value of the dependent variable are called the independent variables and are denoted by x."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\nSimple linear regression involves one independent variable and one dependent variable.\nThe relationship between the two variables is linear, approximated by a straight line.\n\nAs an initial estimate of the unknown relationship.\nA regression analysis is a trial-and-error exercise.\n\nRegression analysis involving two or more independent variables is called multiple regression."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#regression-objective",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#regression-objective",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Regression Objective",
    "text": "Regression Objective\n\n\nInterpretation: Determine whether variation in the dependent variable can be explained by (the variation of) the independent variable(s) by testing the statistical significance of the independent variable(s):\n\nAs a group (F-test).\nIndividually (t-tests).\n\nPrediction: Study how to predict the dependent variable given the information of the independent variable(s)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#applications",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#applications",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Applications",
    "text": "Applications\n\n\nRevenue vs. Advertising expenditure\nTotal cost vs. production quantity (variable and fixed costs)\nNFL: Annual revenue vs. estimated team value\nTotal points earned vs. hours spent studying\nNumber of days absent vs. distance to work\nNumber of defective parts vs. line speed\nMaintenance cost vs. age of the equipment\nSale price of 2007 Camry vs. mileage\nSales vs. promotion activities and Price\n(Beta risk) Total return of an individual stock vs. total return for the stock market\nOutput vs. input"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-model",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\n\n\nThe equation that describes how y is related to x and an error term is called the regression model.\nThe simple linear regression model for the data is:\n\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nwhere:\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are the unknown parameters of the model,\n\\(\\epsilon\\) is a random variable called the error term (unexplained error), with mean of 0 and unknown error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-equation",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression Equation",
    "text": "Simple Linear Regression Equation\n\n\n\nThe Simple Linear Regression Equation is:\n\n\\[\nE(y) = \\beta_0 + \\beta_1 x\n\\]\nGraph of the regression equation is a straight line.\n\n\\(E(y)\\) is the expected value of y for a given x value\n\\(\\beta_0\\) is the y intercept of the regression line (unknown)\n\\(\\beta_1\\) is the slope of the regression line (unknown)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-equation-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#simple-linear-regression-equation-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Simple Linear Regression Equation",
    "text": "Simple Linear Regression Equation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPositive Linear Relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative Linear Relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Linear Relationship"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#estimated-simple-linear-regression-equation",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#estimated-simple-linear-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimated Simple Linear Regression Equation",
    "text": "Estimated Simple Linear Regression Equation\n\nThe estimated simple linear regression equation:\n\\[\n\\hat{y} = b_0 + b_1 x\n\\]\nThe graph is called the estimated/predicted regression line.\n\n\\(\\hat{y}\\) is the estimated/predicted value of y for a given x value.\n\\(b_0\\) is the y-intercept of the line, estimating \\(\\beta_0\\)\n\\(b_1\\) is the slope of the line, estimating \\(\\beta_1\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#random-error-epsilon-and-observed-residual-e",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#random-error-epsilon-and-observed-residual-e",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Random Error (\\(\\epsilon\\)) and Observed Residual (\\(e\\))",
    "text": "Random Error (\\(\\epsilon\\)) and Observed Residual (\\(e\\))"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#estimation-process",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#estimation-process",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimation Process",
    "text": "Estimation Process"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residualserrors",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residualserrors",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residuals/Errors",
    "text": "Residuals/Errors"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Least Squares Method",
    "text": "Least Squares Method\n\n\n\\(y_i\\) = observed value of the dependent variable for the i-th observation.\n\\(\\hat{y}_i\\) = estimated value of the dependent variable for the i-th observation.\n\\[\n\\text{Error or Residual} = \\text{Observed} - \\text{Predicted} = y_i - \\hat{y}_i\n\\]\n\nLeast Squares Criterion: To minimize the Sum of Squared Errors (SSE)\n\n\\[\n\\min SSE = \\min \\sum (y_i - \\hat{y}_i)^2 = \\min \\sum (y_i - b_0 - b_1 x_i)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Least Squares Method",
    "text": "Least Squares Method\n\n\nSlope for the Estimated Regression Equation\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\n\nwhere:\n\n\\(x_i\\) = value of independent variable for i-th observation\n\\(y_i\\) = value of dependent variable for i-th observation\n\\(\\bar{x}\\) = mean value for independent variable\n\\(\\bar{y}\\) = mean value for dependent variable"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#least-squares-method-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Least Squares Method",
    "text": "Least Squares Method\n\ny-Intercept for the Estimated Regression Equation\n\\[\nb_0 = \\bar{y} - b_1 \\bar{x}\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#regression-modeling-steps",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#regression-modeling-steps",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Regression Modeling Steps",
    "text": "Regression Modeling Steps\n\n\n\nDefine problem and propose model\nMake model assumptions\nCollect data\nDo descriptive data analysis\nObtain the prediction equation and evaluate the model\n\nEstimate the regression coefficients\nCheck adequacy of the overall model\nTest significance of the overall model and the individual independent variables\nResidual Analysis: Validate the model assumptions\n\nSpecial issues (in multiple regression)\n\nMulticollinearity and variables selection\n\nSpecial models and transformations (for model-building)\n(Repeat Steps 1-7 if necessary.)\nPrediction, Implementation, …"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example\n\nExample: Armand’s Pizza Parlor Restaurants\nData was collected from a sample of 10 Armand’s Pizza Parlor Restaurants near college campuses.\nFor the i-th observation or restaurant in the sample:\n\n\\(x_i\\) is the size of the student population\n\\(y_i\\) is the quarterly sales."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example\n\n\n\nRestaurant\nStudent population (1000s)\nQuarterly sales ($1000s)\n\n\n\n\n1\n2\n58\n\n\n2\n6\n105\n\n\n3\n8\n88\n\n\n4\n8\n118\n\n\n5\n12\n117\n\n\n6\n16\n137\n\n\n7\n20\n157\n\n\n8\n20\n169\n\n\n9\n22\n149\n\n\n10\n26\n202"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-excel",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example Excel",
    "text": "Linear Regression: Example Excel\n\nExample: Armand’s Pizza Parlor Restaurants"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example\n\n\n\n\nMetric\nPopulation\nSales\n\n\n\n\nMean\n14\n130\n\n\nStandard Error\n2.512\n13.220\n\n\nMedian\n14\n127.5\n\n\nMode\n8\nN/A\n\n\nStandard Deviation\n7.944\n41.806\n\n\nSample Variance\n63.111\n1747.778\n\n\nKurtosis\n-1.332\n-0.033\n\n\nSkewness\n0\n-0.014\n\n\nRange\n24\n144\n\n\nMinimum\n2\n58\n\n\nMaximum\n26\n202\n\n\nSum\n140\n1300\n\n\nCount\n10\n10\n\n\nConfidence Level (95%)\n5.683\n29.907"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-excel-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-excel-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example Excel",
    "text": "Linear Regression: Example Excel\n\n\n\n\n\nProducing a Scatter Diagram\n\nStep 1: Select cells B2:C11\nStep 2: Click the Insert tab on the Ribbon\nStep 3: In the Charts group, select Insert Scatter (X,Y) or Bubble Chart\nStep 4: When the list of scatter diagram subtypes appears, select Scatter (chart in upper left corner)\n\n\n\n\n\n\n\n\nEditing a Scatter Diagram\n\nStep 1: Click the Chart Title and replace it with Armand’s Pizza Parlors\nStep 2: Click the Chart Elements button\nStep 3: When the list of chart elements appears:\n\nClick Axis Titles (creates placeholders for titles)\nClick Gridlines (to deselect gridlines option)\nClick Trendline\n\nStep 4: Click the horizontal Axis Title and replace it with Student population (1000s)\nStep 5: Click the Vertical (Value) Axis Title and replace it with Quarterly Sales ($1000s)\nStep 6: Select the Format Trendline option\nStep 7: When the Format Trendline dialog box appears:\n\nSelect Display equation on chart\nClick the Fill & Line button\nIn the Dash type box, select Solid\nClose the Format Trendline dialog box"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-4",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\n\\(x_i\\)\n\\(y_i\\)\n\\(x_i - \\bar{x}\\)\n\\(y_i - \\bar{y}\\)\n\\((x_i - \\bar{x})(y_i - \\bar{y})\\)\n\\((x_i - \\bar{x})^2\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n1\n2\n58\n-12\n-72\n864\n144\n5184\n\n\n2\n6\n105\n-8\n-25\n200\n64\n625\n\n\n3\n8\n88\n-6\n-42\n252\n36\n1764\n\n\n4\n8\n118\n-6\n-12\n72\n36\n144\n\n\n5\n12\n117\n-2\n-13\n26\n4\n169\n\n\n6\n16\n137\n2\n7\n14\n4\n49\n\n\n7\n20\n157\n6\n27\n162\n36\n729\n\n\n8\n20\n169\n6\n39\n234\n36\n1521\n\n\n9\n22\n149\n8\n19\n152\n64\n361\n\n\n10\n26\n202\n12\n72\n864\n144\n5184\n\n\nTotal\n140\n1300\n-\n-\n2840\n568\n15730\n\n\n\n\nSample correlation:\n\\[\nr_{xy} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}} = \\frac{2840}{\\sqrt{568 \\times 15730}} = 0.9501\n\\]\n\n= correl(x-array, y-array)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-5",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#linear-regression-example-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Regression: Example",
    "text": "Linear Regression: Example\n\n\n\nSlope for the Estimated Regression Equation\n\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\frac{2840}{568} = 5\n\\]\n\ny-Intercept for the Estimated Regression Equation\n\n\\[\nb_0 = \\bar{y} - b_1 \\bar{x} = 130 - 5(14) = 60\n\\]\n\nEstimated Regression Equation\n\n\\[\n\\hat{y} = 60 + 5x\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#remarks",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#remarks",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Remarks",
    "text": "Remarks\n\n\n\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y}) / (n - 1)}{\\sum (x_i - \\bar{x})^2 / (n - 1)} = \\frac{s_{xy}}{s_x^2}\n\\]\n\\[\n= \\frac{s_{xy}}{s_x s_y} \\cdot \\frac{s_y}{s_x} = r_{xy} \\frac{s_y}{s_x}\n\\]\n\\(b_1\\), \\(s_{xy}\\), and \\(r_{xy}\\) have the same signs, reflecting the relationship’s direction between \\(x\\) and \\(y\\).\n\n\n\n\n\n\\(b_1\\): Slope of the regression line, indicating the change in \\(y\\) for a unit change in \\(x\\).\n\\(\\sum (x_i - \\bar{x})(y_i - \\bar{y})\\): Numerator for covariance, showing the joint deviation of \\(x\\) and \\(y\\) from their means.\n\\(\\sum (x_i - \\bar{x})^2\\): Denominator representing variance of \\(x\\).\n\\(s_{xy}\\): Sample covariance, indicating how \\(x\\) and \\(y\\) vary together.\n\\(s_x^2\\): Sample variance of \\(x\\), measuring the dispersion of \\(x\\) around its mean.\n\\(s_x\\) and \\(s_y\\): Standard deviations of \\(x\\) and \\(y\\), showing spread around their means.\n\\(r_{xy}\\): Correlation coefficient, measuring strength and direction of the linear relationship between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Partition of Total Variation",
    "text": "Partition of Total Variation"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Partition of Total Variation",
    "text": "Partition of Total Variation\n\n\nPrediction equation: \\(\\hat{y} = 60 + 5x\\)\nFor Observation #1 \\((x = 2, y = 58)\\): \\(\\hat{y} = 60 + 5(2) = 70\\)\n\nUnexplained error/residual = \\(y - \\hat{y} = 58 - 70 = -12\\)\n\n\n\n\nNote:\n\n\\[\ny - \\bar{y} = (\\hat{y} - \\bar{y}) + (y - \\hat{y})\n\\]\n\\[\n58 - 130 = (70 - 130) + (58 - 70)\n\\]\n\\[\n\\text{Total} = \\text{Explained} + \\text{Unexplained}\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#partition-of-total-variation-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Partition of Total Variation",
    "text": "Partition of Total Variation\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(x_i\\)\nObserved Y\nPredicted Y\nTotal\nExplained\nUnexplained\n\n\n\n\n1\n2\n58\n70\n-72\n-60\n-12\n\n\n2\n6\n105\n90\n-25\n-40\n15\n\n\n3\n8\n88\n100\n-42\n-30\n-12\n\n\n4\n8\n118\n100\n-12\n-30\n18\n\n\n5\n12\n117\n120\n-13\n-10\n-3\n\n\n6\n16\n137\n140\n7\n10\n-3\n\n\n7\n20\n157\n160\n27\n30\n-3\n\n\n8\n20\n169\n160\n39\n30\n9\n\n\n9\n22\n149\n170\n19\n40\n-21\n\n\n10\n26\n202\n190\n72\n60\n12\n\n\n\nMean Y\n130\nSum of square\n15730\n14200\n1530\n\n\n\n\n\n\nSST\nSSR\nSSE"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\n\nRelationship Among SST, SSR, SSE\n\n\\[\n\\text{SST} = \\text{SSR} + \\text{SSE}\n\\]\n\\[\n\\sum(y_i - \\bar{y})^2 = \\sum(\\hat{y}_i - \\bar{y})^2 + \\sum(y_i - \\hat{y}_i)^2\n\\]\n\\[\n\\text{Degrees of freedom:} \\quad n-1 \\quad\\quad\\quad\\quad\\quad 1 \\quad\\quad\\quad\\quad\\quad n-2 \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n\\]\nwhere:\n\nSST = total sum of squares (Total variation)\nSSR = sum of squares due to regression (Explained variation)\nSSE = sum of squares due to error (Unexplained variation)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\n\nThe coefficient of determination is:\n\n\\[\nR^2 = \\frac{\\text{Explained variation}}{\\text{Total variation}} = \\frac{\\text{SSR}}{\\text{SST}}\n\\]\nwhere:\n\nSSR = sum of squares due to regression\nSST = total sum of squares"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\nExample: Armand’s Pizza Parlor Restaurants\n\n\\[\nR^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{14200}{15730} = 0.9027 = 90.27\\%\n\\]\n\\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}} = 1 - \\frac{1530}{15730} = 1 - 0.973 = 90.27\\%\n\\]\n\nThe regression relationship is very strong;\n90.27% of the variability in the sales can be explained by (the variability of) the size of the student population through the proposed linear relationship between them."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-excel",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#coefficient-of-determination-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Coefficient of Determination: Excel",
    "text": "Coefficient of Determination: Excel\n\n\n\n\n\nAdding \\(R^2\\) Value to Scatter Diagram\n\nStep 1: Right-click on the trendline and select the Format Trendline option\nStep 2: When the Format Trendline dialog box appears:\n\nSelect Display R-squared on chart\nClose the Format Trendline dialog box"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sample-correlation-coefficient",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sample-correlation-coefficient",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Correlation Coefficient",
    "text": "Sample Correlation Coefficient\n\n\\[\nr_{xy} = (\\text{sign of } b_1) \\sqrt{\\text{Coefficient of Determination}}\n\\]\n\\[\nr_{xy} = (\\text{sign of } b_1) \\sqrt{R^2}\n\\]\nwhere:\n\n\\(b_1\\) = the slope of the estimated regression equation \\(\\hat{y} = b_0 + b_1x\\)\n\nOnly in simple regression."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sample-correlation-coefficient-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sample-correlation-coefficient-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sample Correlation Coefficient",
    "text": "Sample Correlation Coefficient\n\nExample: Armand’s Pizza Parlor Restaurants\n\\[\nr_{xy} = (\\text{sign of } b_1) \\sqrt{R^2}\n\\]\nThe sign of \\(b_1\\) in the equation \\(\\hat{y} = 10 + 5x\\) is “+”.\n\\[\nr_{xy} = +\\sqrt{0.9027}\n\\]\n\\[\nr_{xy} = +0.9501\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#assumptions-about-the-error-term-epsilon",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#assumptions-about-the-error-term-epsilon",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Assumptions About the Error Term \\(\\epsilon\\)",
    "text": "Assumptions About the Error Term \\(\\epsilon\\)\n\nThe error term \\(\\epsilon\\) is a random variable with a mean or expected value of zero; that is, \\(E(\\epsilon) = 0\\).\n\nImplication: \\(\\beta_0\\) and \\(\\beta_1\\) are constants; therefore \\(E(\\beta_0) = \\beta_0\\) and \\(E(\\beta_1) = \\beta_1\\); thus, for a given value of \\(x\\), the expected value of \\(y\\) is:\n\n\n\n\\[\n   E(y) = \\beta_0 + \\beta_1x\n   \\]\n\n\nThe variance of \\(\\epsilon\\), denoted by \\(\\sigma^2\\), is the same for all values of \\(x\\).\n\nImplication: The variance of \\(y\\) about the regression line equals \\(\\sigma^2\\) and is the same for all values of \\(x\\).\n\nThe values of \\(\\epsilon\\) are independent.\n\nImplication: The value of \\(\\epsilon\\) for a particular value of \\(x\\) is not related to the value of \\(\\epsilon\\) for any other value of \\(x\\); thus, the value of \\(y\\) for a particular value of \\(x\\) is not related to the value of \\(y\\) for any other value of \\(x\\).\n\nThe error term \\(\\epsilon\\) is a normally distributed random variable for all values of \\(x\\).\n\nImplication: Because \\(y\\) is a linear function of \\(\\epsilon\\), \\(y\\) is also a normally distributed random variable for all values of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#assumptions-about-the-error-term-epsilon-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#assumptions-about-the-error-term-epsilon-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Assumptions About the Error Term \\(\\epsilon\\)",
    "text": "Assumptions About the Error Term \\(\\epsilon\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Significance",
    "text": "Testing for Significance\n\n\n\nTo test for a significant regression relationship, we must conduct a hypothesis test to determine whether the unknown \\(\\beta_1\\) is zero.\nTwo equivalent Tests are commonly used in simple regression:\n\n\\(t\\) Test\n\n\\(F\\) test\n\nBoth the \\(t\\) test and \\(F\\) test require an estimate of the unknown error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Significance",
    "text": "Testing for Significance\n\n\nIdea: Estimating the population error variance (\\(\\sigma^2\\)) by the sample residual variance (denoted by \\(s^2\\) or MSE).\nThe mean square error (MSE), or \\(s^2\\) is:\n\\[\ns^2 = \\text{MSE} = \\frac{\\text{SSE}}{(n-2)}, \\quad \\text{SSE = Sum of Squared Errors (unexplained variation).}\n\\]\nwhere:\n\\[\n\\text{SSE} = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-significance-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Significance",
    "text": "Testing for Significance\n\n\nAn Estimate of \\(\\sigma\\)\n\nTo estimate \\(\\sigma\\), we take the square root of \\(s^2\\).\nThe resulting \\(s\\) is called the standard error (of the estimate).\n\n\n\\[\ns = \\sqrt{MSE} = \\sqrt{\\frac{SSE}{n - 2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\n\n\n\nHypotheses\n\\[ H_0 : \\beta_1 = 0 \\quad\\] The proposed linear model is insignificant; No linear relationship; the predictor is insignificant; the predictor explains no variation in the response.\n\\[ H_a : \\beta_1 \\neq 0 \\]\nTest Statistic\n\n\\[\nF = \\frac{SSR / 1}{SSE / (n-2)} = \\frac{MSR}{MSE} \\quad \\text{follows } F(1, n-2), \\text{ if } H_0 \\text{ is true.}\n\\] where\n\nSSR (Sum of Squares for Regression): measures how much of the variation in the dependent variable is explained by the independent variable(s).\nMSR (Mean Square Regression): is the average variation explained by the model per degree of freedom for the regression.In the case above, we have only one independent variable as the degree of freedom for the regression (denominator).\nSSE (Sum of Squares for Error/Residuals): measures the variation in the response variable that is not explained by the model.\nMSE (Mean Square Error): is the average variation that remains unexplained by the model, per degree of freedom for the error. The degrees of freedom for the error is the number of observations (\\(n\\)) in the data set minus the number of parameters being estimated (\\(\\beta_0\\) and \\(\\beta_1\\))."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\n\n\nDecision Rules (right-tail):\n\nReject \\(H_0\\) if \\(p\\text{-value} \\leq \\alpha\\), or \\(F \\geq F_{\\alpha, 1, n-2}\\)\n\n\n\nwhere:\n\n\\(F_{\\alpha, 1, n-2}\\) is based on an \\(F\\) distribution with 1 degree of freedom in the numerator and \\(n - 2\\) degrees of freedom in the denominator.\n\\(F_{\\alpha, 1, n-2} = F.\\text{INV}(1-\\alpha, 1, n - 2)\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-overall-significance-f-test-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\n\n\nCompute the value of the test statistic.\n\n\\[\nF = \\frac{SSR / 1}{SSE / (n-2)} = \\frac{14,200 / 1}{1,530 / (10-2)} = 74.25\n\\]\n\n\nDecision.\n\n\\(F_{\\alpha} = F.\\text{INV}(0.99,1,8) = 11.26\\) and \\(F = 74.25 &gt; 11.26.\\) We reject \\(H_0.\\)\n\\(p\\text{-value} = 1 - F.\\text{DIST}(74.25,1,8, \\text{TRUE}) \\approx 0.000 &lt; 1\\% = \\alpha\\)\n\n\n\n\nThe statistical evidence is sufficient to conclude that a significant relationship exists between the size of the student population and quarterly sales. The significance level is \\(\\alpha = 1\\%\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\n\n\nIn regression, \\(b_1\\) follows a normal distribution with mean \\(\\beta_1\\).\n\n\nHypotheses\n\\(H_0 : \\beta_1 = 0 \\quad (\\text{Independent variable x is insignificant})\\)\n\\(H_a : \\beta_1 \\neq 0\\)\n\n\n\nTest Statistic\n\n\\[\nt = \\frac{b_1 - 0}{s_{b_1}}, \\quad \\text{where} \\quad s_{b_1} = \\frac{s}{\\sqrt{\\sum(x_i - \\bar{x})^2}}, \\quad \\text{follows } t(n-2), \\text{ if } H_0 \\text{ is true.}\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\n\n\n\nDecision Rules (two-tail):\n\nReject \\(H_0\\) if \\(p\\text{-value} \\leq \\alpha\\)\nReject \\(H_0\\) if \\(t \\leq -t_{\\alpha/2,n-2}\\) or \\(t \\geq t_{\\alpha/2,n-2}\\) (i.e., \\(|t| &gt; t_{\\alpha/2,n-2}\\))\n\n\nwhere the critical value, \\(t_{\\alpha/2, n-2}\\), is based on a \\(t\\) distribution with \\(n - 2\\) df."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#testing-for-individual-significance-t-test-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\n\n\n\nCompute the value of the test statistic:\n\n\\[\nt = \\frac{b_1 - 0}{s_{b_1}} = \\frac{5 - 0}{.5803} = 8.62\n\\] \n\nDecision:\n\n\\(t_{0.01/2, 8} = T.\\text{INV}(0.995,8) = 3.355\\) provides an area of 0.005 in the upper tail of a \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\nBecause \\(|t| = 8.62 &gt; 3.355\\), we can reject \\(H_0\\) at \\(\\alpha = 1\\%\\).\n\\(p\\text{-value} = 2*(1-T.\\text{DIST}(8.62,10-2,\\text{TRUE})) \\approx 0.000&lt; 1\\%\\).\n\nWe can reject \\(H_0\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(\\beta_1\\)",
    "text": "Confidence Interval for \\(\\beta_1\\)\n\n\nWe can use a 99% confidence interval for \\(\\beta_1\\) to test the hypotheses just used in the t test.\n\\(H_0\\) is rejected if the hypothesized value of \\(\\beta_1\\) is not included in the corresponding confidence interval for \\(\\beta_1\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(\\beta_1\\)",
    "text": "Confidence Interval for \\(\\beta_1\\)\n\n\n\nThe form of a confidence interval for \\(\\beta_1\\) is:\n\n\\[\nb_1 \\pm t_{\\alpha/2, n-2} s_{b_1}\n\\]\nwhere:\n\n\\(b_1\\) is the point estimator,\n\\(t_{\\alpha/2, n-2} s_{b_1}\\) is the margin of error (MOE),\n\\(t_{\\alpha/2, n-2}\\) is the t value providing an upper-tail area of \\(\\alpha/2\\) in a t distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-beta_1-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(\\beta_1\\)",
    "text": "Confidence Interval for \\(\\beta_1\\)\n\n\n\n99% Confidence Interval for \\(\\beta_1\\):\n\n\\[\nb_1 \\pm t_{\\alpha/2} s_{b_1} = 5 \\pm 3.355(.5803) = 5 \\pm 1.95\n\\]\nor 3.05 to 6.95\n\nConclusion: 0 is not included in the 99% confidence interval. Reject \\(H_0\\) at \\(\\alpha\\) = 1%."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#some-cautions-about-the-interpretation-of-significance-tests",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#some-cautions-about-the-interpretation-of-significance-tests",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Some Cautions about the Interpretation of Significance Tests",
    "text": "Some Cautions about the Interpretation of Significance Tests\n\n\n\nRejecting \\(H_0: \\beta_1 = 0\\) and concluding that the relationship between x and y is significant does not enable us to conclude that a cause-and-effect relationship is present between x and y.\nBecause we are able to reject \\(H_0: \\beta_1 = 0\\) and demonstrate statistical significance does not enable us to conclude that there is a linear relationship between x and y."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#excels-regression-tool-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#excels-regression-tool-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Excel’s Regression Tool",
    "text": "Excel’s Regression Tool\n\n\n\nExcel also has a comprehensive tool in its Data Analysis package called Regression.\nThe Regression tool can be used to perform a complete regression analysis."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel’s Regression Tool",
    "text": "Using Excel’s Regression Tool\n\nPerforming the Regression Analysis\n\n\n\n\n\n\nStep 1 Click the DATA tab on the Ribbon\nStep 2 In the Analyze group, click Data Analysis\nStep 3 Choose Regression from the list of Analysis Tools\n\n\n\n\n\n\n\n\nStep 4 When the Regression dialog box appears:\n\nEnter C1:C11 in the Input Y Range box\nEnter B1:B11 in the Input X Range box\nSelect the check box for Labels\nSelect the check box for Confidence Level\nEnter 99 in the Confidence Level box\nSelect Output Range\nEnter A13 in the Output Range box\nClick OK"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel’s Regression Tool",
    "text": "Using Excel’s Regression Tool\n\nExample: Armand’s Pizza Parlors: Regression tool dialog box"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel’s Regression Tool",
    "text": "Using Excel’s Regression Tool\n\nExample: Armand’s Pizza Parlors: Regression tool dialog box"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excels-regression-tool-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel’s Regression Tool",
    "text": "Using Excel’s Regression Tool\n\n\n\n\n\n\n\n\nRegression Statistics:\n\nMultiple R: 0.9501. Indicates a strong positive correlation between observed and predicted values.\nR Square (Coefficient of Determination): Approximately 90.27% of the variance in the dependent variable (Sales) is explained by the independent variable (Population).\nAdjusted R Square: 0.8906. Adjusts the R Square for the number of predictors, providing a more accurate measure.\nStandard Error: 13.8293. Measures the typical distance that the observed values fall from the regression line.\n\n\nANOVA\n\nRegression SS: 14200. Variability explained by the model.\n\nResidual SS: 1530. Variability not explained by the model.\nF-Statistic: 74.2484. Indicates the model is statistically significant.\nSignificance F: 2.55E-05. \\(p\\)-value indicating strong evidence against the null hypothesis.\n\n\n\nModel Results\n\nCoefficients:\n\nIntercept: 60 (p-value: 0.0002)\n\nPopulation: 5 (p-value: 2.55E-05)\n\nBoth coefficients are statistically significant.\n\nConfidence Intervals:\n\n95% CI for Intercept: [38.7247, 81.2753]\n95% CI for Population: [3.6619, 6.3381]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-the-estimated-regression-equation-for-estimation-and-prediction-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-the-estimated-regression-equation-for-estimation-and-prediction-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using the Estimated Regression Equation for Estimation and Prediction",
    "text": "Using the Estimated Regression Equation for Estimation and Prediction\n\n\nA confidence interval is an interval estimate of the mean of all values of \\(y\\) for a given value of \\(x\\).\nA prediction interval is used whenever we want to predict an individual value of \\(y\\) for a new randomly-chosen observation corresponding to a given value of \\(x\\).\nThe given \\(x\\)-value is denoted by \\(x^*\\) , and the corresponding point estimate is obtained from the estimated regression equation:\n\n\\[\n\\hat{y}^* = b_0 + b_1 x^*\n\\]\n\nThe margin of error (MOE) is larger for a prediction interval."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-the-estimated-regression-equation-for-estimation-and-prediction-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-the-estimated-regression-equation-for-estimation-and-prediction-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using the Estimated Regression Equation for Estimation and Prediction",
    "text": "Using the Estimated Regression Equation for Estimation and Prediction\n\n\nConfidence Interval Estimate of the mean, \\(E(\\hat{y}^*)\\):\n\n\\[\n\\hat{y}^* \\pm t_{\\alpha/2, n-2} s_{\\hat{y}^*}\n\\]\n\nPrediction Interval Estimate of individual \\(\\hat{y}^*\\):\n\n\\[\n\\hat{y}^* \\pm t_{\\alpha/2, n-2} s_{pred}\n\\]\nwhere:\n\nconfidence coefficient is \\(1-\\alpha\\) and \\(t_{\\alpha/2, n-2}\\) is based on a t distribution with \\(n-2\\) degrees of freedom."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#point-estimation",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#point-estimation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Point Estimation",
    "text": "Point Estimation\n\nExample: Armand’s Pizza Parlors\nTo predict quarterly sales or expected quarterly sales of restaurants near a campus or campuses with 10,000 students (\\(x* = 10\\)),\n\\[\n\\hat{y}^* = 60 + 5(10) = 110\n\\]\nPredicted quarterly sales of $110,000"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sums-of-squares",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#sums-of-squares",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Sums of Squares",
    "text": "Sums of Squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(y_i\\)\n\\(x_i - \\bar{x}\\)\n\\(y_i - \\bar{y}\\)\n\\((x_i - \\bar{x})(y_i - \\bar{y})\\)\n\\((x_i - \\bar{x})^2\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n1\n2\n58\n-12\n-72\n864\n144\n5184\n\n\n2\n6\n105\n-8\n-25\n200\n64\n625\n\n\n3\n8\n88\n-6\n-42\n252\n36\n1764\n\n\n4\n8\n118\n-6\n-12\n72\n36\n144\n\n\n5\n12\n117\n-2\n-13\n26\n4\n169\n\n\n6\n16\n137\n2\n7\n14\n4\n49\n\n\n7\n20\n157\n6\n27\n162\n36\n729\n\n\n8\n20\n169\n6\n39\n234\n36\n1521\n\n\n9\n22\n149\n8\n19\n152\n64\n361\n\n\n10\n26\n202\n12\n72\n864\n144\n5184\n\n\nTotal\n140\n1300\n\n\n2840\n568\n15730\n\n\n\n\n\\(\\bar{x} = 14, \\bar{y} = 130\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(E(\\hat{y}^*)\\)",
    "text": "Confidence Interval for \\(E(\\hat{y}^*)\\)\n\nEstimate of the Standard Deviation of \\(\\hat{y}^*\\):\n\\[\ns_{\\hat{y}^*} = s \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\n\\]\n\\[\n= 13.829 \\sqrt{\\frac{1}{10} + \\frac{(10 - 14)^2}{568}}\n\\]\n\\[\n= 13.829 \\sqrt{.1282} = 4.95\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(E(\\hat{y}^*)\\)",
    "text": "Confidence Interval for \\(E(\\hat{y}^*)\\)\n95% confidence interval of the mean quarterly sales for all Armand’s restaurants located near campuses with 10,000 students is\n\\[\n\\hat{y}^* \\pm t_{\\alpha/2} s_{\\hat{y}^*}\n\\]\n\\[\n110 \\pm 2.306(4.95)\n\\]\n\\[\n110 \\pm 11.415, \\text{ or } \\$98,585 \\text{ to } \\$121,415\n\\]\nWe are 95% confident that the mean quarterly sales for all Armand’s restaurants located near campuses with 10,000 students falls between $98,585 and $121,415."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#confidence-interval-for-ehaty-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Confidence Interval for \\(E(\\hat{y}^*)\\)",
    "text": "Confidence Interval for \\(E(\\hat{y}^*)\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Prediction Interval for \\(y^*\\)",
    "text": "Prediction Interval for \\(y^*\\)\n\nEstimate of the Standard Deviation of an Individual Value of \\(y^*\\)\n\\[\ns_{pred} = s \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}}\n\\]\n\\[\ns_{pred} = 13.829 \\sqrt{1 + \\frac{1}{10} + \\frac{(10 - 14)^2}{568}}\n\\]\n\\[\n= 13.829 \\sqrt{1.282} = 14.69\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Prediction Interval for \\(y^*\\)",
    "text": "Prediction Interval for \\(y^*\\)\n\n95% prediction interval for quarterly sales for the new Armand’s restaurant located near a campus with 10,000 students is\n\\[\n\\hat{y}^* \\pm t_{\\alpha/2} s_{pred}\n\\]\n\\[\n110 \\pm 2.306(14.69)\n\\]\n\\[\n110 \\pm 33.875, \\text{ or } \\$76,125 \\text{ to } \\$143,875\n\\]\nIf we randomly select one Armand’s restaurant located near a campus with 10,000 students, we are 95% confident that its quarterly sales will be between $76,125 and $143,875."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#prediction-interval-for-y-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Prediction Interval for \\(y^*\\)",
    "text": "Prediction Interval for \\(y^*\\)"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#comments",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#comments",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Comments",
    "text": "Comments\n\n\n\nPrediction intervals for individuals are wider than the corresponding confidence intervals for means.\nTwo \\(x\\)-values with the same distance to the \\(\\bar{x}\\), their corresponding confidence intervals have the same width (and hence, the same margin of error). True also for the prediction intervals.\nThe confidence interval is wider when the given \\(x\\) is further away from the \\(\\bar{x}\\) – It is more difficult to predict for outlying \\(x\\)’s."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-analysis-validating-model-assumptions-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-analysis-validating-model-assumptions-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Analysis: Validating Model Assumptions",
    "text": "Residual Analysis: Validating Model Assumptions\n\n\nIf the assumptions about the error term \\(\\epsilon\\) appear questionable, the hypothesis tests about the significance of the regression relationship and the interval estimation results may not be valid.\nThe residuals provide the best information about \\(\\epsilon\\).\nResidual for observation \\(i\\)\n\n\\[\ny_i - \\hat{y}_i\n\\]\n\nMuch of the residual analysis is based on an examination of graphical plots."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#model-assumptions-and-validation-procedures-with-diagnosis-plots",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#model-assumptions-and-validation-procedures-with-diagnosis-plots",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Model Assumptions and Validation Procedures with Diagnosis Plots",
    "text": "Model Assumptions and Validation Procedures with Diagnosis Plots\n\n\\(E(\\epsilon) = 0\\).\n\nValidation procedure: A plot of residuals against the predicted values of the dependent variable \\(y\\).\n\nThe variance of \\(\\epsilon\\), denoted by \\(\\sigma^2\\), is the same for all values of \\(x\\).\n\nValidation procedure: A plot of the residuals against values of the independent variable \\(x\\).\n\nThe values of \\(\\epsilon\\) are independent.\n\nValidation procedure: A standardized residual plot.\n\nThe error term \\(\\epsilon\\) has a normal distribution.\n\nValidation procedure: A normal probability plot."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Plot Against \\(x\\)",
    "text": "Residual Plot Against \\(x\\)\n\nIf the assumption that the variance of \\(\\epsilon\\) is the same (constant) for all values of \\(x\\) is valid, and the assumed regression model is an adequate representation of the relationship between the variables, then the residual plot should give an overall impression of a horizontal band of points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel C"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Plot Against \\(x\\)",
    "text": "Residual Plot Against \\(x\\)\nExample: Armand’s Pizza Parlors\n\n\n\n\n\n\n\n\n\nStudent Population (\\(x_i\\))\nSales (\\(y_i\\))\nPredicted sales \\(y_i = 60 + 5(x_i)\\)\nResiduals (\\(y_i - \\hat{y_i}\\))\n\n\n\n\n2\n58\n70\n-12\n\n\n6\n105\n90\n15\n\n\n8\n88\n100\n-12\n\n\n8\n118\n100\n18\n\n\n12\n117\n120\n-3\n\n\n16\n137\n140\n-3\n\n\n20\n157\n160\n-3\n\n\n20\n169\n160\n9\n\n\n22\n149\n170\n-21\n\n\n26\n202\n190\n12"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Plot Against \\(x\\)",
    "text": "Residual Plot Against \\(x\\)\nExample: Armand’s Pizza Parlors\nPlot of the residuals against the independent variable \\(x\\).\n\n\n\nThe residuals appear to approximate the horizontal pattern in Panel A.\nBased on this subjective evaluation, we conclude that the residual plot does not provide evidence that the assumptions made for Armand’s regression model should be challenged.\nWe can be confident that Armand’s simple regression model is valid."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x---excel",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-x---excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Plot Against \\(x\\) - Excel",
    "text": "Residual Plot Against \\(x\\) - Excel\n\n\n\n\nUsing Excel to Produce a Residual Plot\n\nThe steps outlined earlier to obtain the regression output are performed with one change.\nWhen the Regression dialog box appears, we must also select the Residual Plot option.\nThe output will include two new items:\n\nA plot of the residuals against the independent variable, and\nA list of predicted values of \\(y\\) and the corresponding residual values."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-haty",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#residual-plot-against-haty",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Plot Against \\(\\hat{y}\\)",
    "text": "Residual Plot Against \\(\\hat{y}\\)\nExample: Armand’s Pizza Parlors\nPlot of the residuals against the independent variable \\(\\hat{y}\\).\n\n\n\nThe pattern of this residual plot is the same as the pattern of the residual plot against the independent variable \\(x\\).\nIt is not a pattern that would lead us to question the model assumptions.\nFor simple linear regression, both the residual plot against \\(x\\) and the residual plot against \\(\\hat{y}\\) provide the same pattern.\nFor multiple regression analysis, the residual plot against \\(\\hat{y}\\) is more widely used because of the presence of more than one independent variable."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residuals",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residuals",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\nStandardized Residual for Observation \\(i\\)\n\n\\[\n\\frac{y_i - \\hat{y_i}}{s_{y_i - \\hat{y_i}}}\n\\]\nwhere \\(s_{y_i - \\hat{y_i}}\\) is the standard deviation of residual \\(i\\)\n\\[\ns_{y_i - \\hat{y_i}} = s \\sqrt{1 + h_i}\n\\] where \\(s\\) is the standard error of the estimate\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n\\]"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual-plot",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual-plot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residual Plot",
    "text": "Standardized Residual Plot\n\nThe standardized residual plot can provide insight about the assumption that the error term \\(\\epsilon\\) has a normal distribution.\nIf this assumption is satisfied, the distribution of the standardized residuals should appear to come from a standard normal probability distribution.\n\nEmpirical rule applies."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residual",
    "text": "Standardized Residual\nExample: Armand’s Pizza Parlors\n\n\n\n\n\n\n\n\n\nObservation\nPredicted sales \\(y_i = 60 + 5(x_i)\\)\nResiduals (\\(y_i - \\hat{y_i}\\))\nStandardized Residual\n\n\n\n\n1\n70\n-12\n-1.0792\n\n\n2\n90\n15\n1.2224\n\n\n3\n100\n-12\n-.9487\n\n\n4\n100\n18\n1.4230\n\n\n5\n120\n-3\n-.2296\n\n\n6\n140\n-3\n-.2296\n\n\n7\n160\n-3\n-.2372\n\n\n8\n160\n9\n.7115\n\n\n9\n170\n-21\n-1.7114\n\n\n10\n190\n12\n1.0792"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual-plot-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#standardized-residual-plot-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residual Plot",
    "text": "Standardized Residual Plot\nExample: Armand’s Pizza Parlors\n\n\nAll of the standardized residuals are between -2 and +2 indicating that there is no reason to question the assumption that ε has a normal distribution."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excel-to-construct-a-standardized-residual-plot",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#using-excel-to-construct-a-standardized-residual-plot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel to construct a Standardized Residual Plot",
    "text": "Using Excel to construct a Standardized Residual Plot\nExample: Armand’s Pizza Parlors"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Plot",
    "text": "Normal Probability Plot\nAnother approach for determining the validity of the assumption that the error term has a normal distribution.\nTo show how a normal probability plot is developed, we introduce the concept of normal scores:\n\nSuppose 10 values are selected randomly from a normal probability distribution with a mean of zero and a standard deviation of one, and that the sampling process is repeated over and over with the values in each sample of 10 ordered from smallest to largest. For now, let us consider only the smallest value in each sample. The random variable representing the smallest value obtained in repeated sampling is called the first-order statistic.\n\n\nFor samples of size 10 from a standard normal probability distribution, the expected value of the first-order statistic is -1.55. This expected value is called a normal score. For the case with a sample of size \\(n = 10\\), there are 10 order statistics and 10 normal scores (next slide). In general, a data set consisting of \\(n\\) observations will have \\(n\\) order statistics and hence \\(n\\) normal scores.\n\nNormal Scores: are expected values of the order statistics from a standard normal distribution."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Plot",
    "text": "Normal Probability Plot\nNormal Scores for \\(n = 10\\)\n\n\n\n\nOrder Statistic\nNormal Score\n\n\n\n\n1\n-1.55\n\n\n2\n-1.00\n\n\n3\n-0.65\n\n\n4\n-0.37\n\n\n5\n-0.12\n\n\n6\n0.12\n\n\n7\n0.37\n\n\n8\n0.65\n\n\n9\n1.00\n\n\n10\n1.55"
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Plot",
    "text": "Normal Probability Plot\n\nTo determine whether the standardized residuals for Armand’s Pizza Parlors appear to come from a standard normal probability distribution.\nWe order the 10 standardized residuals in parallel with the 10 normal scores:\n\n\n\nNormal Scores\nOrdered Standardized Residuals\n\n\n\n\n-1.55\n-1.7114\n\n\n-1.00\n-1.0792\n\n\n-0.65\n-0.9487\n\n\n-0.37\n-0.2372\n\n\n-0.12\n-0.2296\n\n\n0.12\n-0.2296\n\n\n0.37\n0.7115\n\n\n0.65\n1.0792\n\n\n1.00\n1.2224\n\n\n1.55\n1.4230\n\n\n\nInterpreting the Normal Probability Plot: If the normality assumption is satisfied, the smallest standardized residual should be close to the smallest normal score, the next smallest standardized residual should be close to the next smallest normal score, and so on."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-3",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#normal-probability-plot-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Normal Probability Plot",
    "text": "Normal Probability Plot\n\nA plot with the normal scores on the horizontal axis and the corresponding standardized residuals on the vertical axis, the plotted points should cluster closely around a 45-degree line passing through the origin if the standardized residuals are approximately normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nThe points are grouped closely about the line. Therefore, we can conclude that the assumption of the error term having a normal probability distribution is reasonable.\nIn general, the more closely the points are clustered about the 45-degree line, the stronger the evidence supporting the normality assumption. Any substantial curvature in the normal probability plot is evidence that the residuals have not come from a normal distribution."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#outliers",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#outliers",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Outliers",
    "text": "Outliers\nAn outlier is a data point (observation) that does not fit the trend shown by the remaining data. Represent observations that are suspect and warrant careful examination.\n\nErroneous Data\n\nInvestigate the source of the outlier to identify any errors in data collection or entry.\nCorrect the data point if possible, or remove it if the correct value cannot be determined.\n\nViolation of Model Assumptions\n\nUse diagnostic tools to check if the outlier violates the assumptions of simple regression.\nConsider alternative models, such as transformations or robust regression methods, if the assumptions are violated.\n\nLegitimate but Unusual Observations\n\nRecognize that some outliers are natural occurrences and represent rare events.\nRetain these outliers in the analysis to reflect the true variability in the data.\nReport and interpret the presence of these outliers in the context of the study."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Outliers",
    "text": "Detecting Outliers\n\nFor the case of simple linear regression, one can often detect outliers by simply examining the scatter diagram.\n\n\n\n\n\n\n\n\n\n\n\\(R^2 = .4968\\)\n\nGiven the pattern of the rest of the data, we would have expected \\(y_4\\) to be much smaller and hence would consider observation 4 to be an outlier."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers-with-standardized-residuals",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers-with-standardized-residuals",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Outliers with Standardized Residuals",
    "text": "Detecting Outliers with Standardized Residuals\n\nIf an observation deviates greatly from the pattern of the rest of the data, the corresponding standardized residual will be large in absolute value.\nAny observation with a standardized residual of less than -2 or greater than +2 is a potential outlier. With normally distributed errors, standardized residuals should be outside these limits approximately 5% of the time.\n\n\n\n\n\n\n\n\n\nIn the residual output section in the Figure we see that the standard residual value for observation 4 is 2.68."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-outliers-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Outliers",
    "text": "Detecting Outliers\n\nThe identification of the outlier enables us to correct the data error and improve the regression results. This is the result with \\(y_4 = 30\\).\n\n\n\n\n\n\n\n\n\n\nNow, no standard residuals are less than -2 or greater than +2.\nThe value of \\(R^2\\) has increased from 0.4968 to 0.8380\nThe value of \\(b_0\\) has decreased from 64.95 to 59.23.\nThe slope of the line has changed from -7.330 to -6.949."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#influential-observations",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#influential-observations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Influential Observations",
    "text": "Influential Observations\n\n\nAn influential observation is an observation that has a strong influence on the regression results.\nAn influential observation may be an outlier, it may correspond to an \\(x\\) value far from its mean, or it may be caused by a combination of a somewhat off-trend \\(y\\) value and a somewhat extreme \\(x\\) value.\nIf the observation is valid, it can contribute to a better understanding of the appropriate model and can lead to a better estimated regression equation."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Influential Observations",
    "text": "Detecting Influential Observations\n\n\n\n\n\n\n\n\n\n\n\n\nWith observation 7 deleted,\n\nThe value of \\(b_0\\) has increased from 127.4 to 138.1.\nThe slope of the line has changed from -0.425 to -1.090."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Influential Observations",
    "text": "Detecting Influential Observations\nHigh Leverage Points: Observations with extreme values for the independent variable. Observation 7 in the data set shown is a point with high leverage.\nThe leverage of an observation is determined by how far the value of the independent variable is from its mean value.\nFor the single-independent-variable case, the leverage of the \\(i\\)th observation, denoted \\(h_i\\), can be computed using:\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n\\]\nFrom the formula, farther \\(x_i\\) is from its mean \\(\\bar{x}\\), the higher the leverage of observation \\(i\\)."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations-2",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#detecting-influential-observations-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Detecting Influential Observations",
    "text": "Detecting Influential Observations\n\nFor the data shown, the leverage of observation 7 is as follows:\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2} = \\frac{1}{7} + \\frac{(70 - 24.286)^2}{2621.43} = 0.94\n\\]\nFor simple linear regression, we consider observations as having high leverage if \\(h_i &gt; \\frac{6}{n}\\). For the example, \\(\\frac{6}{n} = \\frac{6}{7} = 0.86\\).\nThus, because \\(h_i = 0.94 &gt; 0.86\\), observation 7 would be identified as having high leverage.\n\n\nNotes and Comments:\n\nThe number 6 in the criterion \\(h_i &gt; \\frac{6}{n}\\) is an arbitrary rule of thumb used to identify high leverage points in linear regression. Some statisticians use \\(\\frac{2p}{n}\\), where \\(p\\) is the number of predictors plus one (for the intercept) in the model.\nOnce an observation is identified as potentially influential because of a large residual or high leverage, its impact on the estimated regression equation should be evaluated.\nMore advanced texts discuss diagnostics for doing so. However, if one is not familiar with the more advanced material, a simple procedure is to run the regression analysis with and without the observation. This approach will reveal the influence of the observation on the results."
  },
  {
    "objectID": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#summary-1",
    "href": "lecture_slides/14_chapter_simple_regression/14_chapter_simple_regression.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nSimple Linear Regression Model: Helps to understand the linear relationship between two variables—predicting the Dependent Variable (\\(Y\\)) based on the Independent Variable (\\(X\\)).\n\nThis method is foundational for exploring how variables are related and how changes in one variable can predict changes in another.\n\nModel Interpretation and Assumptions: It is essential to interpret the regression coefficients and validate model assumptions to ensure that the model is reliable.\n\nAssumptions include linearity, independence, homoscedasticity (constant variance), and normality of residuals.\n\nResidual Analysis: Residual plots and other diagnostic tools are critical for validating the assumptions of the regression model and ensuring its accuracy.\n\nProper validation techniques prevent misinterpretation of data and help maintain the integrity of the analysis."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#overview",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nMultiple Regression Model\nLeast Squares Method\nMultiple Coefficient of Determination\nModel Assumptions\nTesting for Significance: Overall and Individual\n\n\n\nMulticollinearity Issue\nResidual Analysis\nPrediction\nCategorical Independent Variables"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\nPuzzle: What is the effect of education on income?\nY: Income\nX: Education\nObjective: X \\(\\rightarrow\\) Y\nChallenge: X \\(\\leftarrow\\) W \\(\\rightarrow\\) Y\nW: IQ (Intelligence)\nSolution: Control for W"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\n\n\n\nDAG\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#motivation-controlling-for-a-variable-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Controlling for a Variable",
    "text": "Motivation: Controlling for a Variable\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#omitted-variables-confounders",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#omitted-variables-confounders",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Omitted Variables (Confounders)",
    "text": "Omitted Variables (Confounders)\n\nOne of the most common errors in observational studies (besides selection bias and information bias — classification or measurement error);\nIt occurs when we suggest that the explanation for something is “confounded” with the effect of another variable;\nFor example, “the sun rose because the rooster crowed,” and not because of Earth’s rotation."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#how-to-address-omitted-variable-bias",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#how-to-address-omitted-variable-bias",
    "title": " MGMT 30500: Business Statistics ",
    "section": "How to Address Omitted Variable Bias?",
    "text": "How to Address Omitted Variable Bias?\n\nBe well-versed in the literature;\nSelect good control variables for your model;\nThat is, perform a multiple regression model."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nRegression analysis involving two or more independent variables (x’s).\nThis subject area, called multiple regression analysis, enables us to consider more independent variables (factors) and thus obtain better estimates of the relationship than are possible with simple linear regression."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression Model",
    "text": "Multiple Regression Model\nThe equation that describes how the dependent variable \\(y\\) is related to the independent variables \\(x_1, x_2, \\ldots x_p\\) and an error term \\(\\epsilon\\) is:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n\\]\nWhere:\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\) are the unknown parameters.\n\\(\\epsilon\\) is a random variable called the error term with the same assumptions as in simple regression (Normality, zero mean, constant variance, independence).\n\\(p\\) is the number of independent variables (dimension or complexity of the model)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-equation",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression Equation",
    "text": "Multiple Regression Equation\nThe equation that describes how the mean value of \\(y\\) is related to \\(x_1, x_2, \\ldots x_p\\) is:\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n\\]\n\\(\\beta_1, \\ldots, \\beta_p\\) measure the marginal effects of the respective independent variables.\n\nFor example, \\(\\beta_1\\) is the change in \\(E(y)\\) corresponding to a 1-unit increase in \\(x_1\\), when all other independent variables are held constant or when we control for all other independent variables."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimated-multiple-regression-equation",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimated-multiple-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimated Multiple Regression Equation",
    "text": "Estimated Multiple Regression Equation\n\n\\[\n\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_p x_p\n\\]\nA simple random sample is used to compute sample slopes \\(b_0, b_1, b_2, \\dots, b_p\\) that are used as the point estimators of the population slopes \\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\).\n\nHence, \\(\\hat{y}\\) estimates \\(E(Y)\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#least-squares-method-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#least-squares-method-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Least Squares Method",
    "text": "Least Squares Method\n\nLeast Squares Criterion: Minimize the Sum of Squared Errors (SSE):\n\\[\n\\min \\sum (y_i - \\hat{y_i})^2\n\\]\nWhere \\(y_i - \\hat{y_i}\\) is the \\(i\\)-th residual/error.\n\n\nThe formulas for the regression coefficients \\(b_0, b_1, b_2, \\dots, b_p\\) involve the use of matrix algebra. We will rely on computer software packages to perform the calculations.\nThe emphasis will be on how to interpret the computer output rather than on how to make the multiple regression computations."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimation-process",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimation-process",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimation Process",
    "text": "Estimation Process"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression Model",
    "text": "Multiple Regression Model\nExample: Butler Trucking Company\nManagers at Butler Trucking Company want to develop better work schedules for their drivers. They believe that the total daily travel time would be closely related to the number of miles traveled in making the daily deliveries and also to the number of deliveries.\nA simple random sample of 10 driving assignments was taken."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression Model",
    "text": "Multiple Regression Model\nExample: Butler Trucking Company - Butler.xlsx\n\n\n\n\n\n\n\n\n\nDriving Assignment\nMiles traveled \\(x_1\\)\nDeliveries \\(x_2\\)\n\\(y\\) = Travel Time (hours)\n\n\n\n\n1\n100\n4\n9.3\n\n\n2\n50\n3\n4.8\n\n\n3\n100\n4\n8.9\n\n\n4\n100\n2\n6.5\n\n\n5\n50\n2\n4.2\n\n\n6\n80\n2\n6.2\n\n\n7\n75\n3\n7.4\n\n\n8\n65\n4\n6.0\n\n\n9\n90\n3\n7.6\n\n\n10\n90\n2\n6.1"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-3",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-regression-model-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Regression Model",
    "text": "Multiple Regression Model\nExample: Butler Trucking Company\nSuppose we believe that total daily travel time (\\(y\\)) is related to the miles traveled (\\(x_1\\)) and the number of deliveries made (\\(x_2\\)) by the following multiple linear regression model:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\nWhere:\n\n\\(y\\) = Total travel time\n\\(x_1\\) = Miles traveled\n\\(x_2\\) = Deliveries made\n\\(n = 10, p = 2\\)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#summary-statistics",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#summary-statistics",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\n\n\n\nMiles (\\(x_1\\))\nDeliveries (\\(x_2\\))\nTime (\\(Y\\))\n\n\n\n\nMean\n80\n2.9\n6.7\n\n\nStandard Error\n6.191\n0.277\n0.515\n\n\nMedian\n85\n3\n6.35\n\n\nMode\n100\n2\n#N/A\n\n\nStandard Deviation\n19.579\n0.876\n1.630\n\n\nSample Variance\n383.333\n0.767\n2.656\n\n\nKurtosis\n-1.114\n-1.734\n-0.547\n\n\nSkewness\n-0.583\n0.223\n0.196\n\n\nRange\n50\n2\n5.1\n\n\nMinimum\n50\n2\n4.2\n\n\nMaximum\n100\n4\n9.3\n\n\nSum\n800\n29\n67\n\n\nCount\n10\n10\n10"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#correlations",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#correlations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Correlations",
    "text": "Correlations\n\n\n\n\n\n\n\n\n\n\nMiles (\\(x_1\\))\nDeliveries (\\(x_2\\))\nTime (\\(Y\\))\n\n\n\n\nMiles (\\(x_1\\))\n1\n\n\n\n\nDeliveries (\\(x_2\\))\n0.162\n1\n\n\n\nTime (\\(Y\\))\n0.815\n0.615\n1"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#regression-output",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#regression-output",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Regression Output",
    "text": "Regression Output\nExample: Butler Trucking Company"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimated-regression-equation",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimated-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimated Regression Equation",
    "text": "Estimated Regression Equation\nExample: Butler Trucking Company\nPrediction equation:\n\\[\n\\hat{y} = -0.8687 + 0.0611x_1 + 0.9234x_2\n\\]\n\n\nFor observation #1 where \\(x_1 = 100\\), \\(x_2 = 4\\), and \\(y = 9.3\\):\n\\[\n\\hat{y} = -0.8687 + 0.0611(100) + 0.9234(4) = 8.9385\n\\]\nUnexplained residual = \\(y - \\hat{y} = 9.3 - 8.9385 = 0.3615\\)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpreting-the-regression-coefficients",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpreting-the-regression-coefficients",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpreting the Regression Coefficients",
    "text": "Interpreting the Regression Coefficients\n\nEach \\(b_i\\) (for \\(x_i\\)) represents an estimate of the change in the expected \\(y\\) corresponding to a 1-unit increase in \\(x_i\\), when all other independent variables are held constant or when we control for all other independent variables."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpreting-the-regression-coefficients-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpreting-the-regression-coefficients-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpreting the Regression Coefficients",
    "text": "Interpreting the Regression Coefficients\nExample: Butler Trucking Company\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(b_1 = +0.0611\\) (for \\(x_1\\))\n\n+0.0611 is the estimated average change (increase) in the expected travel time corresponding to an increase of one mile in the distance traveled when the number of deliveries is held constant.\n\n\n\n\n\\(b_2 = +0.9234\\) (for \\(x_2\\))\n\n+0.9234 is the estimated average change (increase) in the expected travel time corresponding to an increase of one delivery when the number of miles traveled is held constant."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Coefficient of Determination",
    "text": "Multiple Coefficient of Determination\n\nRelationship Among SST, SSR, SSE\n\n\n\n\n\n\n\n\n\n\nSST\nSSR\nSSE\n\n\n\n\nFormula\n\\(\\sum (y_i - \\bar{y})^2\\)\n\\(\\sum (\\hat{y}_i - \\bar{y})^2\\)\n\\(\\sum (y_i - \\hat{y}_i)^2\\)\n\n\nDegrees of Freedom\n\\(n-1\\)\n\\(p\\)\n\\(n-p-1\\)\n\n\n\n\\[\n\\sum (y_i - \\bar{y})^2 = \\sum (\\hat{y_i} - \\bar{y})^2 + \\sum (y_i - \\hat{y_i})^2\n\\]\nWhere:\n\nSST = Total sum of squares (total variation of the response)\nSSR = Sum of squares due to regression (explained by all predictors)\nSSE = Sum of squares due to error (unexplained variation of residuals)\n\\(n\\) = sample size.\n\\(p\\) = number of predictors (independent variables).\nThe degrees of freedom for SSE reflect the number of observations left after accounting for the number of estimated parameters (\\(n - (p + \\text{one intercept})\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#partition-of-sst",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#partition-of-sst",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Partition of SST",
    "text": "Partition of SST\n\n\n\n\ndf: SST = 9, SSE = 7, SSR = 2"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Coefficient of Determination",
    "text": "Multiple Coefficient of Determination\nExample: Butler Trucking Company\nANOVA Table\n\n\n\nANOVA\ndf\nSS\nMS\nF\nSignificance F\n\n\n\n\nRegression\n2\n21.6006\n10.8003\n32.8784\n0.0003\n\n\nResidual\n7\n2.2994\n0.3285\n\n\n\n\nTotal\n9\n23.9"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-3",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multiple-coefficient-of-determination-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Coefficient of Determination",
    "text": "Multiple Coefficient of Determination\nExample: Butler Trucking Company\n\\[\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n\\]\n\\[\nR^2 = \\frac{21.6006}{23.9} = 1 - \\frac{2.2994}{23.9} = 90.38\\%\n\\]\n\\[\nR^2 = 1 - \\frac{SSE / (n - 1)}{SST / (n - 1)} = 1 - \\frac{\\text{Residual variance}}{\\text{Response variance}}\n\\]"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#remarks-on-multiple-coefficient-of-determination",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#remarks-on-multiple-coefficient-of-determination",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Remarks on Multiple Coefficient of Determination",
    "text": "Remarks on Multiple Coefficient of Determination\n\nAdding independent variables, even ones that are not statistically significant, will reduce the prediction errors, thus the SSE will become smaller.\nBecause SST = SSR + SSE is fixed, SSR will become larger, SSE will become smaller, and hence, \\(R^2 = SSR/SST\\) will always increase.\nBut, adding independent variables, the model will become more complex (a larger \\(p\\))."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adjusted Multiple Coefficient of Determination",
    "text": "Adjusted Multiple Coefficient of Determination\n\nThe adjusted multiple coefficient of determination (\\(R^2_a\\)) takes into account the following factors:\n\nThe number of independent variables in the model (\\(p\\), dimension or complexity)\nThe \\(R^2\\) (Adequacy)\nThe sample size (\\(n\\), available information)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adjusted Multiple Coefficient of Determination",
    "text": "Adjusted Multiple Coefficient of Determination\nExample: Butler Trucking Company\n\\[\nR^2 = 1 - \\frac{SSE}{SST} = 90.38\\%\n\\]\n\\[\nR^2_a = 1 - \\frac{\\frac{SSE}{(n - p - 1)}}{\\frac{SST}{(n - 1)}} = 1 - (1 - R^2) \\cdot \\frac{n - 1}{n - p - 1}\n\\]\n\\[\nR^2_a = 1 - \\frac{2.2994 / (10 - 2 - 1)}{23.9 / (10 - 1)} = 1 - (1 - 0.9038) \\cdot \\frac{10 - 1}{10 - 2 - 1} = 87.63\\%\n\\]"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#adjusted-multiple-coefficient-of-determination-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adjusted Multiple Coefficient of Determination",
    "text": "Adjusted Multiple Coefficient of Determination\n\\[\nR^2_a = 1 - \\frac{\\frac{SSE}{(n - p - 1)}}{\\frac{SST}{(n - 1)}} = 1 - \\frac{MSE}{\\frac{SST}{(n - 1)}} = 1 - \\frac{S^2}{\\frac{SST}{(n - 1)}}\n\\]\n\nA smaller \\(S^2\\) gives a higher \\(R^2_a\\).\nBringing in a new independent variable will increase the \\(R^2_a\\) when and only when it can decrease the estimated error variance (\\(S^2\\) or MSE). Hence, \\(R^2_a\\) is a good criterion for model building or model selection.\n\n(Recall: MSE or \\(S^2\\) estimates the error variance \\(\\sigma^2\\).)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-significance-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-significance-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Significance",
    "text": "Testing for Significance\n\nIn simple linear regression, the F and \\(t\\) Tests provide the same conclusion:\n\nThey are equivalen\\(t\\) Tests: Squared t-value = F-value.\nThey have the same p-value.\n\nIn multiple regression, the F and \\(t\\) Tests have different purposes and are not equivalent."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\n\nThe \\(F\\) Test is used to determine whether a significant linear relationship exists between the dependent variable and the set of all the independent variables, by testing their population slopes (\\(\\beta_1, \\dots, \\beta_p\\)) together as a group.\nThe \\(F\\) Test is referred to as the test for overall significance."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\n\nHypotheses\n\\[\nH_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0 \\quad (\\text{No linear relationship})\n\\]\n\\[\nH_a: \\text{One or more of the slopes is not equal to zero}\n\\]\nTest Statistics\n\\[\nF = \\frac{MSR}{MSE}\n\\] where,\n\\[\nMSR = \\frac{SSR}{p} \\quad \\text{and} \\quad MSE = \\frac{SSE}{n-p-1}\n\\]\nRejection Rule\nReject \\(H_0\\) if \\(F \\geq F_\\alpha\\) or \\(p \\leq \\alpha\\).\nwhere \\(F_\\alpha\\) is based on an \\(F\\) distribution with \\(p\\) d.f. (numerator) and \\(n - p - 1\\) d.f. (denominator)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\nExample: Butler Trucking Company\nHypotheses:\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\n\\[\nH_a: \\text{One or both slopes are not equal to zero}\n\\]\nRejection Rule\nFor \\(\\alpha = 0.01\\) and degrees of freedom = 2 and 7, \\(F_{\\alpha} = 9.55\\).\nReject \\(H_0\\) if \\(F \\geq 9.55\\) or \\(p \\leq .01\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-3",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\nTest Statistics\n\\[\nF = \\frac{MSR}{MSE} = \\frac{10.8003}{0.3285} = 32.8784\n\\]\nConclusion:\nWe reject \\(H_0\\) because \\(F = 32.8784 &gt; 9.55\\).\nWe have significant statistical evidence to conclude that at least one of the independent variables is useful in explaining the variation in the total travel time, or for predicting the total travel time. The significance level is 1%."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-4",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\nANOVA Output\n\n\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\nF\nSignificance F\n\n\n\n\nRegression\n\\(p\\)\nSSR\n\\(MSR=\\frac{SSR}{p}\\)\n\\(F=\\frac{MSR}{MSE}\\)\n\\(p-value\\)\n\n\nResidual\n\\(n-p-1\\)\nSSE\n\\(MSE=\\frac{SSE}{n-p-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\nSST"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-5",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-overall-significance-f-test-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Overall Significance: \\(F\\) Test",
    "text": "Testing for Overall Significance: \\(F\\) Test\nANOVA Output\n\n\n\nSource\ndf\nSS\nMS\nF\nSignificance F\n\n\n\n\nRegression\n2\n21.6006\n10.8003\n32.8784\n0.0003\n\n\nResidual\n7\n2.2994\n0.3285\n\n\n\n\nTotal\n9\n23.9\n\n\n\n\n\n\n\\[\np-value = 1 - F.DIST(32.8784, 2, 7, TRUE) \\approx 0.0003\n\\]"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\n\nIf the \\(F\\) test shows an overall significance, a separate \\(t\\) test is conducted for each of the independent variables in the model.\nThe \\(t\\) test is used to determine whether each of the individual independent variables is significant in the presence of all other independent variables in the model.\n\nIt tests the additional contribution in reducing the variation of the unexplained error (or in increasing the explained variation).\n\nWe refer to each of these \\(t\\) tests as a test for individual significance."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\nHypotheses\n\\[\nH_0: \\beta_i = 0 \\quad (x_i \\text{ is not significant in the presence of all other } x's)\n\\]\n\\[\nH_a: \\beta_i \\neq 0\n\\]\nTest Statistic\n\\[\nt = \\frac{b_i - 0}{s_{b_i}} \\quad (s_{b_i} \\text{ is the standard error of } b_i)\n\\]\nRejection Rule\nReject \\(H_0\\) if \\(t \\leq -t_{\\alpha / 2}\\) or \\(t \\geq t_{\\alpha / 2}\\) or if \\(p\\)-value \\(\\leq \\alpha\\).\nwhere \\(t_{\\alpha/2}\\) is based on a \\(t\\)-distribution with \\(n - p - 1\\) degrees of freedom."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\nExample: Butler Trucking Company\n\n\\(H_0: \\beta_i = 0\\)\n\\(H_a: \\beta_i \\neq 0 \\quad \\text{(Two-tailed)}\\)\n\n\nFor \\(\\alpha = 0.01\\) and \\(df = 7\\):\n\\[\nt_{0.005} = 3.499\n\\]\nReject \\(H_0\\) if \\(p\\)-value \\(\\leq .01\\), or if \\(t \\leq -3.499\\) or \\(t \\geq 3.499\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-3",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\nTest Statistics\n\\[\nt = \\frac{b_1 - 0}{s_{b_1}} = \\frac{0.0611 - 0}{0.0099} = 6.1717, \\quad \\text{for } x_1\n\\]\n\\[\nt = \\frac{b_2 - 0}{s_{b_2}} = \\frac{0.9234 - 0}{0.2211} = 4.1764, \\quad \\text{for } x_2\n\\]\nConclusion\nReject each of \\(H_0: \\beta_1 = 0\\) and \\(H_0: \\beta_2 = 0\\). Each independent variable is statistically significant in the presence of the other."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-4",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#testing-for-individual-significance-t-test-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Testing for Individual Significance: \\(t\\) Test",
    "text": "Testing for Individual Significance: \\(t\\) Test\nExample: Butler Trucking Company\nRegression Table"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multicollinearity-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multicollinearity-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nThe term multicollinearity refers to the correlation among the independent variables.\nIt is important to compute pairwise correlations in a multiple regression analysis.\nRule of Thumb: When independent variables are highly correlated (e.g., \\(|r| &gt; 0.7\\)), it is difficult to determine the separate/marginal effect of each independent variable on the dependent variable.\nEvery attempt should be made to avoid including independent variables that are highly correlated.\nIf the estimated regression equation is to be used only for predictive purposes, multicollinearity is usually not a serious problem"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multicollinearity-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#multicollinearity-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nThe coefficients do not measure marginal effects of the predictors (rather combined effects may be measured).\nCoefficient estimates (b’s) are unstable (having “inflated” standard errors — also see Variance Inflation Factor, VIF).\n\nNonsignificant test results for important predictor variables.\nEstimated regression coefficients with an algebraic sign that is the opposite of what is expected from theoretical considerations or prior experience.\nLarge changes in the estimated regression coefficients (slopes) when a predictor variable is added or deleted.\nWide confidence intervals for the regression coefficients representing important predictor variables."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimation-and-prediction-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#estimation-and-prediction-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimation and Prediction",
    "text": "Estimation and Prediction\n\nThe procedures for estimating the mean value of \\(y\\) and predicting an individual value of \\(y\\) in multiple regression are similar to those in simple regression.\nWe substitute the given values of \\(x_1, x_2, ..., x_p\\) into the estimated regression equation and use the corresponding value of \\(\\hat{y}\\) as the point estimate.\nThe interpretations of the Confidence Interval (CI) and Prediction Interval (PI) in multiple regression are similar to those in simple regression:\n\nThe Confidence Interval (CI) estimates the range within which the mean value of the dependent variable is expected to lie, given specific values of the independent variables.\nThe Prediction Interval (PI) estimates the range within which an individual observation of the dependent variable is expected to lie, given specific values of the independent variables.\n\n\n\n\nThe formulas required to develop interval estimates, CI and PI, are beyond the scope of the course and our textbook. Software packages for multiple regression will often provide these interval estimates."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\n\nIn many situations, we need to work with categorical independent variables such as gender (an independent variable with 2 categories: Male, Female), method of payment (an independent variable with 3 categories: Cash, Check, Credit card), etc.\nNeed to code categorical independent variables. For example, if \\(x_2\\) is a gender variable, let \\(x_2 = 1\\) for males and \\(x_2 = 0\\) for females.\nIn this case, \\(x_2\\) is called a dummy or indicator variable, and the category with a 0 (i.e., female) is called the reference category or level."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\n\nGender: F, M\nJob Type: Lawyer, Salesperson, Educator, Government (for salary)\nTool Type: A, B, C (for yields)\nRegion: NE, NW, SE, SW (for sales)\nSeason (for sales)\nTraining Method: Online, Hybrid, In-person (for learning outcome)\nPosition: Manager, AVP, Senior VP, Chair (for retirement age)\nAdvertising Media: Newspaper, Magazine, Spot TV (for sales)\nSmoker: Yes, No (for health risk)\nMethod of Payment: Cash, Check, Credit Card (for credit rating or loan approvals)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-3",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nExample: Johnson Filtration, Inc.\nManagers of Johnson Filtration, Inc. want to predict the repair time necessary for processing its maintenance requests. Repair time is believed to be related to two factors, the number of months since last service and the type of repair problem (mechanical or electrical). Data for a sample of 10 service calls are reported in the table (next slide)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-4",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nExample: Johnson Filtration, Inc\n\n\n\n\n\n\n\n\n\nService Call\nMonths since last service\nType of repair\nRepair time (hours)\n\n\n\n\n1\n2\nElectrical\n2.9\n\n\n2\n6\nMechanical\n3.0\n\n\n3\n8\nElectrical\n4.8\n\n\n4\n3\nMechanical\n1.8\n\n\n5\n2\nElectrical\n2.9\n\n\n6\n7\nElectrical\n4.9\n\n\n7\n9\nMechanical\n4.2\n\n\n8\n8\nMechanical\n4.8\n\n\n9\n4\nElectrical\n4.4\n\n\n10\n6\nElectrical\n4.5"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-5",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nExample: Johnson Filtration, Inc: Now with a dummy or indicator variable.\n\n\n\n\n\n\n\n\n\nService Call\nMonths since last service\nType of repair\nRepair time (hours)\n\n\n\n\n1\n2\n1\n2.9\n\n\n2\n6\n0\n3.0\n\n\n3\n8\n1\n4.8\n\n\n4\n3\n0\n1.8\n\n\n5\n2\n1\n2.9\n\n\n6\n7\n1\n4.9\n\n\n7\n9\n0\n4.2\n\n\n8\n8\n0\n4.8\n\n\n9\n4\n1\n4.4\n\n\n10\n6\n1\n4.5\n\n\n\n0 = Mechanical (reference)\n1 = Electrical"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-6",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nExample: Johnson Filtration, Inc.\nRegression model:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\nWhere:\n\\(y\\) = Repair time in hours\n\\(x_1\\) = Number of months since last maintenance service\n\\(x_2\\) = 0 if type of repair is mechanical, 1 if the type of repair is electrical ( \\(x_2\\) is a dummy variable)"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-7",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-7",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nExample: Johnson Filtration, Inc."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-8",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-8",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\nInterpretation of regression coefficient \\(\\beta_2\\) of dummy variable \\(x_2\\):\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\nFor mechanical (reference):\n\\[\nE(y|mechanical) = \\beta_0 + \\beta_1 x_1 + \\beta_2 \\cdot 0 = \\beta_0 + \\beta_1 x_1\n\\]\nFor electrical:\n\\[\nE(y|electrical) = \\beta_0 + \\beta_1 x_1 + \\beta_2 \\cdot 1 = (\\beta_0 + \\beta_2) + \\beta_1 x_1\n\\]"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-9",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-9",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables\n\nThe mean repair time is a linear function of \\(x_1\\) for both mechanical and electrical repairs.\nThe slope of both equations is \\(\\beta_1\\), but the y-intercept differs:\n\nFor mechanical repairs: the y-intercept is \\(\\beta_0\\).\nFor electrical repairs: the y-intercept is \\(\\beta_0 + \\beta_2\\).\n\nThe coefficient \\(\\beta_2\\) indicates the difference between the mean repair time for electrical repairs and the mean repair time for mechanical repairs."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpretation-of-beta_2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#interpretation-of-beta_2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of \\(\\beta_2\\)",
    "text": "Interpretation of \\(\\beta_2\\)\n\nIf \\(\\beta_2\\) is positive: The mean repair time for an electrical repair is greater than for a mechanical repair.\nIf \\(\\beta_2\\) is negative: The mean repair time for an electrical repair is less than for a mechanical repair.\nIf \\(\\beta_2 = 0\\): There is no difference in the mean repair time between electrical and mechanical repairs, indicating that the type of repair is not related to repair time.\nIn effect, the use of the dummy variable for type of repair provides two estimated regression equations that can be used to predict the repair time."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-10",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#categorical-independent-variables-10",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Categorical Independent Variables",
    "text": "Categorical Independent Variables"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#more-complex-categorical-variables",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#more-complex-categorical-variables",
    "title": " MGMT 30500: Business Statistics ",
    "section": "More Complex Categorical Variables",
    "text": "More Complex Categorical Variables\n\nA categorical variable with \\(k\\) levels must be modeled using \\(k - 1\\) dummy variables. Care must be taken in defining and interpreting the dummy variables.\n\n\nIn the Johnson Filtration example, a categorical variable (mechanical and electrical) with two levels was represented by a single dummy variable.\nWhen a categorical variable has more than two levels, \\(k - 1\\) dummy variables are required, with each coded as 0 or 1."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-categorical-variable-with-three-levels",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-categorical-variable-with-three-levels",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Categorical Variable with Three Levels",
    "text": "Example: Categorical Variable with Three Levels\n\nSuppose a manufacturer organizes the sales territories into three regions: A, B, and C.\nThe number of copiers sold per week is the dependent variable.\nThe sales region is a categorical variable with three levels: A, B, and C.\n\n\nDummy Variables for Sales Region: We need \\(3 - 1 = 2\\) dummy variables.\n\\[\nx_1 =\n\\begin{cases}\n1 & \\text{if sales region B} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\\[\nx_2 =\n\\begin{cases}\n1 & \\text{if sales region C} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-dummy-variable-coding",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-dummy-variable-coding",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Dummy Variable Coding",
    "text": "Example: Dummy Variable Coding\n\nThe following table shows the coding for the dummy variables \\(x_1\\) and \\(x_2\\) for each sales region.\n\n\n\n\n\nRegion\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\nA\n0\n0\n\n\nB\n1\n0\n\n\nC\n0\n1\n\n\n\n\n\nObservations corresponding to region A are coded \\(x_1 = 0,  x_2 = 0\\).\nObservations corresponding to region B are coded \\(x_1 = 1,  x_2 = 0\\).\nObservations corresponding to region C are coded \\(x_1 = 0,  x_2 = 1\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-regression-equation-with-dummy-variables",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-regression-equation-with-dummy-variables",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Regression Equation with Dummy Variables",
    "text": "Example: Regression Equation with Dummy Variables\n\n\nThe regression equation for the expected number of units sold is:\n\n\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\n\nTo interpret the parameters \\(\\beta_0,  \\beta_1,  \\beta_2\\), consider the three variations of the regression equation:\nRegion A: \\[\nE(y | \\text{region A}) = \\beta_0 + \\beta_1 (0) + \\beta_2 (0) = \\beta_0\n\\]\nRegion B: \\[\nE(y | \\text{region B}) = \\beta_0 + \\beta_1 (1) + \\beta_2 (0) = \\beta_0 + \\beta_1\n\\]\nRegion C: \\[\nE(y | \\text{region C}) = \\beta_0 + \\beta_1 (0) + \\beta_2 (1) = \\beta_0 + \\beta_2\n\\]\nInterpretation:\n\n\\(\\beta_0\\): Mean or expected value of sales for region A.\n\\(\\beta_1\\): Difference in mean sales between region B and region A.\n\\(\\beta_2\\): Difference in mean sales between region C and region A."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-important-points-on-dummy-variables",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#example-important-points-on-dummy-variables",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Important Points on Dummy Variables",
    "text": "Example: Important Points on Dummy Variables\n\nTwo dummy variables are used because the sales region has three levels.\nThe assignment of \\(x_1 = 0, x_2 = 0\\) for region A is arbitrary.\nAlternative coding choices are possible, which would change the interpretation of \\(\\beta_1\\) and \\(\\beta_2\\).\nKey Point: For a categorical variable with \\(k\\) levels, \\(k - 1\\) dummy variables are required.\n\nFor a fourth region \\(D\\), we would need three dummy variables."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#precision-and-accuracy",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#precision-and-accuracy",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Precision and Accuracy",
    "text": "Precision and Accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision: Refers to the consistency or reliability of the model’s predictions.\nAccuracy: Refers to how close the model’s predictions are to the true values.\n\n\nIn the context of regression:\n\nHigh Precision, Low Accuracy: Predictions are consistent but biased.\nHigh Precision, High Accuracy: Predictions are both consistent and valid.\nLow Precision, Low Accuracy: Predictions are neither consistent nor valid.\nLow Precision, High Accuracy: Predictions are valid on average but have high variability.\n\n\nTo achieve high precision and high accuracy, we need to meet the model assumptions."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model",
    "text": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model\n\nConsider the following two-independent-variable multiple regression equation:\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n\\]\n\n\n\n\nThe graph of this equation is a plane in three-dimensional space. The value of \\(\\epsilon\\) shown is the difference between the actual \\(y\\) value and the expected value of \\(y\\), \\(E(y)\\), when \\(x_1 = x_1^*\\) and \\(x_2 = x_2^*\\)."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model",
    "text": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p + \\epsilon\n\\]\n\nThe error term \\(\\epsilon\\) is a random variable with \\(E(\\epsilon) = 0\\).\n\n\nImplication: For given values of \\(x_1, x_2, \\ldots, x_p\\), the expected, or average, value of \\(y\\) is given by:\n\n\n\\[\nE(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n\\]\n\n\nValidation: Residuals vs. Predicted Values Plot or the Standardized Residuals vs. Predicted Values Plot\n\n\nThe variance of \\(\\epsilon\\) is denoted by \\(\\sigma^2\\) and is the same for all values of the independent variables \\(x_1, x_2, \\ldots, x_p\\).\n\n\nImplication: The variance of \\(y\\) about the regression line equals \\(\\sigma^2\\) and is the same for all values of \\(x_1, x_2, \\ldots, x_p\\).\nValidation: Residuals vs. Predicted Values Plot or the Standardized Residuals vs. Predicted Values Plot"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model-2",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#assumptions-about-the-error-term-epsilon-in-the-multiple-regression-model-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model",
    "text": "Assumptions about the Error Term \\(\\epsilon\\) in the Multiple Regression Model\n\nThe values of \\(\\epsilon\\) are independent.\n\nImplication: The value of \\(\\epsilon\\) for a particular set of values for the independent variables is not related to the value of \\(\\epsilon\\) for any other set of values.\nValidation: Standardized Residuals vs. Predicted Values Plot\n\nThe error term \\(\\epsilon\\) is a normally distributed random variable reflecting the deviation between the \\(y\\) value and the expected value of \\(y\\).\n\nImplication: Because \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are constants for the given values of \\(x_1, x_2, \\ldots, x_p\\), the dependent variable \\(y\\) is also a normally distributed random variable.\nValidation: Normal Probability Plot or Q-Q Plot (Quantile-Quantile Plot) of Residuals."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#residual-analysis-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#residual-analysis-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nFor simple linear regression, the residual plot against \\(\\hat{y}\\) and the residual plot against \\(x\\) provide the same information.\nIn multiple regression analysis, it is preferable to use the residual plot against \\(\\hat{y}\\) to determine if the model assumptions are satisfied."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residuals",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residuals",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\nStandardized residuals are frequently used in residual plots for purposes of:\n\nIdentifying outliers (typically, standardized residuals \\(&lt; -2\\) or \\(&gt; +2\\))\nProviding insight about the assumption that the error term \\(\\epsilon\\) has a normal distribution, using the Empirical Rule.\n\nThe computation of standardized residuals in multiple regression analysis is too complex to be done by hand.\n\nExcel’s Regression tool can be used.\n\nNormality of residuals can be checked by the normal probability plot."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residuals-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residuals-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nExample: Butler Trucking Company: Residual output"
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residual-plot-against-haty",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#standardized-residual-plot-against-haty",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Standardized Residual Plot Against \\(\\hat{y}\\)",
    "text": "Standardized Residual Plot Against \\(\\hat{y}\\)\nExample: Butler Trucking Company\n\n\n\n\n\nThe standardized residual plot does not indicate any abnormalities, and no standard residual is less than -2 or greater than +2.\nNote: the pattern of the standardized residual plot against \\(\\hat{y}\\) is the same as the pattern of the residual plot against \\(\\hat{y}\\). But the standardized residual plot enables us to check for outliers and determine whether the assumption of normality for the regression model (Distribution of \\(y\\)) is reasonable."
  },
  {
    "objectID": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#summary-1",
    "href": "lecture_slides/15_chapter_multiple_regression/15_chapter_multiple_regression.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nMultiple regression analysis enables the inclusion of more independent variables, improving estimates over simple regression.\nThe multiple regression model describes how the dependent variable is related to multiple independent variables.\nLeast Squares Method is used to estimate the coefficients by minimizing the sum of squared errors.\nAdjusted \\(R^2\\) accounts for the number of predictors, making it a more reliable criterion for model selection.\nMulticollinearity can affect the stability of coefficient estimates and make it difficult to isolate the effects of individual predictors.\nDummy variables can be used to incorporate categorical variables in regression models.\nThe overall significance of the model is tested using the F-test, while individual predictors are tested using t-tests."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#overview",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\nLogistic Regression Model."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#motivation-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#motivation-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation",
    "text": "Motivation\n\n\nWhat is the probability of success of a candidate based on the data below?\n\n\n\n\n\n\nadmit\ngre\ngpa\nrank\n\n\n\n\n0\n380\n3.61\n3\n\n\n1\n660\n3.67\n3\n\n\n1\n800\n4.00\n1\n\n\n1\n640\n3.19\n4\n\n\n0\n520\n2.93\n4\n\n\n1\n760\n3.00\n2\n\n\n\n\n\n\n\n\ngre: Graduate Record Exam scores\ngpa: Grade Point Average\nrank: prestige of undergraduate institution\nadmit: admission to graduate school"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#review-of-linear-estimation",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#review-of-linear-estimation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Review of Linear Estimation",
    "text": "Review of Linear Estimation\n\n\n\\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + \\epsilon\\)\n\n\n\nWe could transform or add variables to achieve model linearity:\n\nTaking logarithms of Y and/or X’s;\nAdding quadratic terms;\nAdding interactions.\n\n\n\n\nThen we run our estimation, check model quality, visualize results, etc."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#review-of-linear-estimation-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#review-of-linear-estimation-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Review of Linear Estimation",
    "text": "Review of Linear Estimation\n\n\n\\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + \\epsilon\\)\n\n\nIn all linear models we have covered, the dependent variable (\\(Y\\)) was numerical/continuous.\n\n\nWhat do we do when it is not numerical/continuous?"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#general-linear-model-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#general-linear-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "General Linear Model",
    "text": "General Linear Model\n\nModels in which the parameters \\((\\beta_0, \\beta_1, \\ldots, \\beta_p)\\) all have exponents of one are called linear models.\nA general linear model involving \\(p\\) independent variables (\\(z_i\\)’s) is:\n\n\n\\[\ny = \\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + \\ldots + \\beta_p z_p + \\epsilon\n\\]\nwhere each independent variable \\(z_i\\) is a (linear or nonlinear) function of \\(x_1, x_2, \\ldots, x_k\\) (the variables for which data have been collected).\n\nHere, \\(y\\) can be a function of the original response variable as well."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#general-linear-model-2",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#general-linear-model-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "General Linear Model",
    "text": "General Linear Model\n\n\nThe General Linear Model can be used to relates the dependent variable \\(Y\\) to the systematic part of the model (linear predictors) through a link function.\n\nFrom:\n\n\n\\(Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}Z + \\epsilon\\)\n\n\n\n\nTo:\n\n\n\\(P_{i} = P (Y = 1) = \\Lambda(\\beta_{0} + \\beta_{1}X_{1i} + \\beta_{2}Z_{2i} + \\epsilon) = \\Lambda(XB + \\epsilon)\\),\n\n\nwhere \\(\\Lambda\\) (lambda) represents the link function that strictly assumes values between 0 and 1.\n\n\nSource: Wikipedia - Generalized linear model."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#binary-dependent-variable",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#binary-dependent-variable",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Binary Dependent Variable",
    "text": "Binary Dependent Variable\n\nTherefore, the predicted probabilities of a binary dependent variable model can be given by:\n\n\n\\(\\hat{P}_{i} = \\hat{P} (Y = 1) = \\Lambda (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{1i} + \\hat{\\beta}_{2}Z_{2i}) = \\Lambda(\\hat{X}_{i}B)\\) ,\n\n\nwhere \\(\\Lambda(\\hat{X}_{i}B)\\) is the systematic (linear) component of the model."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Understanding the Link Function",
    "text": "Understanding the Link Function\n\n\nHow to solve this problem?"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-2",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Understanding the Link Function",
    "text": "Understanding the Link Function\n\n\nHow to solve this problem?\n\n\nWe need to transform \\(Y\\) (dichotomous) into a continuous variable \\(Y'\\) (\\(-\\infty\\), \\(\\infty\\));\nTo do this, we need a link function that performs this transformation."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#solution",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#solution",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Solution:",
    "text": "Solution:"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-3",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#understanding-the-link-function-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Understanding the Link Function",
    "text": "Understanding the Link Function\n\nLet’s remember the log-level model transformation:\n\n\\[\n\\log(\\hat{Y}_{i}) = \\beta_{0} + \\beta X\n\\]\n\n\n\nA variation of one unit in \\(X_i\\) implies a \\(\\beta\\%\\) variation in \\(Y_i\\);\nThe link function is: \\(F(Y) = \\log(Y)\\);"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-role-of-the-link-function",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-role-of-the-link-function",
    "title": " MGMT 30500: Business Statistics ",
    "section": "The Role of the Link Function",
    "text": "The Role of the Link Function\n\n\n\n\n\nThe logistic function transforms the linear prediction \\(\\beta_0 + \\beta X\\) into a probability that \\(Y = 1\\), denoted as \\(\\Pr(Y = 1|X)\\).\nThis function is sigmoidal (S-shaped), which is why the probability smoothly transitions between 0 and 1.\nThe line \\(E(y^*|X)\\) represents the expected value of the latent variable for a given value of \\(X\\).\n\n\n\n\n\n\\[\nY =\n  \\begin{cases}\n    1       & \\quad \\text{if } y^{*}_{i} &gt; 0 \\\\\n    0  & \\quad \\text{if } y^{*}_{i} \\leq 0\n  \\end{cases}\n\\]\n\n\\(y^{*}\\): is a latent (unobserved) variable, which represents an underlying continuous measure that influences the binary outcome. It drives the observed binary outcome \\(Y\\).\n\n\n\\[\ny^{*}_{i} = \\beta_{0} + \\beta X + \\epsilon\n\\]\n\nThe binary variable \\(Y\\) is determined by a threshold applied to the latent variable \\(y^{*}\\).\nThe plot illustrates the role of the logistic function in converting the continuous latent variable into probabilities that \\(Y = 1\\) or \\(Y = 0\\). As \\(X\\) increases past a certain threshold (denoted \\(\\tau = 0\\) on the graph), the probability that \\(Y = 1\\) rises above 0.5, indicating a higher likelihood of observing \\(Y = 1\\).\nIn summary: The logistic model maps a linear predictor (\\(\\beta_0 + \\beta X\\)) to a binary outcome, using the latent variable \\(y^*\\) to link the continuous world with the binary outcome via a threshold and the logistic function."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#logit-models",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#logit-models",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Logit Models",
    "text": "Logit Models\n\n\n\nLogistic distribution: In the Logit model, \\(\\epsilon\\) has a logistic distribution, \\(\\mu = 0\\) and variance given by: \\(\\sigma^2(u) = \\pi^2/3\\)\n\n\\[\n\\text{Pr}(y = 1|x) = \\frac{\\exp(\\beta_{0} + \\beta x)}{1 + \\exp(\\beta_{0} + \\beta x)} \\quad \\text{or} \\quad \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\n\\]"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#logistic-function-vs-normal-distribution",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#logistic-function-vs-normal-distribution",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Logistic Function vs Normal Distribution",
    "text": "Logistic Function vs Normal Distribution"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-logit-link-function-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-logit-link-function-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "The Logit Link Function",
    "text": "The Logit Link Function"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-logit-link-function-2",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#the-logit-link-function-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "The Logit Link Function",
    "text": "The Logit Link Function\n\n\nThe probability of the event occurring is the cumulative density function of \\(\\epsilon\\) evaluated at values determined by the independent variables"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#estimating-the-event-probability",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#estimating-the-event-probability",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimating the Event Probability",
    "text": "Estimating the Event Probability\n\\[\nP(Y_i = 1) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\n\\]\nLogit link function\n\\[\n\\hat{Y}_i = \\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\n\\]\nPrediction equation\nA common estimation method to obtain \\(b_0\\) and \\(b_1\\) is called the Maximum Likelihood Method. It finds the values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the probability (or likelihood) of observing the data."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-beta_1-via-lnodds",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-beta_1-via-lnodds",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of \\(\\beta_1\\) via Ln(Odds)",
    "text": "Interpretation of \\(\\beta_1\\) via Ln(Odds)\nDefine the odds for the event:\n\\[\n\\text{Odds}(x) \\equiv \\frac{\\text{Event probability}}{\\text{Non-event probability}} = \\frac{P(Y = 1)}{1 - P(Y = 1)} = e^{\\beta_0 + \\beta_1 x}\n\\]\n\\[\n\\ln(\\text{Odds}(x)) = \\beta_0 + \\beta_1 x\n\\]\n\n\\(b_1\\) is the predicted change in the \\(\\ln(\\text{Odds}(x))\\) per-unit increase in \\(x\\).\n\\(b_1\\) is the predicted difference: \\[\n\\ln(\\text{Odds}(x+1)) - \\ln(\\text{Odds}(x)).\n\\]\nThis relationship is true for all \\(x\\)."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-ebeta_1-via-odds-ratio",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-ebeta_1-via-odds-ratio",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of \\(e^{\\beta_1}\\) via Odds Ratio",
    "text": "Interpretation of \\(e^{\\beta_1}\\) via Odds Ratio\n\nDefine the odds ratio:\n\\[\n\\text{Odds Ratio}(x) \\equiv \\frac{\\text{Odds}(x+1)}{\\text{Odds}(x)} = e^{\\beta_1}\n\\]\nBy increasing \\(x\\) by one unit, the odds of the event change by a factor of \\(e^{\\beta_1}\\)."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-ebeta_1-continuous-predictor-x",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-ebeta_1-continuous-predictor-x",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of \\(e^{\\beta_1}\\) (Continuous Predictor, \\(x\\))",
    "text": "Interpretation of \\(e^{\\beta_1}\\) (Continuous Predictor, \\(x\\))\n\n\\[\n\\frac{\\text{Odds}(x+1)}{\\text{Odds}(x)} = e^{\\beta_1}\n\\]\n\nThe odds for the event will change by a factor of \\(e^{\\beta_1}\\) per unit increase in \\(x\\), where \\(\\beta_1\\) is the population slope of \\(x\\).\nIf \\(\\beta_1 = 0\\) or \\(e^{\\beta_1} = 1\\), there is no change in odds.\nIf \\(\\beta_1 &gt; 0\\) or \\(e^{\\beta_1} &gt; 1\\), the odds increase.\nIf \\(\\beta_1 &lt; 0\\) or \\(e^{\\beta_1} &lt; 1\\), the odds decrease.\n\nIf \\(\\beta_1\\) is estimated by \\(b_1\\), then the odds for the event are predicted to change by a factor of \\(e^{b_1}\\) per unit increase in \\(x\\)."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#motivation-binomial-logit-model-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#motivation-binomial-logit-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Motivation: Binomial Logit Model",
    "text": "Motivation: Binomial Logit Model\nA researcher is interested in how variables such as GRE (Graduate Record Exam scores), GPA (Grade Point Average), and the prestige of the undergraduate institution affect admission into graduate school. The response variable, admit/don’t admit, is binary. What is the probability of success of a candidate based on these data?\n\n\n\n\n\nadmit\ngre\ngpa\nrank\n\n\n\n\n0\n380\n3.61\n3\n\n\n1\n660\n3.67\n3\n\n\n1\n800\n4.00\n1\n\n\n1\n640\n3.19\n4\n\n\n0\n520\n2.93\n4\n\n\n1\n760\n3.00\n2\n\n\n\n\n\n\n\n Source: LOGIT REGRESSION - R DATA ANALYSIS EXAMPLES"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#estimating-the-model-r-output",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#estimating-the-model-r-output",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Estimating the Model: R Output",
    "text": "Estimating the Model: R Output\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-model-summary",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-model-summary",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Output: Model Summary",
    "text": "Output: Model Summary\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations: 400\n\nThe number of data points used in the model (400 observations).\n\nDependent Variable: admit\n\nThe binary outcome variable being predicted (in this case, admission to a school).\n\nType: Generalized linear model\n\nThe model used is a generalized linear model (GLM).\n\nFamily: binomial\n\nThe binomial family is appropriate for binary outcomes.\n\nLink: logit\n\nThe logit link function is used to model the log-odds of the probability of success."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-model-fit-statistics",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-model-fit-statistics",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Output: Model Fit Statistics",
    "text": "Output: Model Fit Statistics\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nχ²(5) = 41.46:\n\nThe chi-squared test statistic for the overall model fit.\n\np = 0.00:\n\nThe p-value for the model, indicating that the model is statistically significant (p &lt; 0.05).\n\nPseudo-R² (Cragg-Uhler) = 0.14:\n\nThe pseudo-R², similar to the R² in linear regression, but interpreted more loosely. Indicates that 14% of the variation in the outcome is explained by the model.\n\nPseudo-R² (McFadden) = 0.08:\n\nAnother form of pseudo-R², typically smaller than Cragg-Uhler’s.\n\nAIC = 470.52:\n\nThe Akaike Information Criterion, a measure of model fit that penalizes for complexity. Lower values indicate a better fit.\n\nBIC = 494.47:\n\nThe Bayesian Information Criterion, similar to AIC but with a stronger penalty for complexity."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-coefficients",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#output-coefficients",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Output: Coefficients",
    "text": "Output: Coefficients\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntercept (Est. = -3.99, p = 0.00):\n\nThe log-odds of being admitted when all predictors are zero. The negative value indicates a low baseline probability of admission.\n\nGRE (Est. = 0.00, p = 0.04):\n\nThe coefficient for GRE is positive and statistically significant (p &lt; 0.05), but the value is very close to zero. This means GRE has a very small positive effect on the log-odds of admission.\n\nGPA (Est. = 0.80, p = 0.02):\n\nA positive coefficient indicates that higher GPA increases the likelihood of admission. It is statistically significant (p &lt; 0.05).\n\nRank 2 (Est. = -0.68, p = 0.03):\n\nBeing in rank 2 (relative to rank 1) decreases the odds of admission. This is statistically significant.\n\nRank 3 (Est. = -1.34, p = 0.00):\n\nA strong negative effect on the log-odds of admission for rank 3. It is statistically significant.\n\nRank 4 (Est. = -1.55, p = 0.00):\n\nBeing in rank 4 strongly reduces the chances of admission, statistically significant with a very low p-value."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-odds-and-probabilities-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-odds-and-probabilities-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "From Log-Odds to Odds and Probabilities",
    "text": "From Log-Odds to Odds and Probabilities\n\n\nLog-Odds (Linear Equation)\n\nThe log-odds (logit) from a logistic regression model can be written as:\n\\[\n\\text{Log-Odds}(Y = 1) = \\beta_0 + \\beta_1 \\times X_1 + \\beta_2 \\times X_2 + \\dots + \\beta_k \\times X_k\n\\]\nWhere:\n\n\\(\\beta_0\\) is the intercept,\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\) are the coefficients for predictors \\(X_1, X_2, \\dots, X_k\\)."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-odds",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-odds",
    "title": " MGMT 30500: Business Statistics ",
    "section": "From Log-Odds to Odds",
    "text": "From Log-Odds to Odds\n\n\nConverting Log-Odds to Odds\n\nTo convert the log-odds to odds, use the exponential function:\n\\[\n\\text{Odds} = e^{\\text{Log-Odds}} = e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k}\n\\]\nThe odds represent the ratio of the probability of the event happening to the probability of the event not happening."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-probabilities",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#from-log-odds-to-probabilities",
    "title": " MGMT 30500: Business Statistics ",
    "section": "From Log-Odds to Probabilities",
    "text": "From Log-Odds to Probabilities\n\n\nComputing the Probability\n\nTo compute the probability from the log-odds, use the logistic function:\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-\\text{Log-Odds}}}\n\\]\nOr equivalently:\n\\[\nP(Y = 1) = \\frac{e^{\\text{Log-Odds}}}{1 + e^{\\text{Log-Odds}}}\n\\]\nThis formula converts the log-odds into a probability value between 0 and 1."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-log-odds",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-log-odds",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Results: Log-Odds",
    "text": "Interpretation of Results: Log-Odds\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRE: For a one-unit increase in gre, the log-odds of the dependent variable (admit) increase by 0.002;\nGPA: For a one-unit increase in gpa, the log-odds of the DV increase by 0.804;\nRank: In the case of the variable rank, being a graduate of an institution with rank\\(=2\\) changes the log-odds by \\(-0.675\\) compared to an institution with rank\\(=1\\).\n\n\nSource: How do I interpret Odds Ratios in Logistic Regression?"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-odds",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-odds",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Results: Odds",
    "text": "Interpretation of Results: Odds\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRE: For a one-unit increase in GRE, the odds of being admitted increase by a factor of: \\[\ne^{0.002} \\approx 1.002\n\\] This means that for every additional point in GRE, the odds of admission increase by 0.2%.\nGPA: For a one-unit increase in GPA, the odds of being admitted increase by a factor of: \\[\ne^{0.804} \\approx 2.23\n\\] This means that for each additional GPA point, the odds of admission more than double (2.23 times higher).\nRank (Rank 2): For being in Rank 2 compared to Rank 1, the odds of admission decrease by a factor of: \\[\ne^{-0.675} \\approx 0.51\n\\] This means that students from Rank 2 institutions have approximately half the odds of being admitted compared to those from Rank 1 institutions."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-probability",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-probability",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Results: Probability",
    "text": "Interpretation of Results: Probability\n\n\n\n\n\n\n\n\n\nObservations\n400\n\n\nDependent variable\nadmit\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\n𝛘²(5)\n41.46\n\n\np\n0.00\n\n\nPseudo-R² (Cragg-Uhler)\n0.14\n\n\nPseudo-R² (McFadden)\n0.08\n\n\nAIC\n470.52\n\n\nBIC\n494.47\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n-3.99\n1.14\n-3.50\n0.00\n\n\ngre\n0.00\n0.00\n2.07\n0.04\n\n\ngpa\n0.80\n0.33\n2.42\n0.02\n\n\nrank2\n-0.68\n0.32\n-2.13\n0.03\n\n\nrank3\n-1.34\n0.35\n-3.88\n0.00\n\n\nrank4\n-1.55\n0.42\n-3.71\n0.00\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo convert log-odds into probabilities:\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-(\\text{log-odds})}}\n\\]\n\nGRE: The change in probability for a one-unit increase in GRE is minimal, given the very small log-odds coefficient (0.002). The probability increases slightly for each GRE point.\nGPA: A one-unit increase in GPA substantially increases the probability of admission, as the log-odds increase by 0.804. This leads to a noticeable jump in the probability of being admitted.\nRank (Rank 2): Being in Rank 2 compared to Rank 1 decreases the probability of admission due to the reduction in log-odds by 0.675. This results in a lower probability of being admitted for Rank 2 graduates."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-predicted-probability",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#interpretation-of-results-predicted-probability",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Results: Predicted Probability",
    "text": "Interpretation of Results: Predicted Probability\n\\[\np = \\frac{\\exp(\\beta x)}{1 + \\exp(\\beta x)}\n\\]"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#example-output-conclusion",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#example-output-conclusion",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example Output: Conclusion",
    "text": "Example Output: Conclusion\n\nThis model indicates that GPA, GRE, and rank significantly influence the likelihood of admission, with lower ranks (higher numerical values) significantly decreasing the chances of admission."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#additional-material",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#additional-material",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Additional Material",
    "text": "Additional Material\n\nBinomial Logistic Regression.\nHow do I interpret Odds Ratios in Logistic Regression?.\nMultinomial Logistic Regression.\nTutorial: Leveraging Labelled Data in R.\nUCLA: Data Analysis Examples.\nIntroduction to Econometrics with R.\nBeyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-1",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nWhen to Use Logistic Regression: Use logistic regression when the dependent variable is binary (e.g., success/failure).\nProbabilistic Interpretation: Models the probability of an event occurring based on predictor variables.\nInterpretation of Coefficients: Log-Odds:\n\\[\n\\ln\\left(\\frac{P(Y = 1)}{1 - P(Y = 1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\nCoefficients represent the change in log-odds for a one-unit increase in the predictor.\n\nInterpretation of Coefficients: Odds Ratio:\n\\[\n\\text{Odds Ratio} = e^{\\beta_i}\n\\]\n\nAn odds ratio &gt; 1 indicates increased odds of the event occurring with a one-unit increase in \\(X_i\\).\nAn odds ratio &lt; 1 indicates decreased odds."
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-2",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\nSome key takeaways from this session:\n\nLog-Odds to Odds:\n\\[\n\\text{Odds} = e^{\\text{Log-Odds}}\n\\]\nOdds to Probability:\n\\[\nP(Y = 1) = \\frac{\\text{Odds}}{1 + \\text{Odds}}\n\\]\nDirect Conversion:\n\\[\nP(Y = 1) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p}}\n\\]"
  },
  {
    "objectID": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-3",
    "href": "lecture_slides/16_02_logistic_regression/16_02_logistic_regression.html#summary-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nMaximum Likelihood Estimation (MLE): Estimates the parameters that maximize the likelihood of observing the given data.\nModel Assessment: AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better balance between model fit and complexity.\nThe logistic function ensures predicted probabilities are between 0 and 1.\nCoefficients can be interpreted in terms of log-odds and odds ratios.\nModel fit and predictor significance can be evaluated using pseudo R-squared, AIC, BIC, and statistical tests.\nUnderstanding how to convert between log-odds, odds, and probabilities is crucial for interpretation."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#overview",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nGeneral Linear Model\nModeling Curvilinear Relationships\nInteraction\nTransformations\nNonlinear Models That Are Intrinsically Linear\n\n\n\nDetermining When to Add or Delete Variables\n\nVariable Selection Procedures\n\nStepwise Method\nForward Method\nBackward Method\nBest Subsets Method"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#general-linear-model-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#general-linear-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "General Linear Model",
    "text": "General Linear Model\n\nModels in which the parameters \\((\\beta_0, \\beta_1, \\ldots, \\beta_p)\\) all have exponents of one are called linear models.\nA general linear model involving \\(p\\) independent variables (\\(z_i\\)’s) is:\n\n\n\\[\ny = \\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + \\ldots + \\beta_p z_p + \\epsilon\n\\]\nwhere each independent variable \\(z_i\\) is a (linear or nonlinear) function of \\(x_1, x_2, \\ldots, x_k\\) (the variables for which data have been collected).\n\nHere, \\(y\\) can be a function of the original response variable as well."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#general-linear-model-2",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#general-linear-model-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "General Linear Model",
    "text": "General Linear Model\n\nThe simplest case is when we have collected data for just one variable \\(x_1\\) and want to estimate \\(y\\) by using a straight-line relationship. In this case \\(z_1 = x_1\\).\nThis model is called a simple first-order model with one predictor variable.\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Modelling Curvilinear Relationships",
    "text": "Modelling Curvilinear Relationships"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-2",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Modelling Curvilinear Relationships",
    "text": "Modelling Curvilinear Relationships\n\nSome non-linear models can be expressed as a general linear model.\nTo account for a curvilinear relationship, we might consider a second-order model with one predictor variable \\((x_1)\\):\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\epsilon\n\\]\n\nIt is a linear model because we can set: \\(z_1 = x_1\\) and \\(z_2 = x_1^2\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-3",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#modelling-curvilinear-relationships-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Modelling Curvilinear Relationships",
    "text": "Modelling Curvilinear Relationships\n\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\epsilon\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#interpretation-of-independent-variable-effect-in-a-second-order-model",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#interpretation-of-independent-variable-effect-in-a-second-order-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Independent Variable Effect in a Second-Order Model",
    "text": "Interpretation of Independent Variable Effect in a Second-Order Model\n\n\\(\\beta_1\\): Represents the linear effect of \\(x_1\\) on \\(y\\). It gives the initial (or marginal) change in \\(y\\) for a one-unit increase in \\(x_1\\) when \\(x_1^2\\) is held constant.\n\\(\\beta_2\\): Represents the quadratic effect of \\(x_1\\) on \\(y\\). It determines whether the curve opens upwards \\((\\beta_2 &gt; 0)\\) or downwards \\((\\beta_2 &lt; 0)\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#marginal-effect-of-x_1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#marginal-effect-of-x_1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Marginal Effect of \\(x_1\\)",
    "text": "Marginal Effect of \\(x_1\\)\n\nThe overall effect of \\(x_1\\) on \\(y\\) can be expressed as:\n\n\n\\[\n\\frac{dy}{dx_1} = \\beta_1 + 2\\beta_2 x_1\n\\]\n\nThis shows that the effect of \\(x_1\\) on \\(y\\) changes as \\(x_1\\) increases or decreases due to the presence of the quadratic term \\(x_1^2\\).\nInstead of a constant change (as in linear models), the presence of \\(2\\beta_2 x_1\\) shows a varying slope depending on the value of \\(x_1\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#practical-interpretation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#practical-interpretation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Practical Interpretation",
    "text": "Practical Interpretation\n\nIf \\(\\beta_2 &gt; 0\\), \\(y\\) increases at an increasing rate as \\(x_1\\) increases, resulting in a U-shaped curve.\nIf \\(\\beta_2 &lt; 0\\), \\(y\\) increases at a decreasing rate and then decreases, resulting in an inverted U-shaped curve.\nThe effect of \\(x_1\\) should always be considered in light of both \\(\\beta_1\\) and \\(\\beta_2\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#interaction-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#interaction-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interaction",
    "text": "Interaction\n\nIf the original data set consists of observations for \\(y\\) and two independent variables, \\(x_1\\) and \\(x_2\\), we might develop a second-order model with two predictor variables \\((x_1\\) and \\(x_2)\\) with interaction:\n\n\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\n\\]\n\nThe variable \\(x_1 x_2\\) is added to account for the potential effects of the two variables acting together.\n\\(\\beta_3\\) measures the interaction effect."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interaction",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interaction",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Interaction",
    "text": "Example: Interaction\n\nLets check the regression study conducted by Tyler Personal Care for one of its new shampoo products. Two factors believed to have the most influence on sales are:\n\nUnit selling price\nAdvertising expenditure\n\nTo investigate the effects of these two variables on sales, prices of $2.00, $2.50, and $3.00 were paired with advertising expenditures of $50,000 and $100,000 in 24 test markets."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-difference-in-mean-sales",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-difference-in-mean-sales",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Difference in Mean Sales",
    "text": "Example: Difference in Mean Sales\n\n\nMean Sales (1000s) for the Tyler Personal Care Example\n\n\n\nAdvertising Expenditure\n$2.00\n$2.50\n$3.00\n\n\n\n\n$50,000\n461\n364\n332\n\n\n$100,000\n808\n646\n375\n\n\n\n\n\n\nWith a price of $2.00, the difference in mean sales between advertising expenditures of $50,000 and $100,000 is:\n\\[\n808,000 - 461,000 = 347,000 \\, units\n\\]\nWhen the price is $2.50, the difference is:\n\\[\n646,000 - 364,000 = 282,000 \\, units\n\\]\nWhen the price is $3.00, the difference is:\n\\[\n375,000 - 332,000 = 43,000 \\, units\n\\]\n\n\nClearly, the difference in mean sales between advertising expenditures depends on the price of the product. The effect of increased advertising expenditure diminishes at higher selling prices, providing evidence of interaction between the price and advertising expenditure variables."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-regression-model-with-interaction",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-regression-model-with-interaction",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Regression Model with Interaction",
    "text": "Example: Regression Model with Interaction\n\nTo account for the effect of interaction, we use the following regression model:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\n\\]\nWhere:\n\n\\(y\\) = unit sales (1000s)\n\\(x_1\\) = price ($)\n\\(x_2\\) = advertising expenditure ($1000s)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-estimated-regression-equation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-estimated-regression-equation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Estimated Regression Equation",
    "text": "Example: Estimated Regression Equation\nUsing the estimated regression equation:\n\\[\n\\text{Sales} = -275.8333 + 175 \\, \\text{Price} + 19.68 \\, \\text{AdvExp} - 6.08 \\, \\text{PriceAdv}\n\\]\nWhere:\n\nSales = unit sales (1000s)\nPrice = price of the product ($)\nAdvExp = advertising expenditure ($1000s)\nPriceAdv = interaction term (Price times AdvExp)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-significance-of-interaction",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-significance-of-interaction",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Significance of Interaction",
    "text": "Example: Significance of Interaction\n\n\nThe \\(p\\)-value corresponding to the \\(t\\)-test for PriceAdv is 0.0000, which indicates significant interaction between the price of the product and the advertising expenditure."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interpretation-of-coefficients",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interpretation-of-coefficients",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Interpretation of Coefficients",
    "text": "Example: Interpretation of Coefficients\n\n\n\\(\\beta_0\\): Intercept. Represents the expected value of \\(y\\) when \\(x_1\\) and \\(x_2\\) are zero.\n\\(\\beta_1\\): Effect of \\(x_1\\) on \\(y\\) when \\(x_2 = 0\\).\n\\(\\beta_2\\): Effect of \\(x_2\\) on \\(y\\) when \\(x_1 = 0\\).\n\\(\\beta_3\\): Interaction effect between \\(x_1\\) and \\(x_2\\). Indicates how the relationship between \\(x_1\\) and \\(y\\) changes with different values of \\(x_2\\), and vice-versa."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-marginal-effects",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-marginal-effects",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Marginal Effects",
    "text": "Example: Marginal Effects\n\nEffect of \\(x_1\\):\n\n\n\\[\n\\frac{\\partial y}{\\partial x_1} = \\beta_1 + \\beta_3 x_2\n\\]\n\nEffect of \\(x_2\\):\n\n\n\n\\[\n\\frac{\\partial y}{\\partial x_2} = \\beta_2 + \\beta_3 x_1\n\\]\n\n\nThe effect of \\(x_1\\) on \\(y\\) depends on \\(x_2\\), and the effect of \\(x_2\\) on \\(y\\) depends on \\(x_1\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interpretation-of-interaction-effect",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-interpretation-of-interaction-effect",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Interpretation of Interaction Effect",
    "text": "Example: Interpretation of Interaction Effect\n\n\nIf \\(\\beta_3 &gt; 0\\), a positive (or negative) effect of \\(x_1\\) on \\(y\\) increases as \\(x_2\\) increases.\nIf \\(\\beta_3 &lt; 0\\), a positive (or negative) effect of \\(x_1\\) on \\(y\\) decreases as \\(x_2\\) increases."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-coefficient-interpretation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-coefficient-interpretation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Coefficient Interpretation",
    "text": "Example: Coefficient Interpretation\n\n\nPrice: \\(175\\) - When AdvExp is zero, a one-unit increase in Price leads to an expected increase of 175 units in \\(y\\).\nAdvExp: \\(19.68\\) - When Price is zero, a one-unit increase in AdvExp results in an expected increase of 19.68 units in \\(y\\).\nPriceAdv: \\(-6.08\\)- Interaction effect: A one-unit increase in AdvExp decreases the effect of Price on \\(y\\) by 6.08 units (and vice versa).\n\n\nInterpretation of Interaction Effect\n\n\nThe interaction term \\(\\beta_3\\) (PriceAdv) is negative.\n\nAs AdvExp increases, the positive effect of Price on \\(y\\) decreases.\nSuggests diminishing returns on Price when AdvExp is already high (or vice versa).\n\nAn increase in Advertising Expenditures may lead to higher sales, but this effect diminishes as more the Price increases.\nThe interaction effect is negative and significant, showing that the combined effect of Price and AdvExp on \\(y\\) is not purely additive.\nTakeaway: Adjustments to Price or AdvExp should consider their interaction, as increasing both may not yield linear increases in \\(y.\\)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#original-data",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#original-data",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Original Data",
    "text": "Original Data"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#log-transformation-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#log-transformation-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Log Transformation",
    "text": "Log Transformation"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#possibile-logarithmic-transformations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#possibile-logarithmic-transformations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Possibile Logarithmic Transformations",
    "text": "Possibile Logarithmic Transformations\n\n\n\n\n\n\n\n\n\n\nX\nlogX\n\n\n\n\nY\nlinear\\(\\hat{Y}_i = \\alpha + \\beta X_i\\)\nlinear-log\\(\\hat{Y}_i = \\alpha + \\beta \\log X_i\\)\n\n\nlogY\nlog-linear\\(\\log \\hat{Y}_i = \\alpha + \\beta X_i\\)\nlog-log\\(\\log \\hat{Y}_i = \\alpha + \\beta \\log X_i\\)\n\n\n\n\n\n\nSource: Linear Regression Models with Logarithmic Transformations"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#what-changes-after-the-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#what-changes-after-the-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "What Changes After the Transformation",
    "text": "What Changes After the Transformation\n\n\nYou should be cautious when interpreting and reporting the findings of the model.\nThe interpretation varies based on the variable that was transformed (dependent variable, independent variable, or both).\nAs a general rule, you should always keep in mind the logic:\n\n\n\n“What does a one-unit change in this transformed variable mean in terms of the original variable?”\n\n\nThe Effect Book"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#log-transformation-summary",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#log-transformation-summary",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Log Transformation Summary",
    "text": "Log Transformation Summary\n\n\n\n\n\n\n\n\n\n\n\nModel\nModel Equation\nInterpretation of \\(\\beta_1\\)\nInterpretation\n\n\n\n\nLevel-level\n\\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\n\\(\\Delta y = \\beta_1 \\Delta x\\)\nA one-unit change in \\(x\\) results in a \\(\\beta_1\\) unit change in \\(y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel-log\n\\(y = \\beta_0 + \\beta_1 \\log(x) + \\epsilon\\)\n\\(\\Delta y = (\\beta_1 / 100) \\% \\Delta x\\)\nA 1% change in \\(x\\) results in a \\(\\beta_1/100\\) unit change in \\(y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-level\n\\(\\log(y) = \\beta_0 + \\beta_1 x + \\epsilon\\)\n\\(\\%\\Delta y = (100\\beta_1) \\Delta x\\)\nA one-unit change in \\(x\\) results in a \\(\\beta_1\\%\\) change in \\(y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-log\n\\(\\log(y) = \\beta_0 + \\beta_1 \\log(x) + \\epsilon\\)\n\\(\\%\\Delta y = \\beta_1 \\% \\Delta x\\)\nA 1% change in \\(x\\) results in a \\(\\beta_1\\%\\) change in \\(y\\)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-log-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-log-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Log Transformation",
    "text": "Example: Log Transformation\n\nPredict Miles-Per-Gallon (MPG) according to the automobile Weight (in pounds):\n\n\n\n\\[\n\\text{MPG} = 56.0957 - 0.0116 \\times \\text{Weight}\n\\]\n\n\n\n\n\n\n\n\n\n\nThe pattern does not look like the the horizontal band we should expect to find if the assumptions about the error term are valid.\nVariability in the residuals appears to increase as the value of \\(\\hat{y}\\) increases.\n\n\n\n\n\\[\n\\text{LnMPG} = 4.5242 - 0.0005 \\times \\text{Weight}\n\\]\n\n\n\n\n\n\n\n\n\n\nThe wedge-shaped pattern disappeared.\nThe model with the logarithm of miles per gallon as the dependent variable provides an excellent fit to the oberved data."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#nonlinear-models-that-are-intrinsically-linear-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#nonlinear-models-that-are-intrinsically-linear-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Nonlinear Models That Are Intrinsically Linear",
    "text": "Nonlinear Models That Are Intrinsically Linear\n\nModels in which the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) have exponents other than one are called nonlinear models.\nFor the case of the exponential model, we can perform a transformation of variables that will enable us to perform regression analysis using the general linear model.\nThe exponential model involves the following regression equation:\n\\[\nE(y) = \\beta_0 \\beta_1^x\n\\]\nThis model is appropriate when the dependent variable \\(y\\) increases or decreases by a constant percentage, instead of by a fixed amount, as \\(x\\) increases."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-of-exponential-model",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#example-of-exponential-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example of Exponential Model",
    "text": "Example of Exponential Model\n\nSuppose sales for a product \\(y\\) are related to advertising expenditure \\(x\\) (in $1000s) according to the following regression equation:\n\\[\nE(y) = 500(1.2)^x\n\\] Thus,\n\nfor \\(x = 1\\), \\(E(y) = 500(1.2)^1 = 600\\)\nfor \\(x = 2\\), \\(E(y) = 500(1.2)^2 = 720\\)\nfor \\(x = 3\\), \\(E(y) = 500(1.2)^3 = 864\\)\n\nNote that \\(E(y)\\) is not increasing by a constant amount in this case, but by a constant percentage. The percentage increase is 20%."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#logarithmic-transformation-of-the-model",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#logarithmic-transformation-of-the-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Logarithmic Transformation of the Model",
    "text": "Logarithmic Transformation of the Model\n\nWe can transform this nonlinear model to a linear model by taking the natural logarithm of both sides of the equation:\n\\[\n\\ln E(y) = \\ln \\beta_0 + x \\ln \\beta_1\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#linearized-model",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#linearized-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linearized Model",
    "text": "Linearized Model\n\nNow, if we let \\(y' = \\ln E(y)\\), \\(\\beta'_0 = \\ln \\beta_0\\), and \\(\\beta'_1 = \\ln \\beta_1\\), we can rewrite the equation as:\n\\[\ny' = \\beta'_0 + \\beta'_1 x\n\\]\nThe formulas for simple linear regression can now be used to develop estimates of \\(\\beta'_0\\) and \\(\\beta'_1\\). Denoting the estimates as \\(b'_0\\) and \\(b'_1\\), leads to the following estimated regression equation:\n\\[\n\\hat{y'} = b'_0 + b'_1 x\n\\]\nTo obtain predictions of the original dependent variable \\(y\\) given a value of \\(x\\), we would first substitute the value of \\(x\\) into the equation above to compute \\(\\hat{y'}\\), and then raise \\(e\\) to the power of \\(\\hat{y'}\\) to obtain the prediction of \\(y\\), or the expected value of \\(y\\), in its original units."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#linearized-model---example-prediction",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#linearized-model---example-prediction",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linearized Model - Example Prediction",
    "text": "Linearized Model - Example Prediction\n\nGiven the estimates:\n\n\\(b'_0 = 3.5\\)\n\\(b'_1 = 0.2\\)\n\nLet’s predict \\(y\\) when the advertising expenditure \\(x = 5\\) (in $1000s).\nUsing the linearized equation we calculate \\(y'\\):\n\\[\ny' = b'_0 + b'_1 \\cdot x = 3.5 + 0.2 \\cdot 5 = 4.5\n\\]\nNow, exponentiate \\(y'\\) to get the predicted \\(y\\):\n\\[\ny = e^{4.5} \\approx 90.02\n\\]\nThus, the predicted sales \\(y\\) when the advertising expenditure is 5 (in $1000s) is approximately 90 units (in $1000s)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#other-transformations-to-consider-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#other-transformations-to-consider-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Other Transformations to Consider",
    "text": "Other Transformations to Consider\n\n\nSquare-root: \\(\\sqrt{x}\\)\n\nLogarithmic: \\(\\log_{10}(x), \\log_{10}(y), \\ln(x)\\), etc.\n\nReciprocal: \\(1/y, 1/x\\)\n\nExponential: \\(e^x, e^y\\)\n\nSquare: \\(x^2, y^2\\)\n\nPower: \\(x^k, y^k\\)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#square-root-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#square-root-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Square-Root Transformation",
    "text": "Square-Root Transformation\n\nAdd or use \\(\\sqrt{x}\\) term or \\((x^{0.5})\\)"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#logarithmic-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#logarithmic-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Logarithmic Transformation",
    "text": "Logarithmic Transformation\n\nAdd or use \\(\\ln(x) \\text{ or } \\log(x)\\) term."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#reciprocal-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#reciprocal-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Reciprocal Transformation",
    "text": "Reciprocal Transformation\n\nAdd or use \\(1/x\\) term."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#exponential-transformation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#exponential-transformation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Transformation",
    "text": "Exponential Transformation\n\nChange \\(y\\) to \\(\\ln(y)\\) as the new response variable."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#power-transformations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#power-transformations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Power Transformations",
    "text": "Power Transformations\n\nAdd \\(x^2\\) or \\(x^k\\) term."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#overview-of-predictor-evaluation",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#overview-of-predictor-evaluation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview of Predictor Evaluation",
    "text": "Overview of Predictor Evaluation\n\nStatistical Significance: Indicates whether the relationship between a predictor and the dependent variable is unlikely to have occurred by chance.\nEffectiveness: Reflects the practical impact or importance of the predictor on the dependent variable.\n\nAssessed by observing changes in adjusted R-squared when the predictor is included.\nStatistical significance is evaluated using the p-value."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#four-scenarios-for-predictors",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#four-scenarios-for-predictors",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Four Scenarios for Predictors",
    "text": "Four Scenarios for Predictors\n\n\n\n\n\n\n\n\n\n\nStatistically Significant(e.g. p-value &lt; 0.05)\nNot Statistically Significant(e.g. p-value ≥ 0.05)\n\n\n\n\nEffective (Adjusted R-squared increases significantly)\nScenario 1Predictor is both statistically significant and effective.\nScenario 3Effective but not statistically significant.\n\n\nNot Effective (Adjusted R-squared does not increase significantly)\nScenario 2Statistically significant but not effective.\nScenario 4Not statistically significant nor effective."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-1-statistically-significant-and-effective",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-1-statistically-significant-and-effective",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scenario 1: Statistically Significant and Effective",
    "text": "Scenario 1: Statistically Significant and Effective\nCharacteristics\n\np-value below chosen significance level (e.g., \\(p &lt; 0.05\\))\nAdjusted R-squared increases meaningfully when the predictor is included\n\n\nInterpretation\n\nPredictor reliably contributes to the dependent variable’s variance.\nBoth statistically and practically meaningful.\n\n\n\nExample\n\nHealthcare: Adding “age” as a predictor in a model for blood pressure yields a p-value of \\(p &lt; 0.001\\) and increases the adjusted R-squared from 0.30 to 0.45.\nAge is both a statistically significant and effective predictor of blood pressure."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-2-statistically-significant-but-not-effective",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-2-statistically-significant-but-not-effective",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scenario 2: Statistically Significant but Not Effective",
    "text": "Scenario 2: Statistically Significant but Not Effective\nCharacteristics\n\np-value below the significance threshold\nMinimal change in adjusted R-squared: negligible improvement in the model’s explanatory power\n\n\nInterpretation\n\nStatistically reliable but lacks practical impact.\nCommon in large samples where even small effects become significant.\n\n\n\nExample\n\nEconomics: Adding “hair color” as a predictor of income yields p = 0.02, but increases adjusted R-squared from 0.25 to 0.251.\nHair color is statistically significant but not practically effective."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-3-not-statistically-significant-but-effective",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-3-not-statistically-significant-but-effective",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scenario 3: Not Statistically Significant but Effective",
    "text": "Scenario 3: Not Statistically Significant but Effective\nCharacteristics\n\np-value exceeds the significance level (e.g., \\(p &gt; 0.05\\))\nSubstantial increase in adjusted R-squared\n\n\nInterpretation\n\nMeaningful effect, but lacks statistical support.\nMay need a larger sample size or further refinement.\n\n\n\nExample\n\nEducation: Adding “hours of sleep” as a predictor of student performance increases adjusted R-squared from 0.40 to 0.50 but yields p = 0.08.\n“Hours of sleep” has a practical impact but isn’t statistically significant."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-4-not-statistically-significant-and-not-effective",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#scenario-4-not-statistically-significant-and-not-effective",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Scenario 4: Not Statistically Significant and Not Effective",
    "text": "Scenario 4: Not Statistically Significant and Not Effective\nCharacteristics\n\np-value above the significance threshold\nNo increase in adjusted R-squared\n\n\nInterpretation\n\nPredictor lacks both statistical and practical value.\nLikely safe to exclude from the model.\n\n\n\nExample\n\nMarketing: Adding “shoe size” to predict customer satisfaction yields p = 0.60 and decreases adjusted R-squared from 0.35 to 0.34."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#strategies-for-adding-or-removing-variables-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#strategies-for-adding-or-removing-variables-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Strategies for Adding or Removing Variables",
    "text": "Strategies for Adding or Removing Variables\n\n\n\n\n#\nStrategy\nDescription\nAdd Variables\nRemove Variables\n\n\n\n\n1\nP-Value\nBased on statistical significance\nIf p-value &lt; 0.05\nIf p-value &gt; 0.05\n\n\n2\nAdjusted R-Squared\nChecks if model fit improves\nIf adjusted \\(R^2\\) increases\nIf adjusted \\(R^2\\) decreases\n\n\n3\nF-Test\nCompares models with and without added variables\nIf F-test indicates significant improvement\nIf F-test shows no significant improvement\n\n\n4\nAIC or BIC\nBalances model fit and complexity\nIf AIC/BIC decreases\nIf AIC/BIC increases\n\n\n5\nStepwise Regression\nAutomated selection procedure based on statistical contribution\nAdd variables with high statistical contribution\nRemove variables with low contribution\n\n\n6\nMulticollinearity (VIF)\nThe Variance Inflation Factor detects multicollinearity between independent variables\nUse the full model\nIf VIF &gt; 10\n\n\n7\nBest Subset Selection\nCompares all possible combinations of predictors to identify the best model\nAdds the combination of predictors with the best performance based on chosen criteria (e.g., adjusted \\(R^2\\))\nN/A; evaluates models by selecting the best subset\n\n\n8\nCross-Validation\nAssesses model performance across different data subsets\nIf cross-validation performance improves\nIf cross-validation performance worsens\n\n\n9\nGood vs Bad Controls\nFor causal inference purposes\nAdd good controls that help block non-causal paths\nRemove bad controls that open new spurious paths\n\n\n10\nTheoretical Justification\nAdds or removes variables based on theory, domain knowledge, or experience\nAdd based on theory or domain knowledge\nRemove variables that are irrelevant, regardless of statistical significance"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-or-removing-variables",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-or-removing-variables",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding or Removing Variables",
    "text": "Adding or Removing Variables\n\nWe will focus on the following:\n\nStepwise regression\nForward selection\nBackward elimination\nBest-subsets regression.\n\nThe first three procedures are iterative; at each step, a single independent variable is added or deleted, and the new model is evaluated. The process continues until a stopping criterion indicates that the procedure cannot find a better model.\nThe best-subsets procedure is not a one-variable-at-a-time procedure; it evaluates regression models involving different subsets of the independent variables."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nWe can use an F-test to determine whether it is advantageous to add one or more independent variables to a multiple regression model.\nThis is based on determining the reduction in the error sum of squares (SSE) resulting from adding variables.\nThe null and alternative hypotheses are defined as:\n\\[\nH_0: \\beta_{q+1} = \\beta_{q+2} = \\cdots = \\beta_p = 0\n\\]\n\\[\nH_a: \\text{One or more of the parameters is not equal to zero}\n\\]\nwhere \\(q\\) is the number of independent variables in the first model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nLet’s illustrate this using the Butler Trucking example.\nThe regression equation with miles traveled \\(x_1\\) as the only independent variable is:\n\\[\n\\hat{y} = 1.2739 + 0.0678x_1\n\\]\nThe error sum of squares for this model is:\n\\[\nSSE(x_1) = 8.0287\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-2",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nWhen the number of deliveries \\(x_2\\) is added, the regression equation becomes:\n\\[\n\\hat{y} = -0.8687 + 0.0611x_1 + 0.9234x_2\n\\]\nThe error sum of squares for this model is:\n\\[\nSSE(x_1, x_2) = 2.2994\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-3",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nThe reduction in SSE from adding \\(x_2\\) to the model is:\n\\[\nSSE(x_1) - SSE(x_1, x_2) = 8.0287 - 2.2994 = 5.7293\n\\]\nWe can conduct a F-test to determine if this reduction is significant:\n\\[\nF = \\frac{\\frac{SSE(x_1) - SSE(x_1, x_2)}{1}}{\\frac{SSE(x_1, x_2)}{n - p - 1}}\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-4",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\n\n\nSubstituting the values:\n\\[\nF = \\frac{5.7293}{1} \\Big/ \\frac{2.2994}{7} = 17.44\n\\] Where,\n\n\\(n = 10\\)\n\\(p = 2\\)\n\nConclusion from F-Test\nUsing Excel, we obtain a \\(p\\)-value of \\(0.0042\\) for the calculated F-statistic. Since the \\(p\\)-value is less than the significance level \\(\\alpha = 0.05,\\) we reject the null hypothesis. Thus, adding \\(x_2\\) results in a significant reduction in SSE.\nThe t-test and F-test are equivalent when only one independent variable is being added, and we can use either to assess significance."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-5",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nIn the stepwise regression, forward selection, and backward elimination procedures, the criterion for selecting an independent variable to add or delete from the model at each step is based on the F-statistic.\nSuppose we are considering adding \\(x_2\\) to a model involving \\(x_1\\) or deleting \\(x_2\\) from a model involving \\(x_1\\) and \\(x_2\\). To test whether the addition or deletion of \\(x_2\\) is statistically significant, the null and alternative hypotheses can be stated as follows:\n\\[\nH_0: \\beta_2 = 0\n\\]\n\\[\nH_a: \\beta_2 \\neq 0\n\\]"
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-6",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-f-test-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: F-test",
    "text": "Adding Variables in Regression Models: F-test\nWe saw that:\n\\[\nF = \\frac{{SSE(x_1) - SSE(x_1, x_2)}}{1} \\div \\frac{{SSE(x_1, x_2)}}{n - p - 1}\n\\]\ncan be used as a criterion for determining whether the presence of \\(x_2\\) in the model causes a significant reduction in the error sum of squares.\nThe p-value corresponding to this F-statistic is used to determine whether an independent variable should be added or deleted from the regression model. The usual rejection rule applies: Reject \\(H_0\\) if p-value \\(\\leq \\alpha\\)."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression",
    "text": "Stepwise Regression\nThe stepwise regression procedure begins each step by determining whether any of the variables already in the model should be removed.\nIt does so by first computing an F-statistic and corresponding p-value for each independent variable in the model.\nRefering to the level of significance \\(\\alpha\\) for determining whether an independent variable should be removed from the model as p Value to Leave, if the p-value for any independent variable is greater than p Value to Leave, the independent variable with the largest p-value is removed, and the stepwise regression procedure begins a new step."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-process",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-process",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Process",
    "text": "Stepwise Regression Process\nIf no independent variable can be removed from the model, the procedure attempts to enter another independent variable into the model.\nIt does so by first computing an F-statistic and corresponding p-value for each independent variable that is not in the model.\nRefering to the level of significance \\(\\alpha\\) for determining whether an independent variable should be entered into the model as p Value to Enter.\nThe independent variable with the smallest p-value is entered into the model provided its p-value is less than p Value to Enter. The procedure continues in this manner until no independent variables can be deleted from or added to the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-limitations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-limitations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Limitations",
    "text": "Stepwise Regression Limitations\nBecause the one-at-a-time procedures do not consider every possible subset for a given number of independent variables, they will not necessarily select the model with the highest R-Square value.\nIn summary, at each step of the stepwise regression procedure, the first consideration is to see whether any independent variable can be removed from the current model. If none of the independent variables can be removed from the model, the procedure checks to see whether any of the independent variables that are not currently in the model can be entered."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-considerations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-considerations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Considerations",
    "text": "Stepwise Regression Considerations\nBecause of the nature of the stepwise regression procedure, an independent variable can enter the model at one step, be removed at a subsequent step, and then enter the model at a later step.\nThe procedure stops when no independent variables can be removed from or entered into the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Example",
    "text": "Stepwise Regression Example\nLet’s consider a dataset with the following variables:\n\nIncome: Household income (in $1000s)\nAge: Age of the head of the household\nEducation: Years of education\nSpending: Annual spending on goods (dependent variable)\n\n\nInitial Model\nWe start by considering an initial model with Income as the only predictor of Spending.\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\epsilon\n\\]\nUsing an F-statistic and p-value, we find Income is significant, so we keep it in the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Example",
    "text": "Stepwise Regression Example\n\n\nThe remaining variables, Age and Education, are candidates for inclusion.\nCalculating p-values for Inclusion\n\nAdd Each Variable Separately: Create two new models:\n\nAdding Education:\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\beta_2 \\cdot Education + \\epsilon\n\\]\nAdding Age:\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\beta_3 \\cdot Age + \\epsilon\n\\]\n\nEstimate Coefficients: For each model, estimate the new coefficients (\\(\\hat{\\beta}_2\\) or \\(\\hat{\\beta}_3\\)).\nCompute Standard Errors: Calculate the standard errors for these new coefficients.\nCalculate t-statistics:\n\\[\nt_{\\text{Education}} = \\frac{\\hat{\\beta}_2}{SE(\\hat{\\beta}_2)}\n\\]\n\\[\nt_{\\text{Age}} = \\frac{\\hat{\\beta}_3}{SE(\\hat{\\beta}_3)}\n\\]\nDetermine p-values: Find the p-values corresponding to \\(t_{\\text{Education}}\\) and \\(t_{\\text{Age}}\\).\n\nDecision\n\nSelect Variable with Lowest p-value: Choose the variable with the lowest p-value below the entry threshold (e.g., 0.05).\nAdd Significant Variable: Since Education is significant (assuming a p-value of 0.01), add it to the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-2",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Example",
    "text": "Stepwise Regression Example\n\n\nWith Income and Education in the model:\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\beta_2 \\cdot Education + \\epsilon\n\\]\nCalculating p-values for Removal\n\nRe-estimate Coefficients: Re-estimate \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\).\nCompute Standard Errors: Calculate the standard errors for these coefficients.\nCalculate t-statistics:\n\\[\nt_{\\text{Income}} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)}\n\\]\n\\[\nt_{\\text{Education}} = \\frac{\\hat{\\beta}_2}{SE(\\hat{\\beta}_2)}\n\\]\nDetermine p-values: Find the p-values corresponding to \\(t_{\\text{Income}}\\) and \\(t_{\\text{Education}}\\).\n\nDecision\n\nCheck for Non-significance: If any variable’s p-value exceeds the removal threshold (e.g., 0.10), consider removing it.\nRetain Significant Variables: If both variables remain significant (p-values below 0.05), keep them in the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-3",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Example",
    "text": "Stepwise Regression Example\n\n\nAssess Remaining Variables\nNow, only Age remains for consideration.\nCalculating p-value for Age\n\nAdd Age to the Model:\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\beta_2 \\cdot Education + \\beta_3 \\cdot Age + \\epsilon\n\\]\nEstimate \\(\\hat{\\beta}_3\\) and Compute SE: Estimate the coefficient for Age and its standard error.\nCalculate t-statistic and p-value:\n\\[\nt_{\\text{Age}} = \\frac{\\hat{\\beta}_3}{SE(\\hat{\\beta}_3)}\n\\]\nDetermine the p-value: Find the p-value from the t-distribution.\n\nDecision\n\nAdd or Exclude: If the p-value for Age is below 0.05, add it to the model; otherwise, exclude it.\nExample Outcome: Suppose the p-value for Age is 0.15; you would not add Age to the model."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-4",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#stepwise-regression-example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Stepwise Regression Example",
    "text": "Stepwise Regression Example\n\nAfter completing the procedure, our final model could look like this:\n\\[\nSpending = \\beta_0 + \\beta_1 \\cdot Income + \\beta_2 \\cdot Education + \\epsilon\n\\]\nHere, Income and Education are the only significant predictors of Spending."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#summary-of-stepwise-procedure",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#summary-of-stepwise-procedure",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary of Stepwise Procedure",
    "text": "Summary of Stepwise Procedure\n\nStart: Begin with one variable and calculate p-values.\nAdd: Add the variable with the lowest p-value, provided it’s below the threshold.\nRemove: Remove variables if their p-values become too high.\nStop: Stop when no more variables can be added or removed."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#forward-selection-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#forward-selection-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forward Selection",
    "text": "Forward Selection\nThe forward selection procedure starts with no independent variables. It adds variables one at a time using the same procedure as stepwise regression for determining whether an independent variable should be entered into the model.\n\nHowever, the forward selection procedure does not permit a variable to be removed from the model once it has been entered.\nThe procedure stops when the p-value for each of the independent variables not in the model is greater than p Value to Enter."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#forward-selection-limitations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#forward-selection-limitations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forward Selection Limitations",
    "text": "Forward Selection Limitations\n\nNo Removal of Variables: Once a variable is entered into the model, it cannot be removed, even if later additions make it less significant. This could result in keeping variables that are not optimal for the final model.\nIgnores Multicollinearity: Forward selection does not account for multicollinearity between independent variables. High correlations among predictors may lead to unstable coefficients and misleading results.\nGreedy Approach: The procedure focuses on adding one variable at a time based on immediate statistical improvement, which might lead to suboptimal global models. It might overlook combinations of variables that would better explain the dependent variable.\nModel Overfitting: Forward selection increases the risk of overfitting, especially when the dataset has many variables. This results in a model that fits the training data too closely but performs poorly on new data.\nAssumes Independence of Variables: The procedure assumes that variables can be assessed one by one for entry, without considering how combinations of variables interact. This might ignore important interactions between variables that could improve model performance."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#backward-elimination-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#backward-elimination-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Backward Elimination",
    "text": "Backward Elimination\nThe backward elimination procedure begins with a model that includes all the independent variables.\nIt then deletes one independent variable at a time using the same procedure as stepwise regression. However, the backward elimination procedure does not permit an independent variable to be reentered once it has been removed.\nThe procedure stops when none of the independent variables in the model has a p-value greater than p Value to Leave."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#backward-elimination-limitations",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#backward-elimination-limitations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Backward Elimination Limitations",
    "text": "Backward Elimination Limitations\n\nNo Reentry of Variables: Once a variable is removed from the model, it cannot be reentered, even if later stages show that it might be significant. This could lead to models that miss important predictors.\nIgnores Multicollinearity: Backward elimination does not take into account multicollinearity between variables. High correlations between predictors can result in unreliable coefficient estimates.\nOverfitting Risk: By starting with all variables and only eliminating those that don’t meet the p-value threshold, there is a higher risk of overfitting. The model may fit the training data too closely but generalize poorly to new data.\nDependent on Initial Model: The procedure is highly dependent on the initial model, which includes all variables. Poor choices in the initial set of variables can lead to suboptimal models.\nMay Lead to Different Models: Different elimination paths could lead to different final models, and not all of them may be optimal. Backward elimination might overlook variable interactions and combinations that forward selection would have considered."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#best-subsets-regression-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#best-subsets-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Best-Subsets Regression",
    "text": "Best-Subsets Regression\nMany statistical software packages have a procedure called best-subsets regression that enables the user to find, given a specified number of independent variables, the best regression equation.\nTypical output from such packages will enable the user to identify:\n\nThe two best one-variable estimated regression equations,\nThe two best two-variable regression equations,\nThe two best three-variable regression equations, and so on.\n\n\nThe criterion used in determining which estimated regression equations are best for any number of predictors is usually the value of the adjusted coefficient of determination."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-notes-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#adding-variables-in-regression-models-notes-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Adding Variables in Regression Models: Notes",
    "text": "Adding Variables in Regression Models: Notes\n\nLimitations of Procedures: None of the procedures that add or delete variables one at a time can be guaranteed to identify the best regression model.\n\nHowever, they are excellent approaches to finding good models—especially when multicollinearity is present.\n\nSoftware Implementation: The stepwise, forward selection, backward elimination, and best-subsets approaches to building a regression model can be implemented in Excel.\n\nHowever, this would be very inefficient as each approach would potentially require several steps in which various models based on what was learned in the previous step would have to be estimated.\nMost statistical software (including R) are capable of implementing each of these algorithms automatically."
  },
  {
    "objectID": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#summary-1",
    "href": "lecture_slides/16_chapter_model_building/16_chapter_model_building.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nGeneral Linear Model: Models parameters with exponents of 1. Example:\n\\[y = \\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + \\ldots + \\beta_p z_p + \\epsilon\\]\nCurvilinear Relationships: Use quadratic terms to model curvature:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\epsilon\\]\nInteraction Effects: Captures how two variables together influence \\(y\\).Here is a great paper about the topic.\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\\]\nLog Transformations: Useful for handling non-linearity and heteroscedasticity.\nStepwise Regression: Iterative process of adding/removing variables based on statistical significance, but risks overfitting.\nForward Selection: Adds variables one-by-one but doesn’t allow removal after inclusion.\nBackward Elimination: Starts with all variables and eliminates non-significant ones.\nBest-Subsets Regression: Evaluates all possible subsets of variables to find the best model."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#overview",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nTime Series and Time Series Patterns\nForecast Accuracy – Definitions\nNaïve Forecasting Method\nMoving Averages\nExponential Smoothing\n\n\n\nLag Variables\nTrend Projection\nSeasonality without Trend\nSeasonality and Trend"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series",
    "text": "Time Series\n\nTime series are especially important in business applications since they are involved in forecasting (revenues, expenses, stock prices, inventory levels, etc.).\nA time series is a sequence of measurements (of a variable) taken every hour, day, week, month, quarter, year, or any other regular time interval.\nThe pattern of the data is crucial for understanding how the time series has behaved over time.\nIf such behavior can be expected to continue in the future, we can use it to select an appropriate forecasting method."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods",
    "text": "Forecasting Methods"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-qualitative",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-qualitative",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Qualitative",
    "text": "Forecasting Methods: Qualitative\n\n\nForecasting methods can be classified as qualitative or quantitative.\nQualitative methods generally involve expert judgment to develop forecasts.\n\nSuch methods are appropriate when historical data are not applicable or unavailable.\n\nWe will focus on quantitative forecasting methods."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Quantitative",
    "text": "Forecasting Methods: Quantitative\n\nQuantitative forecasting methods can be used when:\n\nPast information about the variable being forecast is available,\nThe information can be quantified,\nIt is reasonable to assume that the pattern of the past will continue.\n\nIn such cases, a forecast can be developed using a time series or a causal method."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Quantitative",
    "text": "Forecasting Methods: Quantitative\n\nQuantitative methods are based on an analysis of historical data concerning one or more time series.\nA time series is a set of observations measured at successive points in time.\nTime Series Method: Restricts data to past values of the series.\nCausal Method: Uses other time series believed to be related to the time series to be forecasted."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Quantitative",
    "text": "Forecasting Methods: Quantitative\n\nTime Series Analysis\n\nThe objective is to discover a pattern in historical data and extrapolate it into the future.\nThe forecast is based solely on past values of the variable or past forecast errors."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Quantitative",
    "text": "Forecasting Methods: Quantitative\n\nCausal Methods\n\nAssumes the variable being forecasted has a cause-effect relationship with other variables.\nRegression analysis can be used to forecast the time series value as the dependent variable.\nIdentifying related independent variables can help in developing a regression equation for forecasting."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecasting-methods-quantitative-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecasting Methods: Quantitative",
    "text": "Forecasting Methods: Quantitative\n\n\nRegression Analysis: Time is treated as the independent variable, and the time series as the dependent variable.\nTime-Series Regression: The sole independent variable is time.\nCross-Sectional Regression: The independent variables are something other than time."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Plot",
    "text": "Time Series Plot\n\nThe first step in selecting an appropriate forecasting method is to construct a time series plot to identify patterns.\nA time series plot is a graphical presentation of the relationship between time and the time series variable.\nIt is a scatterplot where time is on the horizontal axis, and the time series values are on the vertical axis."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Plot: Example",
    "text": "Time Series Plot: Example\nGasoline sale time series\nThe number of gallons of gasoline sold by a gasoline distributor over a period of 12 weeks is given in the table below. The distributor would like to identify the underlying pattern in the data to guide it in selecting an appropriate forecasting method.\n\n\n\n\nWeek\nSales\n\n\n\n\n1\n17\n\n\n2\n21\n\n\n3\n19\n\n\n4\n23\n\n\n5\n18\n\n\n6\n16\n\n\n7\n20\n\n\n8\n18\n\n\n9\n22\n\n\n10\n20\n\n\n11\n15\n\n\n12\n22"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-example-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-plot-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Plot: Example",
    "text": "Time Series Plot: Example\nGasoline sales time series"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns",
    "text": "Time Series Patterns\nThe common types of data patterns that can be identified when examining a time series plot include:\n\nHorizontal\nTrend\nSeasonal\nTrend and Seasonal\nCyclical"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-horizontal-pattern",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-horizontal-pattern",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Horizontal Pattern",
    "text": "Time Series Patterns: Horizontal Pattern\n\n\nA horizontal pattern exists when the data fluctuate around a constant mean.\nChanges in business conditions can often result in a time series that shifts to a new level.\nIt is more difficult to choose an appropriate forecasting method to identify a change in the level of the time series. (The Change-time problem.)"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-horizontal-pattern-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-horizontal-pattern-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Horizontal Pattern",
    "text": "Time Series Patterns: Horizontal Pattern\nStationary Time Series\nThe term stationary time series is used to denote a time series whose statistical properties are independent of time. In particular, this means that:\n\nThe process generating the data has a constant mean.\nThe variability of the time series is constant over time.\n\n\nA time series plot for a stationary time series will always exhibit a horizontal pattern. But simply observing a horizontal pattern is not sufficient evidence to conclude that the time series is stationary.\nMore advanced texts on forecasting discuss procedures for determining if a time series is stationary and provide methods for transforming a time series that is not stationary into a stationary series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-trend-pattern",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-trend-pattern",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Trend Pattern",
    "text": "Time Series Patterns: Trend Pattern\n\n\nA time series may show gradual drifts or movements to relatively higher or lower values over a longer period of time.\nTrend is usually the result of long-term factors such as changes in the population, demographics, technology, or consumer preferences.\nA trend pattern can be identified by analyzing multiperiod movements in historical data."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-seasonal-pattern",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-seasonal-pattern",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Seasonal Pattern",
    "text": "Time Series Patterns: Seasonal Pattern\n\n\nSeasonal patterns are recognized by seeing the same repeating pattern of highs and lows over successive periods of time (within a “short” period of time).\nA seasonal pattern might occur within a day, week, month, quarter, or year.\nA seasonal pattern does not necessarily refer to the four seasons of the year."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-trend-and-seasonal-pattern",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-trend-and-seasonal-pattern",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Trend and Seasonal Pattern",
    "text": "Time Series Patterns: Trend and Seasonal Pattern\n\n\nSome time series include a combination of a trend and seasonal pattern.\nTime series decomposition can be used to separate or decompose a time series into trend and seasonal components."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-cyclical-pattern",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-patterns-cyclical-pattern",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Patterns: Cyclical Pattern",
    "text": "Time Series Patterns: Cyclical Pattern\n\n\nA cyclical pattern exists if the time series plot shows an alternating sequence of points below and above the trend line lasting more than one year.\nOften, the cyclical component of a time series is due to multiyear business cycles.\nBusiness cycles are difficult, if not impossible, to forecast."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#using-excels-chart-tools-to-construct-a-time-series-plot",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#using-excels-chart-tools-to-construct-a-time-series-plot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Using Excel’s Chart Tools to Construct a Time Series Plot",
    "text": "Using Excel’s Chart Tools to Construct a Time Series Plot\n\n\n\n\n\nA time series plot is simply a scatter diagram with lines connecting the points.\nSteps\n\nSelect cells A2:B13 in the Gasoline.xlsx file.\nClick the Insert tab on the Ribbon\nIn the Charts group, click the Insert Scatter (X, Y) or Bubble Chart button\nWhen the list of scatter diagram subtypes appears:\n\nClick the Scatter with Straight Lines and Markers button\n\n\nThe time series plot produced by Excel will appear in the same worksheet."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#selecting-a-forecasting-method-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#selecting-a-forecasting-method-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting a Forecasting Method",
    "text": "Selecting a Forecasting Method\n\n\nThe underlying pattern in the time series is an important factor in selecting a forecasting method.\nA time series plot should be one of the first tools developed to determine what forecasting method to use.\nIf we see a horizontal pattern, then we need to select a method appropriate for this type of pattern.\nIf we observe a trend in the data, then we need to use a method that can handle the trend effectively."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy",
    "text": "Forecast Accuracy\n\n\nMeasures of forecast accuracy are used to determine how well a particular forecasting method can reproduce the time series data that we already have.\nForecast accuracy measures are important factors in comparing different forecasting methods.\nBy selecting the method that has the best accuracy, we will obtain better forecasts for future time periods."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy",
    "text": "Forecast Accuracy\n\n\nThe key concept associated with measuring forecast accuracy is forecast error:\n\n\\[\n\\text{Forecast Error} = \\text{Actual Observed Value} - \\text{Forecast}\n\\]\n\\[\n\\text{Residual} = \\text{Observed} - \\text{Predicted}\n\\]\n\nA positive forecast error indicates the forecasting method underestimated the actual value.\nA negative forecast error indicates the forecasting method overestimated the actual value.\nA forecast error is basically a residual."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy",
    "text": "Forecast Accuracy\n\nMean Error (ME): ME is the mean or average of the forecast errors. Because positive and negative forecast errors tend to offset one another, the mean error is likely to be small. Thus, the mean error is not a very useful measure.\nMean Absolute Error (MAE): MAE avoids the problem of positive and negative errors offsetting one another. It is the mean of the absolute values of the forecast errors."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy",
    "text": "Forecast Accuracy\n\nMean Squared Error (MSE): MSE also avoids the problem of positive and negative errors offsetting one another. It is the average of the squared forecast errors.\nMean Absolute Percentage Error (MAPE): The size of MAE and MSE depend upon the scale of the data, so it is difficult to make comparisons for different time periods. To make such comparisons, we need to work with relative or percentage error measures. Percentage Error is the error divided by the observed value of the time series. The MAPE is the average of the absolute percentage errors of the forecasts."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Naïve Forecast",
    "text": "Forecast Accuracy: Example Naïve Forecast\n\n\n\nTo demonstrate the computation of these measures of forecast accuracy, let’s introduce the simplest of forecasting methods: the naïve forecasting method\nThe naïve forecasting method uses only the most recent observation in the time series as the forecast for the next time period.\n\n\\[\nF_{t+1} = Y_t, \\text{ the actual observed value in period } t\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Naïve Forecast",
    "text": "Forecast Accuracy: Example Naïve Forecast\n\nGasoline sale time series\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nObserved Time series\nNaïve Forecast\nForecast error\nAbsolute value of forecast error\nSquared forecast error\nPercentage error\nAbsolute value of percentage error\n\n\n\n\n1\n17\n\n\n\n\n\n\n\n\n2\n21\n17\n4\n4\n16\n19.05 = \\(\\frac{4}{21}\\)\n19.05\n\n\n3\n19\n21\n-2\n2\n4\n-10.53\n10.53\n\n\n4\n23\n19\n4\n4\n16\n17.39\n17.39\n\n\n5\n18\n23\n-5\n5\n25\n-27.78\n27.78\n\n\n6\n16\n18\n-2\n2\n4\n-12.5\n12.5\n\n\n7\n20\n16\n4\n4\n16\n20.00\n20.00\n\n\n8\n18\n20\n-2\n2\n4\n-11.11\n11.11\n\n\n9\n22\n18\n4\n4\n16\n18.18\n18.18\n\n\n10\n20\n22\n-2\n2\n4\n-10.00\n10.00\n\n\n11\n15\n20\n-5\n5\n25\n-33.33\n33.33\n\n\n12\n22\n15\n7\n7\n49\n31.82\n31.82\n\n\nTotals\n\n\n5\n41\n179\n1.19\n211.69\n\n\n\n\nWeek 2: the postive forecast error indicates that the forecasting method underestimated the actual value of sales.\nWeek 3: the negative forecast error indicates that the forecasting method overestimated the actual value of sales."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-naïve-forecast-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Naïve Forecast",
    "text": "Forecast Accuracy: Example Naïve Forecast\n\nGasoline sale time series: Naive Forecast Accuracy\n\nMean Absolute Error (MAE): It is the mean of the absolute values of the forecast errors.\n\n\n\\[\n\\text{MAE} = \\frac{41}{11} = 3.73\n\\]\n\nMean Squared Error (MSE): It is the average of the squared forecast errors.\n\n\n\n\\[\n\\text{MSE} = \\frac{179}{11} = 16.27\n\\]\n\n\n\nThe size of MAE and MSE depends upon the scale of the data. So, it is not recommended to make comparisons for different time intervals (e.g. months vs weeks).\nTo make comparisons like these we need relative or percentage error measues like MAPE.\nMean Absolute Percentage Error (MAPE): It is the average of the absolute percentage errors of the forecasts.\n\n\n\n\\[\n\\text{MAPE} = \\frac{211.69}{11} = 19.24\\%\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Historical Data Average",
    "text": "Forecast Accuracy: Example Historical Data Average\nLet’s use the average of all the historical data available as the forecast for the next period.\n\nWe begin by developing a forecast for Week 2. Since there is only one historical value available prior to Week 2, the forecast for Week 2 is just the time series value in Week 1. Thus, the forecast for Week 2 is 17 thousand gallons of gasoline.\n\n\nTo compute the forecast for Week 3, we take the average of the sales values in Weeks 1 and 2:\n\\[\n\\text{Forecast for week 3} = \\frac{17 + 21}{2} = 19\n\\]\n\n\nSimilarly, the forecast for Week 4 is:\n\\[\n\\text{Forecast for week 4} = \\frac{17 + 21 + 19}{3} = 19\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Historical Data Average",
    "text": "Forecast Accuracy: Example Historical Data Average\n\nThe forecasts obtained using this method for the gasoline time series are:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTime Series Value\nForecast\nForecast Error\nAbsolute Value of Forecast Error\nSquared Forecast Error\nPercentage Error\nAbsolute Value of Percentage Error\n\n\n\n\n1\n17\n\n\n\n\n\n\n\n\n2\n21\n17.00\n4.00\n4.00\n16.00\n19.05\n19.05\n\n\n3\n19\n19.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n4\n23\n19.00\n4.00\n4.00\n16.00\n17.39\n17.39\n\n\n5\n18\n20.00\n-2.00\n2.00\n4.00\n-11.11\n11.11\n\n\n6\n16\n19.60\n-3.60\n3.60\n12.96\n-22.50\n22.50\n\n\n7\n20\n19.00\n1.00\n1.00\n1.00\n5.00\n5.00\n\n\n8\n18\n19.14\n-1.14\n1.14\n1.31\n-6.35\n6.35\n\n\n9\n22\n19.00\n3.00\n3.00\n9.00\n13.64\n13.64\n\n\n10\n20\n19.33\n0.67\n0.67\n0.44\n3.33\n3.33\n\n\n11\n15\n19.40\n-4.40\n4.40\n19.36\n-29.33\n29.33\n\n\n12\n22\n19.00\n3.00\n3.00\n9.00\n13.64\n13.64\n\n\n\n\nTotals\n4.53\n26.81\n89.07\n2.76\n141.34"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-example-historical-data-average-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Example Historical Data Average",
    "text": "Forecast Accuracy: Example Historical Data Average\nUsing the results shown, we obtained the following values of MAE, MSE, and MAPE:\n\\[\n\\text{MAE} = \\frac{26.81}{11} = 2.44\n\\] \\[\n\\text{MSE} = \\frac{89.07}{11} = 8.10\n\\] \\[\n\\text{MAPE} = \\frac{141.34}{11} = 12.85\\%\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-comparison-of-forecasting-methods",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-accuracy-comparison-of-forecasting-methods",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast Accuracy: Comparison of Forecasting Methods",
    "text": "Forecast Accuracy: Comparison of Forecasting Methods\nWe can now compare the accuracy of the two forecasting methods we have considered by comparing the values of MAE, MSE, and MAPE for each method.\n\n\n\n\nNaive Method\nAverage of Past Values\n\n\n\n\nMAE\n3.73\n2.44\n\n\nMSE\n16.27\n8.10\n\n\nMAPE\n19.24%\n12.85%\n\n\n\nFor every measure, the average of past values provides more accurate forecasts than using the most recent observation as the forecast for the next period.\n\nIf the underlying time series is stationary, the average of all the historical data will generally provide the best results.\nIf the time series is not stationary, adjustments are needed.\n\n\nIn cases where the time series shifts to a new level (e.g., due to contract changes), the naive method might adapt faster than the averaging method."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#importance-of-forecast-accuracy-and-adaptability",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#importance-of-forecast-accuracy-and-adaptability",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Importance of Forecast Accuracy and Adaptability",
    "text": "Importance of Forecast Accuracy and Adaptability\nMeasures of forecast accuracy are crucial but should not be the sole basis for choosing a forecasting method.\n\nConsider the business context and the likelihood of changes in the time series level.\nHistorical forecast accuracy should be weighed alongside the ability of the method to adapt to shifts."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-and-exponential-smoothing-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-and-exponential-smoothing-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages and Exponential Smoothing",
    "text": "Moving Averages and Exponential Smoothing\n\nThree forecasting methods that are appropriate for a time series with a horizontal pattern:\n\n\nMoving Averages (MAs)\nWeighted Moving Averages\nExponentially Weighted Moving Averages (EWMAs)\n\n\nThey are called smoothing methods because their objective is to smooth out the random fluctuations (due to random errors or noises) in the time series.\nThey are most appropriate for short-range forecasts."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages",
    "text": "Moving Averages\nThe moving averages method uses the average of the most recent k data values in the time series as the forecast for the next period. Mathematically, a moving average forecast of order k is as follows:\n\n\n\n\n\n\n\nMoving Average Forecast of Order \\(k\\)\n\n\n\n\\[\nF_{t+1} = \\frac{\\sum (\\text{most recent } k \\text{ data values})}{k} = \\frac{Y_t + Y_{t-1} + \\cdots + Y_{t-k+1}}{k}\n\\]\nwhere:\n\n\\(F_{t+1}\\) = forecast of the time series for period \\(t + 1\\)\n\\(Y_t\\) = actual value of the time series in period \\(t\\)\n\n\n\n\n\n\n\nEach observation in the moving average calculation receives the same weight \\((1/k)\\).\nThe term moving is used because every time a new observation becomes available for the time series, it replaces the oldest observation in the equation.\nAs a result, the average will change, or move, as new observations become available."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages",
    "text": "Moving Averages\n\n\nTo use moving averages to forecast, we must first select the span (the order), which is the number of observed time series values to be included in the moving average.\n\nA smaller value of k will track shifts in a time series more quickly than a larger value of k.\nA larger value of k, the smoother the MA. Less sensitive to changes in the given time series.\nIf more past observations are considered relevant, then a larger value of k is better."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages",
    "text": "Moving Averages\n\nExample: Gasoline sale time series: three-week moving average (\\(k = 3\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nTime series value\nForecast\nForecast error\nAbsolute value of forecast error\nSquared forecast error\nPercentage error\nAbsolute value of percentage error\n\n\n\n\n1\n17\n\n\n\n\n\n\n\n\n2\n21\n\n\n\n\n\n\n\n\n3\n19\n\n\n\n\n\n\n\n\n4\n23\n\\(\\frac{(17 + 21 + 19)}{3} = 19\\)\n4\n4\n16\n17.39\n17.39\n\n\n5\n18\n\\(\\frac{21 + 19 + 23}{3} = 21\\)\n-3\n3\n9\n-16.67\n16.67\n\n\n6\n16\n20\n-4\n4\n16\n-25.00\n25.00\n\n\n7\n20\n19\n1\n1\n1\n5.00\n5.00\n\n\n8\n18\n18\n0\n0\n0\n0.00\n0.00\n\n\n9\n22\n18\n4\n4\n16\n18.18\n18.18\n\n\n10\n20\n20\n0\n0\n0\n0.00\n0.00\n\n\n11\n15\n20\n-5\n5\n25\n-33.33\n33.33\n\n\n12\n22\n19\n3\n3\n9\n13.64\n13.64\n\n\n\n\nTotals\n\n\n0\n24\n92"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages",
    "text": "Moving Averages\nExample: Gasoline sale time series: three-week moving average\n\\[\n\\text{MAE} = \\frac{24}{9} = 2.67\n\\]\n\\[\n\\text{MSE} = \\frac{92}{9} = 10.22\n\\]\n\\[\n\\text{MAPE} = \\frac{129.21}{9} = 14.36\\%\n\\]\n\nThe three-week moving average approach provided more accurate forecasts than the naïve approach (19.24%)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-plot",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-plot",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages Plot",
    "text": "Moving Averages Plot\n\nExample: Gasoline sales time series: three-week moving average\n\n\n\nThe Figure shows the original time series plot and the three-week moving average forecasts.\nNote how the graph of the moving average forecasts has tended to smooth out the random fluctuations in the time series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-excel",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#moving-averages-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Moving Averages: Excel",
    "text": "Moving Averages: Excel\n\nEnter/Access Data: Open the file Gasoline. The data are in cells A2:B13 and labels are in cells A1:B1.\n\nStep 1: Click the Data tab on the Ribbon\nStep 2: In the Analyze group, click Data Analysis\nStep 3: Choose Moving Average from the list of Analysis Tools and click OK\nStep 4: When the Moving Average dialog box appears:\n\nEnter B2:B13 in the Input Range box\nEnter 3 in the Interval box\nEnter C3 in the Output Range box\nSelect the check box for Chart Output\nClick OK\n\n\nThe three-week moving average forecasts appear in column C of the worksheet; a chart showing the actual values of the time series and the forecasted values also appears. Forecasts for periods of other lengths can be computed easily by entering a different value in the interval box."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#weighted-moving-averages-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#weighted-moving-averages-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Weighted Moving Averages",
    "text": "Weighted Moving Averages\n\nIn the moving averages method, each observation in the moving average calculation receives the same weight.\nWeighted Moving Averages: we can select a different weight for each data value (e.g. the most recent observations) and then compute a weighted average of the most recent \\(k\\) values as the forecast.\n\nThe more recent observations are typically given more weight than older observations.\nFor convenience, the weights should sum to 1."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#weighted-moving-averages-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#weighted-moving-averages-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Weighted Moving Averages",
    "text": "Weighted Moving Averages\nFor the Gasoline data, a three-period weighted moving average (3WMA) for Week 4 is:\n\\[\nWMA_3 = \\frac{1}{6}(17) + \\frac{2}{6}(21) + \\frac{3}{6}(19) = 19.33\n\\]\n\nif the weights are \\(\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}\\), which sum to 1.\n19 is the most recent of the three observations and receives the largest weight.\nTo determine whether one particular combination of number of data values and weights provides a more accurate forecast than another combination, it is recommended to use MSE as the measure of the forecast accuracy.\n\nIf we assume that the combination that is best for the past will also be best for the future, we would use the one that minimizes MSE for the historical time series to forecast the next value in the time series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast",
    "text": "Exponential Smoothing Forecast\n\n\nThe exponential smoothing forecast is a special case of the weighted moving averages.\nIt is the weighted average of all the time series data up to the current period.\nWe select only the weight between 0 and 1 for the most recent observation. This weight is called the smoothing constant and is denoted by \\(\\alpha\\).\n\n\\(1 - \\alpha\\) is called the damping factor.\n\n\n\n\n\n\n\n\n\n\n\nSingle Exponential Smoothing Forecast\n\n\n\\[\nF_{t+1} = \\alpha Y_t + (1 - \\alpha) F_t\n\\]\nwhere\n\n\n\\(F_{t+1}\\) = forecast of the time series for period \\(t + 1\\)\n\\(Y_t\\) = actual value of the time series in period \\(t\\)\n\\(F_t\\) = forecast of the time series for period \\(t\\)\n\\(\\alpha\\) = smoothing constant \\((0 \\leq \\alpha \\leq 1)\\)\n\n\n\n\n\n\n\n\n\nThere are a number of exponential smoothing procedures.\nThe term exponential smoothing comes from the exponential nature of the weighting scheme for the historical values.\n\nThe weights assigned to the time series values decrease exponentially as the “age” of the data values increases.\n\nThe statistic computed in each period is called the Exponentially Weighted Moving Average (EWMA)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast",
    "text": "Exponential Smoothing Forecast\nLet’s illustrate by working with a time series involving only three periods of data: \\(Y_1\\), \\(Y_2\\), and \\(Y_3\\).\n\nTo initiate, let \\(F_1\\) equal the actual value of the time series in period 1; that is, \\(F_1 = Y_1\\). Hence, the forecast for period 2 is\n\\[\n\\begin{aligned}\nF_2 &= \\alpha Y_1 + (1 - \\alpha) F_1 \\\\\n    &= \\alpha Y_1 + (1 - \\alpha) Y_1 \\\\\n    &= Y_1\n\\end{aligned}\n\\]\nThus, the exponential smoothing forecast for period 2 is equal to the actual value of the time series in period 1."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast",
    "text": "Exponential Smoothing Forecast\nThe forecast for period 3 is\n\\[\nF_3 = \\alpha Y_2 + (1 - \\alpha) F_2 = \\alpha Y_2 + (1 - \\alpha) Y_1\n\\]\nFinally, substituting this expression for \\(F_3\\) in the expression for \\(F_4\\), we obtain\n\\[\n\\begin{aligned}\nF_4 &= \\alpha Y_3 + (1 - \\alpha) F_3 \\\\\n&= \\alpha Y_3 + (1 - \\alpha)[\\alpha Y_2 + (1 - \\alpha) Y_1] \\\\\n&= \\alpha Y_3 + \\alpha (1 - \\alpha) Y_2 + (1 - \\alpha)^2 Y_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast",
    "text": "Exponential Smoothing Forecast\n\nThe term exponential smoothing comes from the exponential nature of the weighting scheme for the historical values.\nWe now see that \\(F_4\\) is a weighted average of the first three time series values. The sum of the coefficients, or weights, for \\(Y_1\\), \\(Y_2\\), and \\(Y_3\\) equals 1. A similar argument can be made to show that, in general, any forecast \\(F_{t+1}\\) is a weighted average of all the previous time series values.\nDespite the fact that exponential smoothing provides a forecast that is a weighted average of all past observations, all past data do not need to be saved to compute the forecast for the next period. In fact, only two pieces of information are needed: \\(Y_t\\) and \\(F_t\\)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Example",
    "text": "Exponential Smoothing Forecast: Example\nGasoline sale time series\nThe number of gallons of gasoline sold by a distributor over a period of 12 weeks is given below. The distributor uses exponential smoothing to forecast sales. Which value for the smoothing constant \\(\\alpha\\), 0.2 or 0.3, gives better forecasts?\n\n\n\n\nWeek\nSales\n\n\n\n\n1\n17\n\n\n2\n21\n\n\n3\n19\n\n\n4\n23\n\n\n5\n18\n\n\n6\n16\n\n\n7\n20\n\n\n8\n18\n\n\n9\n22\n\n\n10\n20\n\n\n11\n15\n\n\n12\n22"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Example",
    "text": "Exponential Smoothing Forecast: Example\n\nGasoline sale time series with smoothing constant \\(\\alpha = 0.2\\)\n\n\n\n\n\n\n\n\n\n\nWeek\nTime series value\nForecast\nForecast error\nSquared forecast error\n\n\n\n\n1\n17\n\n\n\n\n\n2\n21\n17\n4.00\n16.00\n\n\n3\n19\n\\(0.2(21) + 0.8(17)=17.80\\)\n1.20\n1.44\n\n\n4\n23\n\\(0.2(19) + 0.8(17.80)=18.04\\)\n4.96\n24.60\n\n\n5\n18\n19.03\n-1.03\n1.06\n\n\n6\n16\n18.83\n-2.83\n8.01\n\n\n7\n20\n18.26\n1.74\n3.03\n\n\n8\n18\n18.61\n-0.61\n0.37\n\n\n9\n22\n18.49\n3.51\n12.32\n\n\n10\n20\n19.19\n0.81\n0.66\n\n\n11\n15\n19.35\n-4.35\n18.92\n\n\n12\n22\n18.48\n3.52\n12.39\n\n\n\n\nTotals\n10.92\n98.80"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Example",
    "text": "Exponential Smoothing Forecast: Example\nGasoline sale time series with smoothing constant \\(\\alpha = 0.2\\)\n\\[\n\\text{MSE} = \\frac{98.80}{11} = 8.98\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Example",
    "text": "Exponential Smoothing Forecast: Example\n\nGasoline sale time series with smoothing constant \\(\\alpha = 0.3\\)\n\n\n\n\n\n\n\n\n\n\nWeek\nTime series value\nForecast\nForecast error\nSquared forecast error\n\n\n\n\n1\n17\n\n\n\n\n\n2\n21\n17\n4\n16\n\n\n3\n19\n18.20\n0.80\n0.64\n\n\n4\n23\n18.44\n4.56\n20.79\n\n\n5\n18\n19.81\n-1.81\n3.28\n\n\n6\n16\n19.27\n-3.27\n10.69\n\n\n7\n20\n18.29\n1.71\n2.92\n\n\n8\n18\n18.80\n-0.80\n0.64\n\n\n9\n22\n18.56\n3.44\n11.83\n\n\n10\n20\n19.59\n0.41\n0.17\n\n\n11\n15\n19.71\n-4.71\n22.18\n\n\n12\n22\n18.30\n3.70\n13.69\n\n\n\n\nTotals\n8.03\n102.83"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Example",
    "text": "Exponential Smoothing Forecast: Example\nGasoline sale time series with smoothing constant \\(\\alpha = 0.3\\)\n\\[\n\\text{MSE} = \\frac{102.83}{11} = 9.35\n\\]\n\nExponential smoothing (with \\(\\alpha = 0.2\\)) provided more accurate forecasts (8.98) than exponential smoothing with \\(\\alpha = 0.3\\) in this example."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-excel",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Excel",
    "text": "Exponential Smoothing Forecast: Excel\n\n\nStep 1: Click Data tab on the Ribbon\nIn the Analyze group, click Data Analysis\nChoose Exponential smoothing from the list of Analysis Tools and click OK\nWhen the Exponential smoothing dialog box appears:\n\nEnter B2:B13 in the Input Range box\nEnter 0.8 in the Damping factor box\nEnter C2 in the Output Range box\nSelect Chart Output\n\nClick OK"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-excel-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#exponential-smoothing-forecast-excel-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Exponential Smoothing Forecast: Excel",
    "text": "Exponential Smoothing Forecast: Excel\n\nGasoline sale time series with smoothing constant \\(\\alpha = 0.2\\)"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables",
    "text": "Forecast with Lag Variables\n\nLag variables are often used in time series analysis to capture the correlation (effect) of past time series values with (on) the present or future time series values.\n\nFor example, this quarter’s sales depend on (are correlated with) sales of the last three quarters.\nAt time any \\(t\\), variable \\(v_t\\) may depend on \\(y_{t-1}, y_{t-2}, y_{t-3}, \\dots\\), which are denoted by Lag 1, Lag 2, Lag 3, respectively.\nThese lag variables are considered predictors of \\(v_t\\).\n\nThis type of correlation is called autocorrelation.\nThe choice of lag order (how many periods back to look) and the selection of an appropriate forecasting model are critical for forecasting lag variables effectively."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-global-apple-iphone-quarterly-sales",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-global-apple-iphone-quarterly-sales",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables: Global Apple iPhone Quarterly Sales",
    "text": "Forecast with Lag Variables: Global Apple iPhone Quarterly Sales\n\n\n\n\nQuarter\n\\(t\\)\nSales ($Mil.)\n\n\n\n\nQ210\n1\n8.75\n\n\nQ310\n2\n8.40\n\n\nQ410\n3\n14.10\n\n\nQ111\n4\n16.24\n\n\nQ211\n5\n18.65\n\n\nQ311\n6\n20.34\n\n\nQ411\n7\n17.07\n\n\nQ112\n8\n37.04\n\n\nQ212\n9\n35.06\n\n\nQ312\n10\n26.03\n\n\nQ412\n11\n26.91\n\n\nQ113\n12\n47.79\n\n\n…\n…\n…\n\n\n\n\n\\(n = 32\\)"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables",
    "text": "Forecast with Lag Variables\nAssume lag order is 4.\n\n\n\nQuarter\n\\(t\\)\nSales ($Mil.)\nLag 1\nLag 2\nLag 3\nLag 4\n\n\n\n\nQ210\n1\n8.75\n\n\n\n\n\n\nQ310\n2\n8.40\n8.75\n\n\n\n\n\nQ410\n3\n14.10\n8.40\n8.75\n\n\n\n\nQ111\n4\n16.24\n14.10\n8.40\n8.75\n\n\n\nQ211\n5\n18.65\n16.24\n14.10\n8.40\n8.75\n\n\nQ311\n6\n20.34\n18.65\n16.24\n14.10\n8.40\n\n\n…\n…\n…\n…\n…\n…\n…"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-correlations",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-correlations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables: Correlations",
    "text": "Forecast with Lag Variables: Correlations\n\n\n\n\nSales ($Mil.)\nLag 1\nLag 2\nLag 3\n\n\n\n\nLag 1\n0.642\n\n\n\n\n\nLag 2\n0.418\n0.660\n\n\n\n\nLag 3\n0.603\n0.440\n0.660\n\n\n\nLag 4\n0.914\n0.605\n0.437\n0.664"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-full-model",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-full-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables: Full Model",
    "text": "Forecast with Lag Variables: Full Model\n\nSummary Output:\n\n\n\nRegression Statistics\n\n\n\n\n\nMultiple R\n0.9226\n\n\nR Square\n0.8511\n\n\nAdjusted R Square\n0.8252\n\n\nStandard Error\n7.0864\n\n\nObservations\n28\n\n\n\nANOVA\n\n\n\ndf\nSS\nMS\nF\nSignificance F\n\n\n\n\nRegression\n4\n6602.696\n1650.674\n32.871\n\n\nResidual\n23\n1154.980\n50.217\n\n\n\nTotal\n27\n7757.676\n\n\n\n\n\nCoefficients:\n\n\n\nVariables\nCoefficients\nStandard Error\nt Stat\nP-value\n\n\n\n\nIntercept\n16.438\n4.179\n3.933\n0.001\n\n\nLag 1\n0.011\n0.122\n0.091\n0.928\n\n\nLag 2\n-0.141\n0.119\n-1.188\n0.247\n\n\nLag 3\n0.007\n0.121\n0.057\n0.955\n\n\nLag 4\n0.858\n0.115\n7.467\n0.000"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-reduced-model",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-reduced-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables: Reduced Model",
    "text": "Forecast with Lag Variables: Reduced Model\n\nSummary Output:\n\n\n\nRegression Statistics\n\n\n\n\n\nMultiple R\n0.9144\n\n\nR Square\n0.8361\n\n\nAdjusted R Square\n0.8298\n\n\nStandard Error\n6.9933\n\n\nObservations\n28\n\n\n\nANOVA\n\n\n\ndf\nSS\nMS\nF\nSignificance F\n\n\n\n\nRegression\n1\n6486.115\n6486.115\n132.624\n\n\nResidual\n26\n1271.561\n48.906\n\n\n\nTotal\n27\n7757.676\n\n\n\n\n\nCoefficients:\n\n\n\nVariables\nCoefficients\nStandard Error\nt Stat\nP-value\n\n\n\n\nIntercept\n13.161\n3.002\n4.384\n0.000\n\n\nLag 4\n0.813\n0.071\n11.516\n0.000\n\n\n\n\\[\n\\widehat{Sales_t} = 13.161 + (0.813) \\widehat{Sales_{t-4}}\n\\]\nwhere the predictor is the Lag 4 variable."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-excel",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#forecast-with-lag-variables-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Forecast with Lag Variables: Excel",
    "text": "Forecast with Lag Variables: Excel\n\n\nImportant!Do not include empty cells or fill empty cells with zeros in Step 2.\n\n\n\n\n\nStep 1: Set Up Your Data in Excel\n\n\nOrganize Your Data\n\nPlace your main variable (e.g., \\(Y\\)) in one column (e.g., A) with the header “Y”.\nCreate two new columns for the lagged versions of \\(Y\\):\n\n“Y_lag1” for the first lag, containing values of \\(Y\\) shifted down by one row.\n“Y_lag2” for the second lag, containing values of \\(Y\\) shifted down by two rows.\n\n\nCreate the Lagged Variables\n\nIn B3 (under “Y_lag1”), enter the formula =A2 to reference the previous row’s value of \\(Y\\).\nIn C4 (under “Y_lag2”), enter the formula =A2 to reference the value of \\(Y\\) two rows above.\n\nCopy these formulas down the columns to match the length of the \\(Y\\) column.\nLeave the first two rows empty in the lagged variable columns, as there are no lagged values for the initial observations.\n\n\nPrepare Data for Regression\n\nRemoving the first two rows from your analysis since they lack lagged values in an option, but it is not recommended. Start analysis from row 4 onward.\n\n\n\n\n\n\nStep 2: Run the Regression\n\n\n\nOpen the Data Analysis Tool\n\nGo to the Data tab &gt; Data Analysis.\nIf Data Analysis isn’t visible, enable it via File &gt; Options &gt; Add-Ins &gt; Excel Add-ins &gt; Analysis ToolPak.\n\nSelect Regression: Choose Regression in the Data Analysis dialog and click OK.\nSpecify the Input Range\n\nInput Y Range: Select the range of \\(Y\\) values, excluding the first two rows (e.g., A4:A20). Do not fill empty cells with zeros.\nInput X Range: Select both lagged variable ranges (e.g., B4:C20). Do not fill empty cells with zeros.\n\nSet Up Additional Options\n\nCheck Labels if headers were included in your input ranges.\nChoose the output location for the regression results.\n\nRun the Regression: Click OK to run. Excel will generate the regression output."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#trend-projection-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#trend-projection-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Trend Projection",
    "text": "Trend Projection\n\nIf a time series plot exhibits a linear trend, the method of least squares regression may be used to determine a trend line (projection) for future forecasts.\nLeast squares regression determines the unique trend line forecast, which minimizes the sum of squared (forecast) errors, SSE, between the trend line forecasts (predicted values) and the actual (observed) values for the time series.\nThe independent variable is the time period and the dependent variable is the actual observed value in the time series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression",
    "text": "Linear Trend Regression\n\n\n\n\nBicycle Yearly Sales Time Series\n\n\n\n\nTime (Year)\nSales (1000s)\n\n\n\n\n1\n21.6\n\n\n2\n22.9\n\n\n3\n25.5\n\n\n4\n21.9\n\n\n5\n23.9\n\n\n6\n27.5\n\n\n7\n31.5\n\n\n8\n29.7\n\n\n9\n28.6\n\n\n10\n31.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough it shows some up and down movement over the past 10 years, we can identify a linear trend line.\nThe trend line provides a reasonable approximation o the. long-run movement in the series.\nWe can use the simple linear regression method to develop such a linear trend line."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression",
    "text": "Linear Trend Regression\n\n\n\n\n\n\n\n\nLinear Trend Equation\n\n\n\n\\[\n\\hat{Y_t} = b_0 + b_1 t, \\quad t = 1, 2, \\ldots\n\\]\nwhere:\n\n\\(\\hat{Y_t}\\) = linear trend forecast of mean response for period t\n\\(b_0\\) = estimated intercept of the linear trend line\n\\(b_1\\) = estimated slope of the linear trend line\n\\(t\\) = time period"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression",
    "text": "Linear Trend Regression\n\n\n\n\n\n\n\nComputing the Slope and Intercept for a Linear Trend\n\n\n\n\\[\nb_1 = \\frac{\\sum_{t=1}^{n} (t - \\bar{t})(Y_t - \\bar{Y})}{\\sum_{t=1}^{n} (t - \\bar{t})^2}, \\quad \\text{or} \\quad b_1 = \\frac{\\sum_{t=1}^{n} t Y_t - \\left( \\frac{\\sum_{t=1}^{n} t \\sum_{t=1}^{n} Y_t}{n} \\right)}{\\sum_{t=1}^{n} t^2 - \\left( \\frac{\\left(\\sum_{t=1}^{n} t\\right)^2}{n} \\right)}\n\\]\n\\[\nb_0 = \\bar{Y} - b_1 \\bar{t}\n\\]\nWhere:\n\n\\(Y_t\\) = value of the time series in period \\(t\\)\n\\(n\\) = number of time periods (number of observations)\n\\(\\bar{Y}\\) = average value of the time series\n\\(\\bar{t}\\) = average value of \\(t\\)"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression: Example",
    "text": "Linear Trend Regression: Example\n\nTo compute the linear trend equation for the bicycle sales time series, we begin the calculations by computing \\(\\bar{t}\\) and \\(\\bar{Y}\\).\n\\[\n\\bar{t} = \\frac{\\sum_{t=1}^{n} t}{n} = \\frac{55}{10} = 5.5\n\\]\n\\[\n\\bar{Y} = \\frac{\\sum_{t=1}^{n} Y_t}{n} = \\frac{264.5}{10} = 26.45\n\\]\n\nUsing these values, and the data, we can compute: the slope and intercept.\n\\[\nb_1 = \\frac{\\sum_{t=1}^{n} (t - \\bar{t})(Y_t - \\bar{Y})}{\\sum_{t=1}^{n} (t - \\bar{t})^2} = \\frac{90.75}{82.5} = 1.1\n\\]\n\\[\nb_0 = \\bar{Y} - b_1 \\bar{t} = 26.45 - 1.1(5.5) = 20.4\n\\]\n\n\nTherefore, the linear trend equation is:\n\\[\n\\hat{Y}_t = 20.4 + 1.1t\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression: Example",
    "text": "Linear Trend Regression: Example\n\nBicycle Sales Time Series\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)\n\\(Y_1\\)\n\\(t - \\bar{t}\\)\n\\(Y_1 - \\bar{Y}\\)\n\\((t - \\bar{t})(Y_1 - \\bar{Y})\\)\n\\((t - \\bar{t})^2\\)\n\n\n\n\n1\n21.6\n-4.5\n-4.85\n21.825\n20.25\n\n\n2\n22.9\n-3.5\n-3.55\n12.425\n12.25\n\n\n3\n25.5\n-2.5\n-0.95\n2.375\n6.25\n\n\n4\n21.9\n-1.5\n-4.55\n6.825\n2.25\n\n\n5\n23.9\n-0.5\n-2.55\n1.275\n0.25\n\n\n6\n27.5\n0.5\n1.05\n0.525\n0.25\n\n\n7\n31.5\n1.5\n5.05\n7.575\n2.25\n\n\n8\n29.7\n2.5\n3.25\n8.125\n6.25\n\n\n9\n28.6\n3.5\n2.15\n7.525\n12.25\n\n\n10\n31.4\n4.5\n4.95\n22.275\n20.25\n\n\nTotals\n264.5\n\n\n90.750\n82.5"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression: Example",
    "text": "Linear Trend Regression: Example\nLinear Trend Equation for the Bicycle Sales Time Series:\n\\[\n\\hat{Y}_t = 20.4 + 1.1t\n\\]\nThe slope 1.1 indicates that over the past 10 years the firm experienced an average growth in sales of about 1100 units per year.\n\nIf we assume that the past 10-year trend in sales is a good indicator of the future, this trend equation can be used to develop forecasts for future time periods. Substituting \\(t = 11\\) into the equation yields next year’s trend projection or forecast, \\(\\hat{Y}_{11}\\).\n\\[\n\\hat{Y}_t = 20.4 + 1.1 \\times 11 = 32.5\n\\]\nThus, using trend projection, we would forecast sales of 32,500 bicycles next year."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#linear-trend-regression-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Linear Trend Regression: Example",
    "text": "Linear Trend Regression: Example\n\nBicycle Sales Time Series\n\n\n\nYear\nSales\nForecast\nForecast error\nSquared Forecast Error\n\n\n\n\n1\n21.6\n21.5\n0.1\n0.01\n\n\n2\n22.9\n22.6\n0.3\n0.09\n\n\n3\n25.5\n23.7\n1.8\n3.24\n\n\n4\n21.9\n24.8\n-2.9\n8.41\n\n\n5\n23.9\n25.9\n-2.0\n4.00\n\n\n6\n27.5\n27.8\n0.5\n0.25\n\n\n7\n31.5\n28.1\n3.4\n11.56\n\n\n8\n29.7\n29.2\n0.5\n0.25\n\n\n9\n28.6\n30.3\n-1.7\n2.89\n\n\n10\n31.4\n31.4\n0.0\n0.00\n\n\nTotal\n264.5\n\n\n30.7\n\n\n\n\\[\n\\text{MSE} = \\frac{\\sum_{t=1}^{n} (Y_t - F_t)^2}{n} = \\frac{30.7}{10} = 3.07\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#regression-output-example-excel",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#regression-output-example-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Regression Output: Example Excel",
    "text": "Regression Output: Example Excel\n\n\n\n\n\nEnter/Access Data: Open the file Bicycle. The data are in cells A2:B11 and labels are in cells A1:B1.\n\n\nStep 1: Click the Data tab on the Ribbon\nStep 2: In the Analyze group, click Data Analysis\nStep 3: Choose Regression from the list of Analysis Tools\nStep 4: When the Regression dialog box appears:\n\nEnter B1:B11 in the Input Y Range box\nEnter A1:A11 in the Input X Range box\nSelect the check box for Labels\nSelect the check box for Confidence Level\nOutput Range:\n\nEnter A13 in the Output Range box (to identify the upper left corner of the section of the worksheet where the output will appear)\n\nClick OK"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Nonlinear Trend Regression",
    "text": "Nonlinear Trend Regression\n\n\nSometimes time series have a curvilinear or nonlinear trend.\nA variety of nonlinear functions can be used to develop an estimate of the trend in a time series.\nOne example is this quadratic trend equation:\n\n\\[\n\\hat{Y_t} = b_0 + b_1t + b_2t^2\n\\]\n\nAnother example is this exponential trend equation:\n\n\\[\n\\hat{Y_t} = b_0(b_1)^t\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Nonlinear Trend Regression: Example",
    "text": "Nonlinear Trend Regression: Example\n\nCholesterol Revenue Time Series\nThe data regarding cholesterol sales over a period of 10 years is tabulated below. A curvilinear function appears to be needed to model the long-term trend.\n\n\n\n\n\n\n\nYear\nRevenue ($ millions)\n\n\n\n\n1\n23.1\n\n\n2\n21.3\n\n\n3\n27.4\n\n\n4\n34.6\n\n\n5\n33.8\n\n\n6\n43.2\n\n\n7\n59.5\n\n\n8\n64.4\n\n\n9\n74.2\n\n\n10\n99.3"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example-excel",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example-excel",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Nonlinear Trend Regression: Example Excel",
    "text": "Nonlinear Trend Regression: Example Excel\n\n\n\n\n\nEnter/Access Data: Open the file Cholesterol.\n\n\nStep 1: Click the Data tab on the Ribbon\nStep 2: In the Analyze group, click Data Analysis\nStep 3: Choose Regression from the list of Analysis Tools\nStep 4: When the Regression dialog box appears:\n\nEnter C1:C11 in the Input Y Range box\nEnter A1:B11 in the Input X Range box\nSelect the check box for Labels\nSelect the check box for Confidence Level\nOutput Range:\n\nEnter A13 in the Output Range box (to identify the upper left corner of the section of the worksheet where the output will appear)\n\nClick OK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe estimated regression equation is:\n\\[\n\\hat{Y}_t = 24.1817 - 2.1060t + 0.9216t^2\n\\]\nThe forecast of sales revenue for year 11 is:\n\\[\n\\hat{Y}_t = 24.1817 - 2.1060(11) + 0.9216(11)^2 = 112.53, \\quad \\text{or} \\quad \\approx \\text{\\$112.5 million}.\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example-excel-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#nonlinear-trend-regression-example-excel-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Nonlinear Trend Regression: Example Excel",
    "text": "Nonlinear Trend Regression: Example Excel\n\n\n\n\nSteps for Creating a Scatter Plot with Trendline\n\nEnter/Access Data: Open the file Cholesterol\n\n\nStep 1: Select cells B2:B11\nStep 2: Click the Insert tab on the Ribbon\nStep 3: In the Charts group, click the Insert Scatter (X, Y) or Bubble Chart button\nStep 4: When the list of scatter diagram subtypes appears, click the Scatter button (the chart in the upper left corner)\nStep 5: Click OK; the scatter diagram will appear in the current worksheet\nStep 6: Position the mouse pointer over any data point in the scatter diagram, right-click, and choose Add Trendline\nStep 7: In the Format Trendline dialog box:\n\nChoose Polynomial from the Trend/Regression Type list\nEnter 2 in the Order box\nChoose Display Equation on chart"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend",
    "text": "Seasonality Without Trend\n\nWhen seasonality exists, we need to incorporate it into our forecasting models to ensure accurate forecasts.\nWe will first look at the case of a seasonal time series with no trend and then discuss how to model seasonality with trend.\nSeasonal patterns are recognized by seeing the same repeating pattern of highs and lows over successive and short periods of time (within a “short” period of time).\n\nA seasonal pattern might occur within a day, week, month, quarter, year, or some other interval no greater than a year.\nA seasonal pattern does not necessarily refer to the four seasons of the year (spring, summer, fall, and winter)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\n\n\n\n\nUmbrella Quarterly Sales Time Series\n\n\n\n\nYear\nQuarter\nSales\n\n\n\n\n1\n1\n125\n\n\n1\n2\n153\n\n\n1\n3\n106\n\n\n1\n4\n88\n\n\n2\n1\n118\n\n\n2\n2\n161\n\n\n2\n3\n133\n\n\n2\n4\n102\n\n\n3\n1\n138\n\n\n3\n2\n144\n\n\n3\n3\n113\n\n\n3\n4\n80\n\n\n4\n1\n109\n\n\n4\n2\n137\n\n\n4\n3\n125\n\n\n4\n4\n109\n\n\n5\n1\n130\n\n\n5\n2\n165\n\n\n5\n3\n128\n\n\n5\n4\n96"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\n\n\n\nUmbrella Quarterly Sales Time Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe time series plot does not clearly indicate any long-term trend in sales.\nHowever, close inspection of the plot does reveal a seasonal pattern.\n\nThe first and third quarters have moderate sales,\nthe second quarter has the highest sales, and\nthe fourth quarter tends to be the lowest quarter in terms of sales."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\n\nRecall that dummy variables can be used to deal with categorical independent variables in a multiple regression model.\nWe will treat the season as a categorical variable.\nRecall that when a categorical variable has \\(k\\) levels, \\(k - 1\\) dummy variables are required.\nIf there are four seasons, we have the following dummy variables with \\(Qtr4\\) being the reference level:\n\n\n\n\\[\n\\text{Qtr1} =\n\\begin{cases}\n1 & \\text{if Quarter 1} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad\n\\text{Qtr2} =\n\\begin{cases}\n1 & \\text{if Quarter 2} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad\n\\text{Qtr3} =\n\\begin{cases}\n1 & \\text{if Quarter 3} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\n\n\n\n\n\n\nYear\nQuarter\nQtr1\nQtr2\nQtr3\nSales\n\n\n\n\n1\n1\n1\n0\n0\n125\n\n\n1\n2\n0\n1\n0\n153\n\n\n1\n3\n0\n0\n1\n106\n\n\n1\n4\n0\n0\n0\n88\n\n\n2\n1\n1\n0\n0\n118\n\n\n2\n2\n0\n1\n0\n161\n\n\n2\n3\n0\n0\n1\n133\n\n\n2\n4\n0\n0\n0\n102\n\n\n3\n1\n1\n0\n0\n138\n\n\n3\n2\n0\n1\n0\n144\n\n\n3\n3\n0\n0\n1\n113\n\n\n3\n4\n0\n0\n0\n80\n\n\n4\n1\n1\n0\n0\n109\n\n\n4\n2\n0\n1\n0\n137\n\n\n4\n3\n0\n0\n1\n125\n\n\n4\n4\n0\n0\n0\n109\n\n\n5\n1\n1\n0\n0\n130\n\n\n5\n2\n0\n1\n0\n165\n\n\n5\n3\n0\n0\n1\n128\n\n\n5\n4\n0\n0\n0\n96"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\n\n\nGeneral Form of Estimated Regression Equation\n\n\\[\n\\hat{Y} = b_0 + b_1(Qtr1) + b_2(Qtr2) + b_3(Qtr3)\n\\]\n\n\nEstimated Regression Equation\n\n\\[\n\\text{Forecasted Sales} = 95.0 + 29.0(Qtr1) + 57.0(Qtr2) + 26.0(Qtr3)\n\\]\n\n\n\nForecast of Mean Quarterly Sales in Year 6\n\nQuarter 1: Sales = 95 + 29(1) + 57(0) + 26(0) = 124\nQuarter 2: Sales = 95 + 29(0) + 57(1) + 26(0) = 152\nQuarter 3: Sales = 95 + 29(0) + 57(0) + 26(1) = 121\nQuarter 4: Sales = 95 + 29(0) + 57(0) + 26(0) = 95"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-5",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-without-trend-example-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality Without Trend: Example",
    "text": "Seasonality Without Trend: Example\nNote that we could have obtained the quarterly forecasts for next year simply by computing the average number of umbrellas sold in each quarter:\n\n\n\nYear\nQuarter 1\nQuarter 2\nQuarter 3\nQuarter 4\n\n\n\n\n1\n125\n153\n106\n88\n\n\n2\n118\n161\n133\n102\n\n\n3\n138\n144\n113\n80\n\n\n4\n109\n137\n125\n109\n\n\n5\n130\n165\n128\n96\n\n\nAverage\n124\n152\n121\n95\n\n\n\nNonetheless, the regression output provides additional information that can be used to assess the accuracy of the forecast and determine the significance of the results."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality and Trend: Example",
    "text": "Seasonality and Trend: Example\n\n\n\nSmartphone Quarterly Sales Data (in Thousands)\n\n\n\nYear\nQuarter\nSales (1000s)\n\n\n\n\n1\n1\n4.8\n\n\n\n2\n4.1\n\n\n\n3\n6.0\n\n\n\n4\n6.5\n\n\n2\n1\n5.8\n\n\n\n2\n5.2\n\n\n\n3\n6.8\n\n\n\n4\n7.4\n\n\n3\n1\n6.0\n\n\n\n2\n5.6\n\n\n\n3\n7.5\n\n\n\n4\n7.8\n\n\n4\n1\n6.3\n\n\n\n2\n5.9\n\n\n\n3\n8.0\n\n\n\n4\n8.4"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality and Trend: Example",
    "text": "Seasonality and Trend: Example\nMultiple Regression Equation for Seasonal Effects and Trend\nThe general form of the estimated multiple regression equation for modeling both the quarterly seasonal effects and the linear trend in the smartphone time series is as follows:\n\\[\n\\hat{Y}_t = b_0 + b_1 \\text{Qtr1} + b_2 \\text{Qtr2} + b_3 \\text{Qtr3} + b_4 t\n\\]\nwhere\n\n\\(\\hat{Y}_t\\) = estimate or forecast of sales in period \\(t\\)\nQtr1 = 1 if time period \\(t\\) corresponds to the first quarter of the year; 0 otherwise\nQtr2 = 1 if time period \\(t\\) corresponds to the second quarter of the year; 0 otherwise\nQtr3 = 1 if time period \\(t\\) corresponds to the third quarter of the year; 0 otherwise\n\\(t\\) = time period"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality and Trend: Example",
    "text": "Seasonality and Trend: Example\n\n\n\nQuarterly Sales Data with Indicators and Period\n\n\n\nYear\nQuarter\nQtr1\nQtr2\nQtr3\nPeriod\nSales (1000s)\n\n\n\n\n1\n1\n1\n0\n0\n1\n4.8\n\n\n\n2\n0\n1\n0\n2\n4.1\n\n\n\n3\n0\n0\n1\n3\n6.0\n\n\n\n4\n0\n0\n0\n4\n6.5\n\n\n2\n1\n1\n0\n0\n5\n5.8\n\n\n\n2\n0\n1\n0\n6\n5.2\n\n\n\n3\n0\n0\n1\n7\n6.8\n\n\n\n4\n0\n0\n0\n8\n7.4\n\n\n3\n1\n1\n0\n0\n9\n6.0\n\n\n\n2\n0\n1\n0\n10\n5.6\n\n\n\n3\n0\n0\n1\n11\n7.5\n\n\n\n4\n0\n0\n0\n12\n7.8\n\n\n4\n1\n1\n0\n0\n13\n6.3\n\n\n\n2\n0\n1\n0\n14\n5.9\n\n\n\n3\n0\n0\n1\n15\n8.0\n\n\n\n4\n0\n0\n0\n16\n8.4"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality and Trend: Example",
    "text": "Seasonality and Trend: Example\n\nGeneral Form of Estimated Regression Equation\n\\[\n\\hat{Y} = b_0 + b_1 Qtr1 + b_2 \\cdot Qtr2 + b_3 \\cdot Qtr3 + b_4 \\cdot t\n\\]\nWhere \\(t\\) is the time period \\(t = 1, 2, \\ldots\\)\n\n\nEstimated Regression Equation\n\nForecast for Time Period 17 (Quarter 1 in Year 5)\n\\[\n\\text{Sales} = 6.07 - 1.36(1) - 2.03(0) - .304(0) + .146(17) = 7.19\n\\]\n\n\nForecast for Time Period 18 (Quarter 2 in Year 5)\n\\[\n\\text{Sales} = 6.07 - 1.36(0) - 2.03(1) - .304(0) + .146(18) = 6.67\n\\]\n\n\nForecast for Time Period 19 (Quarter 3 in Year 5)\n\\[\n\\text{Sales} = 6.07 - 1.36(0) - 2.03(0) - .304(1) + .146(19) = 8.54\n\\]\n\n\nForecast for Time Period 20 (Quarter 4 in Year 5)\n\\[\n\\text{Sales} = 6.07 - 1.36(0) - 2.03(0) - .304(0) + .146(20) = 8.99\n\\]\n\n\nThus, accounting for the seasonal effects and the linear trend in smartphone sales, the estimates of quarterly sales in year 5 are 7190, 6670, 8540, and 8990."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonality-and-trend-example-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonality and Trend: Example",
    "text": "Seasonality and Trend: Example\n\nThe dummy variables in the estimated multiple regression equation provide four estimated multiple regression equations, one for each quarter.\n\\[\n\\text{Quarter 1: Sales} = 6.07 - 1.36(1) - 2.03(0) - .304(0) + .146t = 4.71 + .146t\n\\]\nSimilarly, if time period \\(t\\) corresponds to quarters 2, 3, and 4, the estimates of quarterly sales are:\n\\[\n\\begin{align*}\n\\text{Quarter 2: Sales} & = 6.07 - 1.36(0) - 2.03(1) - .304(0) + .146t = 4.04 + .146t \\\\\n\\text{Quarter 3: Sales} & = 6.07 - 1.36(0) - 2.03(0) - .304(1) + .146t = 5.77 + .146t \\\\\n\\text{Quarter 4: Sales} & = 6.07 - 1.36(0) - 2.03(0) - .304(0) + .146t = 6.07 + .146t\n\\end{align*}\n\\]\n\nInterpretation of the Regression Coefficients\nThe slope of the trend line for each quarterly forecast equation is .146, indicating a growth in sales of about 146 smartphones per quarter.\nThe only difference in the four equations is that they have different intercepts.\nFor instance, the intercept for the quarter 1 equation is 4.71 and the intercept for the quarter 4 equation is 6.07. Thus, sales in quarter 1 are\n\\[\n4.71 - 6.07 = -1.36\n\\]\nor 1360 smartphones less than in quarter 4.\nIn other words, the estimated regression coefficient for Qtr1 provides an estimate of the difference in sales between quarter 1 and quarter 4."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-decomposition-motivation",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-decomposition-motivation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Decomposition: Motivation",
    "text": "Time Series Decomposition: Motivation\n\n\nUnderstanding what is really going on with a time series often depends upon the use of deseasonalized data.\nFor example, we might be interested in learning whether electrical power consumption is increasing in our area.\nSuppose we learn that electric power consumption in September is down 3% from the previous month. Is it correct to conclude that the electric power consumption is decreasing?\nWe must be careful with such information, because whenever a seasonal influence is present, such comparisons may be misleading if the data have not been deseasonalized.\nThe fact that electric power consumption is down 3% from August to September might be only the seasonal effect associated with a decrease in the use of air conditioning and not because of a long-term decline in the use of electric power.\nIndeed, after adjusting for the seasonal effect, we might even find that the use of electric power increased.\nMany other time series, such as unemployment statistics, home sales, and retail sales, are subject to strong seasonal influences. It is important to deseasonalize such data before making a judgment about any long-term trend."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-decomposition-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#time-series-decomposition-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Time Series Decomposition",
    "text": "Time Series Decomposition\nTime series decomposition can be used to separate or decompose a time series. It assumes that \\(Y_t\\), the actual time series value at period \\(t\\), is a function of three components:\n\nA trend component\nA seasonal component\nAn irregular or error component\n\n\nHow these three components are combined to generate the observed values of the time series depends upon whether we assume the relationship is best described by an additive or a multiplicative model."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#choosing-between-additive-and-multiplicative-models",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#choosing-between-additive-and-multiplicative-models",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Choosing Between Additive and Multiplicative Models",
    "text": "Choosing Between Additive and Multiplicative Models\n\nIf the seasonal fluctuations change over time, growing larger as the sales volume increases because of a long-term linear trend, then a multiplicative model should be used.\nMany business and economic time series follow this pattern."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#additive-decomposition-model",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#additive-decomposition-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Additive Decomposition Model",
    "text": "Additive Decomposition Model\n\nAn additive model is appropriate in situations where the seasonal fluctuations do not depend upon the level of the time series.\nIf the sizes of the seasonal fluctuations in earlier time periods are about the same as the sizes of the seasonal fluctuations in later time periods, an additive model is appropriate."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#additive-decomposition-model-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#additive-decomposition-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Additive Decomposition Model",
    "text": "Additive Decomposition Model\nAn additive decomposition model takes the following form:\n\\[\nY_t = Trend_t + Seasonal_t + Irregular_t\n\\]\nwhere\n\n\\(Trend_t\\) = trend value at time period \\(t\\)\n\\(Seasonal_t\\) = seasonal value at time period \\(t\\)\n\\(Irregular_t\\) = irregular value at time period \\(t\\)\nIn an additive model, the values for the three components are simply added together to obtain the actual time series value \\(Y_t\\).\nThe irregular or error component accounts for the variability in the time series that cannot be explained by the trend and seasonal components."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model",
    "text": "Multiplicative Decomposition Model\nA multiplicative decomposition model takes the following form:\n\\[\nY_t = Trend_t \\times Seasonal_t \\times Irregular_t\n\\]\nwhere\n\n\\(Trend_t\\) = trend value at time period \\(t\\)\n\\(Seasonal_t\\) = seasonal index at time period \\(t\\)\n\\(Irregular_t\\) = irregular index at time period \\(t\\)\nThe Census Bureau uses a multiplicative model in conjunction with its methodology for deseasonalizing time series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model",
    "text": "Multiplicative Decomposition Model\nIn this model, the trend, seasonal, and irregular components are multiplied to give the value of the time series.\n\nTrend is measured in units of the item being forecast.\nThe seasonal and irregular components are measured in relative terms:\n\nValues above 1.00 indicate effects above the trend.\nValues below 1.00 indicate effects below the trend."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example",
    "text": "Multiplicative Decomposition Model: Example\n\n\nSales are lowest in the second quarter of each year and increase in quarters 3 and 4.\nWe conclude that a seasonal pattern exists for the smartphone sales time series."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#procedures-overview",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#procedures-overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Procedures: Overview",
    "text": "Procedures: Overview\n\nThe computational procedure to identify each quarter’s seasonal influence begins by computing a moving average to remove the combined seasonal and irregular effects from the data, leaving us with a time series that contains only trend and any remaining random variation not removed by the moving average calculations.\n\nSteps 1-2: Calculate centered moving averages (Initial deseasonalized data).\nStep 3: Determine the seasonal and irregular factors for each time series value. (Individual factors due to seasonality and irregularity).\nStep 4: Determine Seasonal Index (SI) for each season (Updating the factors).\nStep 5: Determine the deseasonalized data (Update the deseasonalized data using SI).\nStep 6: Determine the Trend Component of the deseasonalized data (Model trend).\nStep 7: Determine the deseasonalized predictions (based on the Trend Component). (Forecast trend).\nStep 8: Add the seasonality to the predictions."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 1)",
    "text": "Multiplicative Decomposition Model: Example (Step 1)\n\nMoving Average Calculation\n\nBecause we are working with a quarterly series, we will use four data values in each moving average.\nThe moving average calculation for the first four quarters of the smartphone sales data is:\n\n\\[\n\\text{First moving average} = \\frac{4.8 + 4.1 + 6.0 + 6.5}{4} = \\frac{21.4}{4} = 5.35\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 1)",
    "text": "Multiplicative Decomposition Model: Example (Step 1)\n\nMoving Average Calculation\n\nNote that the moving average calculation for the first four quarters yields the average quarterly sales over year 1 of the time series.\nContinuing the moving average calculations, we next add the 5.8 value for the first quarter of year 2 and drop the 4.8 for the first quarter of year 1.\n\nThus, the second moving average is:\n\\[\n\\text{Second moving average} = \\frac{4.1 + 6.0 + 6.5 + 5.8}{4} = \\frac{22.4}{4} = 5.60\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 1)",
    "text": "Multiplicative Decomposition Model: Example (Step 1)\nMoving Average Calculation\nSimilarly, the third moving average calculation is:\n\\[\n(6.0 + 6.5 + 5.8 + 5.2) / 4 = 5.875\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-1-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 1)",
    "text": "Multiplicative Decomposition Model: Example (Step 1)\nBefore we proceed with the moving average calculations for the entire time series, let us return to the first moving average calculation, which resulted in a value of 5.35.\n\nThe 5.35 value is the average quarterly sales volume for year 1.\nAs we look back at the calculation of the 5.35 value, associating 5.35 with the “middle” of the moving average group makes sense."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 2)",
    "text": "Multiplicative Decomposition Model: Example (Step 2)\nPeriod Alignment with Moving Averages\nNote that with four quarters in the moving average, there is no middle period.\n\nThe 5.35 value really corresponds to period 2.5, the last half of quarter 2 and the first half of quarter 3.\nSimilarly, if we go to the next moving average value of 5.60, the middle period corresponds to period 3.5, the last half of quarter 3 and the first half of quarter 4."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 2)",
    "text": "Multiplicative Decomposition Model: Example (Step 2)\nCentering Moving Averages\nThe two moving average values we computed do not correspond directly to the original quarters of the time series.\n\nWe can resolve this by computing the average of the two moving averages.\nThe center of the first moving average is period 2.5, and the center of the second moving average is period 3.5.\nBy averaging these, we center the moving average at quarter 3, exactly where it should be."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 2)",
    "text": "Multiplicative Decomposition Model: Example (Step 2)\n\nCentered Moving Average Calculation\nThis moving average is referred to as a centered moving average.\n\nThus, the centered moving average for period 3 is:\n\n\\[\n\\frac{5.35 + 5.60}{2} = 5.475\n\\]\n\n\nSimilarly, the centered moving average value for period 4 is:\n\n\n\\[\n\\frac{5.60 + 5.875}{2} = 5.738\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-2-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 2)",
    "text": "Multiplicative Decomposition Model: Example (Step 2)\n\nCentered Moving Average Calculations\nSummary of the moving average and centered moving average calculations for the smartphone sales data.\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nQuarter\nSales (1000s)\nFour-Quarter Moving Average\nCentered Moving Average\n\n\n\n\n1\n1\n4.8\n\n\n\n\n1\n2\n4.1\n\n\n\n\n\n\n\n5.350\n5.475\n\n\n1\n3\n6.0\n\n\n\n\n\n\n\n5.600\n5.738\n\n\n1\n4\n6.5\n\n\n\n\n\n\n\n5.875\n5.975\n\n\n2\n1\n5.8\n\n\n\n\n\n\n\n6.075\n6.188\n\n\n2\n2\n5.2\n\n\n\n\n\n\n\n6.300\n6.325\n\n\n2\n3\n6.8\n\n\n\n\n\n\n\n6.350\n6.400\n\n\n2\n4\n7.4\n\n\n\n\n\n\n\n6.450\n6.538\n\n\n3\n1\n6.0\n\n\n\n\n\n\n\n6.625\n6.675\n\n\n3\n2\n5.6\n\n\n\n\n\n\n\n6.725\n6.763\n\n\n3\n3\n7.5\n\n\n\n\n\n\n\n6.800\n6.838\n\n\n3\n4\n7.8\n\n\n\n\n\n\n\n6.875\n6.938\n\n\n4\n1\n6.3\n\n\n\n\n\n\n\n7.000\n7.075\n\n\n4\n2\n5.9\n\n\n\n\n\n\n\n7.150\n\n\n\n4\n3\n8.0\n\n\n\n\n4\n4\n8.4"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 3)",
    "text": "Multiplicative Decomposition Model: Example (Step 3)\nWhat do the centered moving averages tell us about this time series?\n\n\n\nThe Figure shows a time series plot of the actual time series values and the centered moving average values.\nNote particularly how the centered moving average values tend to “smooth out” both the seasonal and irregular fluctuations in the time series.\nThe centered moving averages represent the trend in the data and any random variation that was not removed by using moving averages to smooth the data."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nThe multiplicative decomposition model is:\n\\[\nY_t = Trend_t \\times Seasonal_t \\times Irregular_t\n\\]\nBy dividing each side of this equation by the trend component \\(T_t\\), we can identify the combined seasonal-irregular effect in the time series.\n\\[\n\\frac{Y_t}{Trend_t} = \\frac{Trend_t \\times Seasonal_t \\times Irregular_t}{Trend_t} = Seasonal_t \\times Irregular_t\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nFor example, the third quarter of year 1 shows a trend value of 5.475 (the centered moving average).\nSo:\n\\[\n\\frac{6.0}{5.475} = 1.096\n\\]\nThis is the combined seasonal-irregular value."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\n\nSeasonal-irregular values for the entire time series.\n\n\n\n\n\n\n\n\n\n\nYear\nQuarter\nSales (1000s)\nCentered Moving Average\nSeasonal-Irregular Value\n\n\n\n\n1\n1\n4.8\n\n\n\n\n1\n2\n4.1\n\n\n\n\n1\n3\n6.0\n5.475\n1.096\n\n\n1\n4\n6.5\n5.738\n1.133\n\n\n2\n1\n5.8\n5.975\n0.971\n\n\n2\n2\n5.2\n6.188\n0.840\n\n\n2\n3\n6.8\n6.325\n1.075\n\n\n2\n4\n7.4\n6.400\n1.156\n\n\n3\n1\n6.0\n6.538\n0.918\n\n\n3\n2\n5.6\n6.675\n0.839\n\n\n3\n3\n7.5\n6.763\n1.109\n\n\n3\n4\n7.8\n6.838\n1.141\n\n\n4\n1\n6.3\n6.938\n0.908\n\n\n4\n2\n5.9\n7.075\n0.834\n\n\n4\n3\n8.0\n\n\n\n\n4\n4\n8.4"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-3",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nSeasonal-Irregular Values for Third Quarter\nConsider the seasonal-irregular values for the third quarter: 1.096, 1.075, and 1.109.\n\nSeasonal-irregular values greater than 1.00 indicate effects above the trend estimate, while values below 1.00 indicate effects below the trend estimate.\nThe three seasonal-irregular values for quarter 3 show an above-average effect in the third quarter."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-4",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nAveraging to Estimate Seasonal Influence\nSince the year-to-year fluctuations in the seasonal-irregular values are primarily due to random error, we can average the computed values to eliminate the irregular influence and obtain an estimate of the third-quarter seasonal influence.\n\\[\n\\text{Seasonal effect of quarter 3} = \\frac{1.096 + 1.075 + 1.109}{3} = 1.09\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-5",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nSeasonal-Irregular Values and Seasonal Indexes\n\n\n\nQuarter\nSeasonal-Irregular Values\nSeasonal Index\n\n\n\n\n1\n0.971, 0.918, 0.908\n0.93\n\n\n2\n0.840, 0.839, 0.834\n0.84\n\n\n3\n1.096, 1.075, 1.109\n1.09\n\n\n4\n1.133, 1.156, 1.141\n1.14\n\n\n\n\nThe seasonal indexes for the four quarters are 0.93, 0.84, 1.09, and 1.14.\nThe best sales quarter is the fourth quarter, with sales averaging 14% above the trend estimate.\nThe worst, or slowest, sales quarter is the second quarter; its seasonal index of 0.84 shows that the sales average is 16% below the trend estimate.\nThe seasonal component corresponds clearly to the intuitive expectation that smartphone sales increase when a new school year begins (quarter 3) and for the holiday season (quarter 4)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-6",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\nOne final adjustment is sometimes necessary in obtaining the seasonal indexes.\n\nThe multiplicative model requires that the average seasonal index equal 1.00, so the sum of the four seasonal indexes must equal 4.00.\nIn other words, the seasonal effects must even out over the year.\nThe average of the seasonal indexes in our example is equal to 1.00, so this type of adjustment is not necessary."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-7",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#multiplicative-decomposition-model-example-step-4-7",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiplicative Decomposition Model: Example (Step 4)",
    "text": "Multiplicative Decomposition Model: Example (Step 4)\n\nMaking Adjustments (If Required)\nIn cases where adjustment is necessary:\n\nMultiply each seasonal index by the number of seasons divided by the sum of the unadjusted seasonal indexes.\n\nFor quarterly data, this means:\n\\[\n\\text{Adjusted Seasonal Index} = \\frac{4}{\\text{sum of the unadjusted seasonal indexes}}\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-step-5",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-step-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series (Step 5)",
    "text": "Deseasonalizing the Time Series (Step 5)\n\nA time series that has had the seasonal effects removed is referred to as a deseasonalized time series, and the process of using the seasonal indexes to remove the seasonal effects from a time series is referred to as deseasonalizing the time series.\n\nUsing a multiplicative decomposition model, we deseasonalize a time series by dividing each observation by its corresponding seasonal index.\nThe multiplicative decomposition model is:\n\n\\[\nY_t = Trend_t \\times Seasonal_t \\times Irregular_t\n\\]\nWhen we divide each time series observation \\(Y_t\\) by its corresponding seasonal index, the resulting data show only trend and random variability (the irregular component)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalized-sales-data-table-step-5",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalized-sales-data-table-step-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalized Sales Data Table (Step 5)",
    "text": "Deseasonalized Sales Data Table (Step 5)\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nQuarter\nTime Period\nSales (1000s)\nSeasonal Index\nDeseasonalized Sales\n\n\n\n\n1\n1\n1\n4.8\n0.93\n5.16\n\n\n1\n2\n2\n4.1\n0.84\n4.88\n\n\n1\n3\n3\n6.0\n1.09\n5.50\n\n\n1\n4\n4\n6.5\n1.14\n5.70\n\n\n2\n1\n5\n5.8\n0.93\n6.24\n\n\n2\n2\n6\n5.2\n0.84\n6.19\n\n\n2\n3\n7\n6.8\n1.09\n6.24\n\n\n2\n4\n8\n7.4\n1.14\n6.49\n\n\n3\n1\n9\n6.0\n0.93\n6.45\n\n\n3\n2\n10\n5.6\n0.84\n6.67\n\n\n3\n3\n11\n7.5\n1.09\n6.88\n\n\n3\n4\n12\n7.8\n1.14\n6.84\n\n\n4\n1\n13\n6.3\n0.93\n6.77\n\n\n4\n2\n14\n5.9\n0.84\n7.02\n\n\n4\n3\n15\n8.0\n1.09\n7.34\n\n\n4\n4\n16\n8.4\n1.14\n7.37"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalized-sales-plot-step-5",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalized-sales-plot-step-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalized Sales Plot (Step 5)",
    "text": "Deseasonalized Sales Plot (Step 5)"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-step-5-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-step-5-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series (Step 5)",
    "text": "Deseasonalizing the Time Series (Step 5)\nIdentifying Trend in Deseasonalized Data\nThe graph of the deseasonalized smartphone sales time series shown in the Figure appears to have an upward linear trend.\n\nTo identify this trend, we will fit a linear trend equation to the deseasonalized time series.\nThe only difference is that we will be fitting a trend line to the deseasonalized data instead of the original data."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-linear-trend-equation-step-6",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-linear-trend-equation-step-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series: Linear Trend Equation (Step 6)",
    "text": "Deseasonalizing the Time Series: Linear Trend Equation (Step 6)\nRecall that for a linear trend, the estimated regression equation can be written as:\n\\[\nT_t = b_0 + b_1 t\n\\]\nwhere\n\n\\(T_t\\): linear trend forecast in period \\(t\\)\n\\(b_0\\): intercept of the linear trend line\n\\(b_1\\): slope of the trend line\n\\(t\\): time period"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-fitting-a-linear-trend-line-step-6",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-fitting-a-linear-trend-line-step-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series: Fitting a Linear Trend Line (Step 6)",
    "text": "Deseasonalizing the Time Series: Fitting a Linear Trend Line (Step 6)\nTo fit a linear trend line to the deseasonalized data, the only change is that the deseasonalized time series values are used instead of the observed values \\(Y_t\\) in computing \\(b_0\\) and \\(b_1\\).\nUsing Excel’s Regression tool, the estimated linear trend equation is:\n\\[\n\\text{Deseasonalized Sales} = 5.10 + 0.148t\n\\]"
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-interpretation-of-the-slope-step-6",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-interpretation-of-the-slope-step-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series: Interpretation of the Slope (Step 6)",
    "text": "Deseasonalizing the Time Series: Interpretation of the Slope (Step 6)\n\nThe slope of 0.148 indicates that over the past 16 quarters, the firm averaged a deseasonalized growth in sales of about 148 smartphones per quarter.\nIf we assume that the past 16-quarter trend in sales data is a reasonably good indicator of the future, this equation can be used to develop a trend projection for future quarters."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-example-projection-step-7",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#deseasonalizing-the-time-series-example-projection-step-7",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Deseasonalizing the Time Series: Example Projection (Step 7)",
    "text": "Deseasonalizing the Time Series: Example Projection (Step 7)\nSubstituting \\(t = 17\\) into the equation yields next quarter’s deseasonalized trend projection, \\(T_{17}\\):\n\\[\nT_{17} = 5.10 + 0.148(17) = 7.616\n\\]\nThus, using the deseasonalized data, the linear trend forecast for next quarter (period 17) is 7616 smartphones.\nSimilarly, the deseasonalized trend forecasts for the next three quarters (periods 18, 19, and 20) are 7764, 7912, and 8060 smartphones, respectively."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonal-adjustments-step-8",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonal-adjustments-step-8",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonal Adjustments (Step 8)",
    "text": "Seasonal Adjustments (Step 8)\n\nThe final step in developing the forecast when both trend and seasonal components are present is to use the seasonal indexes to adjust the deseasonalized trend projections.\n\nFor the smartphone sales example, we have a deseasonalized trend projection for the next four quarters.\nNow, we must adjust the forecast for the seasonal effect.\n\nThe seasonal index for the first quarter of year 5 (\\(t = 17\\)) is 0.93, so we obtain the quarterly forecast by multiplying the deseasonalized forecast based on trend \\((T_{17} = 7616)\\) by the seasonal index (0.93). Thus, the forecast for the next quarter is \\(7616 \\times 0.93 = 7083\\)."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonal-adjustments-quarterly-forecasts-table-step-8",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#seasonal-adjustments-quarterly-forecasts-table-step-8",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Seasonal Adjustments: Quarterly Forecasts Table (Step 8)",
    "text": "Seasonal Adjustments: Quarterly Forecasts Table (Step 8)\n\n\n\n\n\n\n\n\n\n\nYear\nQuarter\nDeseasonalized Trend Forecast\nSeasonal Index\nQuarterly Forecast\n\n\n\n\n5\n1\n7616\n0.93\n\\(7616 \\times 0.93 = 7083\\)\n\n\n5\n2\n7764\n0.84\n\\(7764 \\times 0.84 = 6522\\)\n\n\n5\n3\n7912\n1.09\n\\(7912 \\times 1.09 = 8624\\)\n\n\n5\n4\n8060\n1.14\n\\(8060 \\times 1.14 = 9188\\)\n\n\n\n\nThe high-volume fourth quarter has a 9188-unit forecast, and the low-volume second quarter has a 6522-unit forecast."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#cyclical-component",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#cyclical-component",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Cyclical Component",
    "text": "Cyclical Component\n\nMathematically, the multiplicative model can be expanded to include a cyclical component.\n\\[\nY_t = Trend_t \\times Cyclical_t \\times Seasonal_t \\times Irregular_t\n\\]\n\nThe cyclical component, like the seasonal component, is expressed as a percentage of trend.\nThis component is attributable to multiyear cycles in the time series.\n\nIt is analogous to the seasonal component but over a longer period. However, due to the length of time involved, obtaining enough relevant data to estimate the cyclical component is often difficult."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#cyclical-component-challenges",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#cyclical-component-challenges",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Cyclical Component: Challenges",
    "text": "Cyclical Component: Challenges\n\nCycles usually vary in length, making it difficult to identify or separate cyclical effects from long-term trend effects.\nIn practice, these effects are often combined and referred to as a combined trend-cycle component."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#summary-1",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nTime Series Analysis: Understand patterns (trend, seasonality, cycles) to forecast future values.\nForecast Accuracy: Key measures include MAE, MSE, and MAPE for assessing model performance.\nSmoothing Techniques: Moving averages and exponential smoothing help in handling horizontal patterns.\nTrend and Seasonality: Apply decomposition and regression for more complex time series with trends and seasonal variations."
  },
  {
    "objectID": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#summary-2",
    "href": "lecture_slides/17_chapter_time_series/17_chapter_time_series.html#summary-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nImportant: The following table is not exhaustive; it merely illustrates the examples discussed in the lecture. Remember that within the General Linear Model framework, these methods can be combined and adapted based on the specific objectives of the analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Method / Pattern\nHorizontal\nLinear Trend\nNon-linear Trend\nSeasonal\nSeasonal + Trend\n\n\n\n\n1. Naive Forecast\nX\n\n\n\n\n\n\n2. Hist. Data Average Method\nX\n\n\n\n\n\n\n3. Moving Average (\\(k\\) and \\(1/k\\))\nX\n\n\n\n\n\n\n4. Weighted Moving Average (\\(k\\), weights should sum to 1)\nX\n\n\n\n\n\n\n5. Exponential Smoothing (\\(\\alpha\\))\nX\n\n\n\n\n\n\n6. Regression Model: Lag Variable\n\nX\nX\nX\nX\n\n\n7. Regression Model: Time as Independent Variable\n\nX\n\n\n\n\n\n8. Regression Model: Time as Independent Variable with Transformation\n\n\nX\n\n\n\n\n9. Regression Model: Season as Independent Variable\n\n\n\nX\n\n\n\n10. Regression Model: Time and Season as Independent Variables\n\n\n\n\nX"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#overview",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nPhilosophies and frameworks\nStatistical Process Control (SPC) with Control Charts for\n\nProcess mean (\\(\\mu\\))\nProcess variability (Range, R)\nProcess proportion of defectives (\\(p\\))\nProcess expected number of defectives (\\(np\\)) in a sample of size \\(n\\)\n\n\n\n\nAcceptance Sampling\nMultiple Sampling Plans"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality",
    "text": "Quality\nThe American Society for Quality (ASQ) defines quality as:\n\n\n“the characteristics of a product or service that bear on its ability to satisfy stated or implied needs.”\n\n\n\nU.S. Organizations recognize that they must strive for high levels of quality (Car industry in 1980’s).\nIn addition to managerial philosophy, they have increased the emphasis on statistical methods for monitoring and maintaining quality."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#total-quality",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#total-quality",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Total Quality",
    "text": "Total Quality\n\nTotal Quality (TQ) is a people-focused management system that aims at continual increase in customer satisfaction at continually lower real cost.\n\nTQ is a total system approach (not a separate work program) and an integral part of high-level strategy. TQ works horizontally across functions (Concurrent engineering).\nTQ involves all employees, top to bottom, and extends backward and forward to include both the supply and customer chains.\nTQ stresses learning and continuous improvement as keys to organizational success.\n\nLearning -&gt; Improve quality -&gt; Increased productivity -&gt; Lower cost -&gt; Gain in market share -&gt; Stay in business -&gt; Responsibility to society."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Gurus and Philosophies",
    "text": "Quality Gurus and Philosophies\n\n\n\n\n\n\n\n\n\n\n\n\nDr. Walter A. Shewhart\n\n\nDeveloped a set of principles that are the basis for what is known today as process control.\nConstructed a diagram that would now be recognized as a statistical control chart.\nBrought together the disciplines of statistics, engineering, and economics and changed the course of industrial history.\nRecognized as the father of statistical quality control."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Gurus and Philosophies",
    "text": "Quality Gurus and Philosophies\n\n\n\n\n\n\n\n\n\n\n\n\nDr. W. Edwards Deming\n\n\n“Without data, you’re just another person with an opinion.”\n\n\n\n\nHelped educate the Japanese on quality management shortly after World War II.\nFather of modern Total Quality Management: Stressed that the focus on quality must be led by managers.\nDeveloped a list of 14 points he believed represent the key responsibilities of managers.\nJapan named its national quality award the Deming Prize in his honor."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-gurus-and-philosophies-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Gurus and Philosophies",
    "text": "Quality Gurus and Philosophies\n\n\n\n\n\n\n\n\n\n\n\n\nJoseph Juran\n\n\nHelped educate the Japanese on quality management shortly after World War II.\nProposed a simple definition of quality: fitness for use\nHis approach to quality focused on three quality processes: quality planning, quality control, and quality improvement."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-malcolm-baldrige-national-quality-award",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-malcolm-baldrige-national-quality-award",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Frameworks: Malcolm Baldrige National Quality Award",
    "text": "Quality Frameworks: Malcolm Baldrige National Quality Award\n\nEstablished in 1987 and given by the U.S. president to organizations that judged to be outstanding in:\n\nLeadership\nStrategy\nCustomer measurement, analysis, and knowledge management\nWorkforce\nOperations\nResults\n\nThe first awards were presented in 1988.\nThe Award is named for Malcolm Baldrige, who was U.S. Secretary of Commerce from 1981 to 1987.\nThe U.S. Commerce Department’s National Institute of Standards and Technology (NIST) manages the Award."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-iso-9000-series",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-iso-9000-series",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Frameworks: ISO 9000 series",
    "text": "Quality Frameworks: ISO 9000 series\n\nA series of five standards published in 1987 by the International Organization for Standardization in Geneva, Switzerland.\nThe standards describe the need for:\n\nan effective quality system,\nensuring that measuring and testing equipment is calibrated regularly,\nmaintaining an adequate record-keeping system.\n\nISO 9000 registration determines whether a company complies with its own quality system."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-six-sigma",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-six-sigma",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Frameworks: Six Sigma",
    "text": "Quality Frameworks: Six Sigma\n\nEmphasizes the importance of taking measurements on critical quality characteristics.\nFocuses more on the process variation than the process mean.\nA process is a Six Sigma process if its mean is centered, and the distances between the mean and Upper and Lower Specifications are six times the process’s standard deviation (σ).\nSix sigma level of quality means that for every million opportunities, no more than 3.4 defects will occur if the process mean drifts or shifts slightly.\nThe methodology created to reach this quality goal is referred to as Six Sigma."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-six-sigma-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-six-sigma-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Frameworks: Six Sigma",
    "text": "Quality Frameworks: Six Sigma\n\n\n\nSix Sigma aims for defects to be so minimal that only 3.4 per million occur.\nThe target is represented as a bell curve centered at the mean (\\(\\mu\\)) with six standard deviations (\\(\\sigma\\)) between the mean and the specification limits.\n\n\n\\[\n\\text{Defective Rate} = \\text{Total Tail Areas}\n\\] \\[\n\\text{Defective Rate} = \\text{NORM.S.DIST(-6, TRUE) + {1 - NORM.S.DIST(6, TRUE)}} \\approx 0\n\\]\n\nWith Six Sigma, shifts in the process mean are accommodated.\n\nA shift of 1.5\\(\\sigma\\) still maintains low defect rates.\nThis shift leads to a defect rate of approximately 3.4 per million produced."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-service-sector",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-frameworks-service-sector",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Frameworks: Service Sector",
    "text": "Quality Frameworks: Service Sector\n\nQuality control is critical in service businesses (e.g., law firms, hotels, airlines, restaurants, and banks).\nFocus on customer satisfaction and improving the customer experience.\nServices are often intangible, making customer satisfaction subjective and quality measurement challenging."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-terminology-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#quality-terminology-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Quality Terminology",
    "text": "Quality Terminology\n\nQuality assurance refers to the entire system of policies, procedures, and guidelines established by an organization to achieve and maintain quality.\nQuality assurance consists of two functions:\n\nQuality engineering - its objective is to include quality in the design of products and processes and to identify potential quality problems prior to production.\nQuality control consists of making a series of inspections and measurements to determine whether quality standards are being met."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-methods-for-quality-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-methods-for-quality-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Methods for Quality",
    "text": "Statistical Methods for Quality\n\nStatistical Design of Experiments (DOEs): For process and product design.\n\nOne-factor ANOVA.\nFactorial design, etc.\n\nStatistical Process Control (SPC): Monitoring and controlling process for stability over time.\nAcceptance Sampling: Sorting products into conforming or nonconforming products, and lot sentencing."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC)",
    "text": "Statistical Process Control (SPC)\n\nCritical process quality characteristic is inspected and measured.\n\nThere will always be variation in the unit-to-unit measurements.\nTwo types of causes of variation: Common Causes and Assignable Causes.\nCommon causes are allowed for the time being, but assignable causes need to be removed immediately.\nAdjusting a process with only common cause variations will increase process variation. (The Deming’s funnel experiment.)\n\nThe goal of SPC is to detect and eliminate the assignable causes as soon as possible, based on samples from the process over time."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-common-causes",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-common-causes",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC): Common Causes",
    "text": "Statistical Process Control (SPC): Common Causes\n\nRandomly occurring variations in materials, humidity, temperature, etc.\nVariation is natural and unexplained.\nAffecting all units/outputs.\nVariations the producer cannot control (in the short-term).\nProcess is in statistical control and hence, does not need adjustment."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-assignable-causes",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-assignable-causes",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC): Assignable Causes",
    "text": "Statistical Process Control (SPC): Assignable Causes\n\nNon-random and identifiable (significant) variations in output due to tools wearing out, operator error, incorrect machine settings of a few machines, poor quality raw material from certain vendors, etc.\nAffecting certain or individual outputs.\nProcess is out of control, and the producer can and must control (immediately).\nCorrective action should be taken."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-applications",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-applications",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC): Applications",
    "text": "Statistical Process Control (SPC): Applications\n\n\nTemperature of a die\nWeights of bags of potato chips of a filling process\nWeekly sales figures\nCustomer complaints\nProduct responses\nInventory level\nAbsenteeism\nAccidents\nAccount receivables\nProcess defective rate"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#spc-vs.-hypothesis-testing",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#spc-vs.-hypothesis-testing",
    "title": " MGMT 30500: Business Statistics ",
    "section": "SPC vs. Hypothesis Testing",
    "text": "SPC vs. Hypothesis Testing\nWe use SPC to monitor and control the parameters (mean, variability, or proportion) of a process at each time and over time. Based on hypothesis-testing methodology, we have:\n\\[\nH_0: \\text{Process is in control } (\\mu = \\mu_0, \\text{ or } R = R_0)\n\\]\n\\[\nH_a: \\text{Process is out of control}\n\\]\n\nThe test statistic is the monitoring statistic in SPC: \\(\\overline{x}\\) for process mean, \\(\\overline{p}\\) for process proportion, etc."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-decisions-and-errors",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-decisions-and-errors",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC): Decisions and Errors",
    "text": "Statistical Process Control (SPC): Decisions and Errors\n\n\n\n\n\n\n\n\n\nDecision\nState of Production Process\n\n\n\n\n\n\nH0 True Process in Control\nH0 False Process Out of Control\n\n\nContinue Process\nCorrect decision\nType II error (allowing an out-of-control process to continue)\n\n\nAdjust Process\nType I error (adjusting an in-control process)\nCorrect decision\n\n\n\n\n\nControl the Probability of a False Alarm at \\(\\alpha\\) (the significance level)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-control-charts",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#statistical-process-control-spc-control-charts",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Statistical Process Control (SPC): Control Charts",
    "text": "Statistical Process Control (SPC): Control Charts\n\nSPC uses graphical displays known as control charts to monitor a production process (mean, variability, or percent defectives).\n\n\n\n\n\n\n\n\n\n\nThe Center Line (CL) represents what the process parameter value should be when the process is in control (denoted by \\(\\mu_0\\) or \\(R_0\\)).\nThe Upper Control Limit (UCL) and Lower Control Limit (LCL) are the two-sided critical values for the monitoring statistic.\n\nThey are chosen so that when the process is in control, there will be a high probability that the monitoring statistic value will be between the two lines.\nThey are chosen to allow common causes variation in the process and hence in the monitoring statistic.\nValues outside of the control limits provide strong evidence that the process is out of control."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known\nExample: KJW Packaging\nWhen KJW’s cereal carton filling process is in control, the weight of cartons of cereal filled by the process is normally distributed with a mean of 16.05 ounces and a standard deviation of 0.10 ounces.\n\nQuestion: What should be the control limits for \\(\\bar{X}\\) based on a sample of n = 10 observations?"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known\nAssume in-control process mean is \\(\\mu_0\\) and standard deviation is \\(\\sigma\\).\nFor each \\(\\bar{X}_j\\),\n\\[\n  \\text{UCL} = \\mu_0 + 3 \\frac{\\sigma}{\\sqrt{n}}\n  \\]\n\\[\n  \\text{CL} = \\mu_0\n  \\]\n\\[\n  \\text{LCL} = \\mu_0 - 3 \\frac{\\sigma}{\\sqrt{n}}\n  \\]\nwhere \\(\\frac{\\sigma}{\\sqrt{n}}\\) is the standard deviation of \\(\\bar{X}_j\\) with a sample size of n.\n\n3 is the z-multiplier with 99.97% confidence level, or a false alarm rate of \\(\\alpha = 0.27\\%\\)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known\nExample: KJW Packaging\n\\[\n\\mu_0 = 16.05, \\quad \\sigma = 0.10, \\quad n = 10\n\\]\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{0.10}{\\sqrt{10}} = 0.032\n\\] \\[\n\\text{UCL} = 16.05 + 3(0.032) = 16.146\n\\]\n\\[\n\\text{CL} = 16.05\n\\] \\[\n\\text{LCL} = 16.05 - 3(0.032) = 15.954\n\\]\nA sample mean based on \\(n\\) observations outside the limits indicates the process mean is out of control."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-4",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-known-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Known\nExample: KJW Packaging\n\n\nNote that the fifth sample shows there is strong evidence that the process is out of control.\nIt indicates that assignable causes of output variation are present and that underfilling is occurring."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\n\nAssume the in-control process mean and standard deviation are unknown.\n\nThey need to be estimated from an in-control process.\n\n\n\nExample: Jensen Computer Supplies Problem\nJCS manufactures 3.5-inch-diameter solid state drives. It wants to develop an X-bar (for process mean) and Range charts (for process variability) based on twenty samples of 5 drives each."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\nExample: Jensen Computer Supplies Problem\nData and Sample Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nSample\nObservation 1\nObservation 2\nObservation 3\nObservation 4\nObservation 5\nMean\nRange\n\n\n\n\n1\n3.5056\n3.5086\n3.5144\n3.5009\n3.5030\n3.5065\n0.0135\n\n\n2\n3.4882\n3.5085\n3.4884\n3.5250\n3.5031\n3.5026\n0.0368\n\n\n3\n3.4897\n3.4898\n3.4995\n3.5130\n3.4969\n3.4978\n0.0233\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n18\n3.4959\n3.4823\n3.4964\n3.5082\n3.4871\n3.4940\n0.0259\n\n\n19\n3.4878\n3.4864\n3.4960\n3.5070\n3.4894\n3.4933\n0.0259\n\n\n20\n3.4969\n3.5144\n3.5053\n3.4985\n3.4885\n3.5007\n0.0259\n\n\n\n\n\n\n\nAverage\n3.4995\n0.0253\n\n\n\n\nTable of sample observations with 5 observations per sample and calculated means and ranges."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\nFor each \\(\\bar{X}_j\\),\n\\[\n\\text{UCL} = \\bar{\\bar{X}} + A_2 \\bar{R}\n\\]\n\\[\n  \\text{CL} = \\bar{\\bar{X}}\n\\]\n\\[\n  \\text{LCL} = \\bar{\\bar{X}} - A_2 \\bar{R}\n\\]\nwhere:\n\n\\(\\bar{\\bar{X}}\\) = overall sample mean (e.g., 3.4995)\n\\(\\bar{R}\\) = average range (e.g., 0.0253)\n\\(A_2\\) = constant depending on \\(n\\) (from the “Factors for Control Charts” table)"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#factors-for-barx-control-chart",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#factors-for-barx-control-chart",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Factors for \\(\\bar{X}\\) Control Chart",
    "text": "Factors for \\(\\bar{X}\\) Control Chart\n\nThe American Society for Testing and Materials Manual on Presentation of Data and Control Chart Analysis provides valuses for \\(d_2\\) for different sample sizes (\\(n\\)) as shown below:\n\n\n\nn\n\\(d_2\\)\n\\(A_2\\)\n\\(d_3\\)\n\\(D_3\\)\n\\(D_4\\)\n\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n5\n2.326\n0.577\n0.864\n0\n2.114\n\n\n6\n2.534\n0.483\n0.848\n0\n2.004\n\n\n7\n2.704\n0.419\n0.833\n0.076\n1.924\n\n\n8\n2.847\n0.373\n0.820\n0.136\n1.864\n\n\n9\n2.970\n0.337\n0.808\n0.184\n1.816\n\n\n10\n3.078\n0.308\n0.797\n0.223\n1.777\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\\(d_2\\) and \\(d_3\\): Constants used for range calculations.\n\\(A_2\\): Used in \\(\\bar{X}\\) charts for control limits.\n\\(D_3\\) and \\(D_4\\): Constants for the R-chart control limits."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#factors-for-barx-control-chart-explanation",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#factors-for-barx-control-chart-explanation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Factors for \\(\\bar{X}\\) Control Chart: Explanation",
    "text": "Factors for \\(\\bar{X}\\) Control Chart: Explanation\n\n\nn: The sample size, or the number of observations within each subgroup.\n\\(d_2\\): A constant used to estimate the mean of the range (\\(\\bar{R}\\)) when the sample size \\(n\\) is known.\n\\(A_2\\): A constant for calculating control limits for the \\(\\bar{X}\\) (X-bar) chart:\n\n\n\\[\n   \\text{UCL} = \\bar{\\bar{X}} + A_2 \\bar{R}\n\\]\nand\n\\[\n   \\text{LCL} = \\bar{\\bar{X}} - A_2 \\bar{R}\n\\]\nwhere \\(\\bar{\\bar{X}}\\) is the mean of sample means, and \\(\\bar{R}\\) is the average range.\n\n\n\\(d_3\\): A constant related to the standard deviation of the range distribution, useful for control limits on range charts.\n\\(D_3\\): Used for the Lower Control Limit (LCL) on the R-chart:\n\n\n\\[\n   \\text{LCL} = D_3 \\times \\bar{R}\n\\]\nFor small sample sizes, \\(D_3\\) is often zero, indicating no lower control limit.\n\n\n\\(D_4\\): Used for the Upper Control Limit (UCL) on the R-chart:\n\n\n\\[\n   \\text{UCL} = D_4 \\times \\bar{R}\n\\]\nAdjusts the upper limit based on sample size, providing a threshold for large ranges."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-4",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\n\nGiven \\(\\bar{X} = 3.4995\\), \\(\\bar{R} = 0.0253\\), \\(n = 5\\),\n\\[\n\\text{UCL} = \\bar{X} + A_2 \\bar{R} = 3.4995 + 0.577(0.0253) = 3.514\n\\]\n\\[\n\\text{CL} = \\bar{X} = 3.4995\n\\]\n\\[\n\\text{LCL} = \\bar{X} - A_2 \\bar{R} = 3.4995 - 0.577(0.0253) = 3.485\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-5",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\n\n\n\n\n\n\n\n\n\n\n\nGraph showing the \\(\\bar{X}\\) chart with control limits at 3.485 and 3.514.\nConclusion: Process mean was in control in each time period."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-6",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#barx-chart-process-mean-and-standard-deviation-unknown-6",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown",
    "text": "\\(\\bar{X}\\) Chart: Process Mean and Standard Deviation Unknown\n\nIn case the process is out-of-control for some samples, exclude these samples and re-calculate the CL, UCL, and LCL to re-check the process.\nThis approach applies to other types of control charts as well."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(R\\) Chart for Process Variability",
    "text": "\\(R\\) Chart for Process Variability\n\nAssume the in-control process mean and standard deviation are unknown.\nBecause the control limits for the \\(\\bar{X}\\) chart depend on the value of the average range (\\(\\bar{R}\\)), these limits will not have much meaning unless the process range is in control.\nIn practice, the \\(R\\) chart is usually constructed before the \\(\\bar{X}\\) chart.\n\nIf the \\(R\\) chart indicates that the process variability is in control, then the \\(\\bar{R}\\) is used when constructing the \\(\\bar{X}\\) chart.\n\nThe plotting statistic is the Sample Range, \\(R_j\\)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(R\\) Chart for Process Variability",
    "text": "\\(R\\) Chart for Process Variability\nFor each \\(R_j\\),\n\\[\n\\text{UCL} = D_4 \\bar{R}\n\\]\n\\[\n\\text{CL} = \\bar{R}\n\\]\n\\[\n\\text{LCL} = D_3 \\bar{R}\n\\]\nwhere:\n\n\\(\\bar{R}\\) = average range\n\\(D_3, D_4\\) = constants that depend on \\(n\\); refer to the “Factors for Control Charts” table."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(R\\) Chart for Process Variability",
    "text": "\\(R\\) Chart for Process Variability\nExample: Jensen Computer Supplies Problem\nGiven: - \\(\\bar{\\bar{X}} = 3.4995\\), \\(\\bar{R} = 0.0253\\), \\(n = 5\\)\nCalculations:\n\\[\n\\text{UCL} = D_4 \\bar{R} = 2.114(0.0253) = 0.053\n\\]\n\\[\n\\text{CL} = \\bar{R} = 0.0253\n\\]\n\\[\n\\text{LCL} = D_3 \\bar{R} = 0(0.0253) = 0\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-4",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#r-chart-for-process-variability-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(R\\) Chart for Process Variability",
    "text": "\\(R\\) Chart for Process Variability\n\n\n\n\n\n\n\n\n\n\n\nConclusion: The process variability was in control in each sample."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#attributes-control-charts-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#attributes-control-charts-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Attributes Control Charts",
    "text": "Attributes Control Charts\nAn attributes control chart is used if the quality of the output is measured in terms of discrete/counting data such as the number of defective units (\\(X\\)) in a sample, etc.\n\n\\(X\\) is assumed to follow a Binomial distribution with a defective rate (proportion of “success”), \\(p\\). The expected value of \\(X\\) is \\(np\\).\n\\(p\\) Chart: Used to monitor \\(p\\). The monitoring statistic at time \\(j\\) is the sample proportion \\(\\overline{p_j}\\).\n\\(np\\) Chart: Used to monitor the expected number of defectives in a sample of size \\(n\\), \\(np\\). The monitoring statistic at time \\(j\\) is \\(X_j\\)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\) Chart for Process Proportion",
    "text": "\\(p\\) Chart for Process Proportion\nAssume the in-control process proportion is known as \\(p_0\\).\nFor each \\(\\bar{p}_j\\), \\[\n\\text{UCL} = p_0 + 3\\sigma_{\\overline{p}}\n\\]\n\\[\n\\text{CL} = p_0\n\\]\n\\[\n\\text{LCL} = p_0 - 3\\sigma_{\\overline{p}}\n\\]\nwhere\n\\[\n\\sigma_{\\overline{p_j}} \\approx \\sqrt{\\frac{p_0(1 - p_0)}{n}}\n\\]\nassuming \\(np_0 \\geq 5\\) and \\(n(1 - p_0) \\geq 5\\).\n\nNote: If computed LCL is negative, set LCL = 0."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\) Chart for Process Proportion",
    "text": "\\(p\\) Chart for Process Proportion\nAssume the in-control process proportion is unknown.\nExample: Automated mail-sorting process.\nThe automated mail sorting process in a post office scans the zip codes on the letters and diverts them to the proper carrier route. Suppose a sample of 200 letters is selected each hour for 24 hours to establish control limits for a p chart, where \\(p\\) is the proportion of incorrectly sorted letters."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\) Chart for Process Proportion",
    "text": "\\(p\\) Chart for Process Proportion\n\nSample Data\n\n\n\n\n\n\n\n\n\nSample\nSample Size\nNumber of Incorrects (\\(X_j\\))\nSample Proportion (\\(\\bar{P}_j\\))\n\n\n\n\n1\n200\n6\n0.03\n\n\n2\n200\n2\n0.01\n\n\n3\n200\n0\n0.00\n\n\n…\n…\n…\n…\n\n\n22\n200\n2\n0.01\n\n\n23\n200\n0\n0.00\n\n\n24\n200\n2\n0.01\n\n\nTotal\n4800\n104\n\n\n\n\n\nSample: Identifies each sample in the sequence.\nSample Size (\\(n\\)): Total number of observations in each sample. Here, each sample size is 200.\nNumber of Incorrects (\\(X_j\\)): Number of incorrect observations in each sample.\nSample Proportion (\\(\\bar{P}_j\\)): Proportion of incorrect observations in each sample, calculated as \\(X_j / n\\).\n\n\n\\[\n\\text{Estimated overall defective rate} = \\overline{p} = \\frac{\\text{Total number of incorrects}}{\\text{Total sample size}} = \\frac{104}{4800} = 0.0217\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-4",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\) Chart for Process Proportion",
    "text": "\\(p\\) Chart for Process Proportion\nExample: Automated mail-sorting process\n\\(\\overline{p} = 0.0217\\)\n\\[\n\\sigma_{\\overline{p_j}} \\approx \\sqrt{\\frac{\\overline{p}(1 - \\overline{p})}{n}} = \\sqrt{\\frac{0.0217(1 - 0.0217)}{200}} = 0.0103\n\\]\n\nUCL = \\(\\overline{p} + 3\\sigma_{\\overline{p}} = 0.0217 + 3(0.0103) = 0.0526\\)\nCL = \\(\\overline{p} = 0.0217\\)\nLCL = \\(\\overline{p} - 3\\sigma_{\\overline{p}} = 0.0217 - 3(0.0103) = -0.0092 \\rightarrow 0\\)"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-5",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#p-chart-for-process-proportion-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(p\\) Chart for Process Proportion",
    "text": "\\(p\\) Chart for Process Proportion\n\nThe process is out-of-control in Hour 8."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#np-chart-for-expected-number-of-defectives-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#np-chart-for-expected-number-of-defectives-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "\\(np\\) Chart for Expected Number of Defectives",
    "text": "\\(np\\) Chart for Expected Number of Defectives\nAn \\(np\\) chart is a control chart developed for the number of defective items in a sample. In this case, \\(n\\) is the sample size and \\(p\\) is the probability of observing a defective item when the process is in control.\nFor each \\(X_j\\), \\[\nUCL = np + 3\\sqrt{np(1 - \\overline{p})}\n\\]\n\\[\nCL = np\n\\]\n\\[\nLCL = np - 3\\sqrt{np(1 - \\overline{p})}\n\\]\nassuming \\(np \\geq 5\\) and \\(n(1 - \\overline{p}) \\geq 5\\).\nNote: If computed LCL is negative, set LCL = 0."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#interpretation-of-control-charts",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#interpretation-of-control-charts",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Interpretation of Control Charts",
    "text": "Interpretation of Control Charts\n\nThe location and pattern of points in a control chart enable us to determine, with a small probability of error, whether a process is in statistical control.\nA primary indication that a process may be out of control is a data point outside the control limits.\nCertain patterns of points within the control limits can be warning signals of quality problems (Nelson’s 8 rules):\n\nA large number of points on one side of the center line (9 points).\nSix or seven points in a row that indicate either an increasing or decreasing trend."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Acceptance Sampling",
    "text": "Acceptance Sampling\n\nAcceptance sampling is a statistical method that enables us to base the accept-reject decision on the results of inspection of a sample of items from the lot.\nThe items of interest can be incoming shipments of raw materials or purchased parts as well as finished goods from final assembly (End-of-line inspection).\nAcceptance sampling has advantages over 100% inspection including:\n\nUsually less expensive.\nLess product damage due to less handling.\nFewer inspectors required.\nProvides the only approach possible if destructive testing must be used."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-procedure",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-procedure",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Acceptance Sampling Procedure",
    "text": "Acceptance Sampling Procedure"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-vs-ht",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-vs-ht",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Acceptance Sampling vs HT",
    "text": "Acceptance Sampling vs HT\n\n\nAcceptance sampling is based on hypothesis-testing methodology.\n\n\\[\nH_0: \\text{Good-quality lot}\n\\] \\[\nH_a: \\text{Poor-quality lot}\n\\]\n\n\n\n\n\n\n\n\nDecision/State of the Lot\n\\(H_0\\) True Good-Quality Lot\n\\(H_0\\) False Poor-Quality Lot\n\n\nAccept the Lot\nCorrect decision\nType II error (accepting a poor-quality lot)\n\n\nReject the Lot\nType I error (rejecting a good-quality lot)\nCorrect decision"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#recall-binomial-distribution",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#recall-binomial-distribution",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Recall: Binomial Distribution",
    "text": "Recall: Binomial Distribution\n\n\n\n\n\n\n\nBinomial Probability Function for Acceptance Sampling\n\n\n\n\\[\nX \\sim \\text{Binomial}(n, p)\n\\]\n\\[\nP(X = x) = f(x) = \\frac{n!}{x!(n - x)!} p^x (1 - p)^{(n - x)}\n\\]\n\\[\nP(X \\leq x) = f(0) + \\dots + f(x)\n\\]\nwhere\n\n\\(n\\) = the sample size\n\\(p\\) = the proportion of defective items in the lot\n\\(x\\) = the number of defective items in the sample \\(n\\)\n\\(f(x)\\) = the probability of \\(x\\) defective items in the sample\nExcel Formula: =BINOM.DIST(# successes trials, # trials, probability of success, TRUE)"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-example-kali-inc.",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-example-kali-inc.",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Acceptance Sampling: Example KALI, Inc.",
    "text": "Acceptance Sampling: Example KALI, Inc.\nKALI, Inc. manufactures home appliances that are marketed under a variety of trade names. It uses acceptance sampling plan to monitor the quality of overload protectors. Suppose a sample of 15 items is selected from each incoming shipment or lot and the policy is to accept a lot if at most one defective item is found in the sample.\n\nFind the probability of accepting the lot when the defective rate of the incoming lot is 5%."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-example-kali-inc.-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#acceptance-sampling-example-kali-inc.-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Acceptance Sampling: Example KALI, Inc.",
    "text": "Acceptance Sampling: Example KALI, Inc.\n\nA simple lot acceptance sampling plan is determined by \\((n, c)\\), where \\(n\\) is the sample size, and \\(c\\) is the acceptance number such that if the number of defectives in the sample is less than or equal to \\(c\\) (i.e., \\(X \\leq c\\)), the entire lot is accepted.\n\\[\nn = 15, \\quad c = 1.\n\\]\nWhen the defective rate of the incoming lot is 5%, \\(p = 0.05\\), the probability of accepting the lot is:\n\\[\n\\begin{align*}\nP(\\text{Accept Lot}) & = P(X \\leq 1) = f(0) + f(1)\\\\\n& = \\frac{15!}{0!(15 - 0)!} \\cdot 0.05^0 (1 - 0.05)^{15 - 0} + \\frac{15!}{1!(15 - 1)!} \\cdot 0.05^1 (1 - 0.05)^{15 - 1}\\\\\n& = 0.4633 + 0.3658 \\\\\n& = 0.8290\n\\end{align*}\n\\]\nFor its turn, the the probability of rejecting the lot is:\n\\[\nP(\\text{Reject Lot}) = 1 - 0.8290 = 0.1710\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\nIn formulating a plan, managers must consider the following:\n\nProducer’s risk: Probability of rejecting a lot with an acceptable defective rate.\nConsumer’s risk: Probability of accepting a lot with an unacceptable defective rate."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\n\n\n\n\n\np\nAcceptance Probability\n\n\n\n\n0.00\n1.0000\n\n\n0.05\n0.8290\n\n\n0.10\n0.5490\n\n\n0.15\n0.3186\n\n\n0.20\n0.1671\n\n\n0.25\n0.0802\n\n\n0.30\n0.0353\n\n\n0.35\n0.0142\n\n\n0.40\n0.0052\n\n\n0.45\n0.0017\n\n\n0.50\n0.0005\n\n\n0.55\n0.0001\n\n\n0.60\n0.0000\n\n\n0.65\n0.0000\n\n\n0.70\n0.0000\n\n\n0.75\n0.0000\n\n\n0.80\n0.0000\n\n\n0.85\n0.0000\n\n\n0.90\n0.0000\n\n\n0.95\n0.0000\n\n\n1.00\n0.0000\n\n\n\n\n\nAcceptance Probability Plot\n\n\n\n\n\n\n\n\n\nThe plot presents the probability of accepting the lot (y-axis) based on the percent of defective items in the lot (x-axis)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-2",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\nIn formulating a plan, managers must specify the following:\n\\[\n\\begin{cases}\np_0 = \\text{Maximum acceptable defective rate (under $H_0$)} \\\\\n\\alpha = \\text{The Producer's Risk: }\\text{the probability that a lot with defective rate $p_0$ will be rejected} \\\\\n\\quad\n\\end{cases}\n\\]\n\\[\n\\begin{cases}\np_1 = \\text{Minimum unacceptable defective rate (under $H_a$)} \\\\\n\\beta = \\text{The Consumer's Risk: } \\text{the probability that a lot with defective rate $p_1$ will be accepted} \\\\\n\\quad\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-3",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-3",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\nA simple sampling plan with \\((n, c)\\) is selected when it comes close to meeting both the \\(\\alpha\\) and \\(\\beta\\) requirements specified.\nExample: For the purpose of illustration, suppose we require:\n\nIf \\(p_0 = 0.03\\), \\(\\alpha = \\Pr(\\text{Reject the lot} \\mid p_0 = 0.03) = 8\\%\\) (Producer’s Risk)\nIf \\(p_1 = 0.15\\), \\(\\beta = \\Pr(\\text{Accept the lot} \\mid p_1 = 0.15) = 4\\%\\) (Consumer’s Risk)\n\n\n\n\nIs \\((n, c) = (15, 0)\\) an acceptable plan?"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-4",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-4",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\n\nIs \\((n, c) = (15, 0)\\) an acceptable plan?\n\nFor \\((n, c) = (15, 0)\\):\n\\[\n\\begin{align*}\n\\text{Producer's risk} & = P(\\text{Reject the lot} \\mid p = 0.03) = P(X &gt; 0 \\mid p = 0.03) \\\\\n& = 1 - P(X \\leq 0 \\mid p = 0.03) \\\\\n&  = 1 - \\text{Bin}(15, 0.03) = 1 - 0.633 = 0.367 \\quad \\text{(Too high)}\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\text{Consumer's risk} & = P(\\text{Accept the lot} \\mid p = 0.15) = P(X \\leq 0 \\mid p = 0.15) \\\\\n& = \\text{Bin}(15, 0.15) = 0.087 \\quad \\text{(Close)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-5",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#selecting-an-acceptance-sampling-plan-example-kali-inc.-5",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.",
    "text": "Selecting an Acceptance Sampling Plan: Example KALI, Inc.\n\n\n\nIs \\((n, c) = (15, 0)\\) an acceptable plan?\n\n\n\nProducer’s Risk: The calculated producer’s risk is 36.7%, which is significantly higher than the acceptable level of 8%. This means there is a high probability that a lot with an acceptable defect rate (\\(p_0 = 0.03\\)) will be incorrectly rejected, resulting in an unacceptably high risk for the producer.\nConsumer’s Risk: The consumer’s risk is 8.7%, which is close to the acceptable level of 4%. This indicates that the plan is nearly adequate in terms of the consumer’s risk, but it is still slightly above the target.\nConclusion: The acceptance sampling plan \\((n, c) = (15, 0)\\) is not acceptable because it does not adequately meet the producer’s risk requirement. The producer’s risk is too high, resulting in a high probability of rejecting acceptable lots.\nRecommendation: Adjustments to the sampling plan are needed to better balance both the producer’s and consumer’s risk requirements. This may involve increasing the acceptance number \\(c\\) or adjusting the sample size \\(n\\)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#multiple-sampling-plans-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#multiple-sampling-plans-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Sampling Plans",
    "text": "Multiple Sampling Plans\n\nA multiple sampling plan uses two or more stages of sampling.\nAt each stage, decisions include:\n\nStopping and accepting the lot,\nStopping and rejecting the lot,\nContinuing sampling.\n\nMultiple sampling plans can reduce total sample size while maintaining Type I and Type II error levels."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#multiple-sampling-plans-two-stage-acceptance-sampling-plan-procedure",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#multiple-sampling-plans-two-stage-acceptance-sampling-plan-procedure",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Multiple Sampling Plans: Two Stage Acceptance Sampling Plan Procedure",
    "text": "Multiple Sampling Plans: Two Stage Acceptance Sampling Plan Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStage 1:\n\nTake an initial sample of \\(n_1\\) items.\nFind the number of defectives \\(x_1\\) in this sample.\nDecision criteria:\n\nIf \\(x_1 \\leq c_1\\): Accept the lot.\nIf \\(x_1 \\geq c_2\\): Reject the lot.\nIf \\(c_1 &lt; x_1 &lt; c_2\\): Proceed to Stage 2.\n\n\nStage 2:\n\nTake an additional sample of \\(n_2\\) items.\nFind the number of defectives \\(x_2\\) in this second sample.\nDecision criteria:\n\nIf \\(x_1 + x_2 \\leq c_3\\): Accept the lot.\nIf \\(x_1 + x_2 &gt; c_3\\): Reject the lot.\n\n\nAcceptance Criteria: The lot is accepted if the defectives in Stage 1 are within \\(c_1\\), or if the combined defectives in both stages are within \\(c_3\\).\nRejection Criteria: The lot is rejected if the defectives in Stage 1 exceed \\(c_2\\), or if the combined defectives in both stages exceed \\(c_3\\)."
  },
  {
    "objectID": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#summary-1",
    "href": "lecture_slides/19_chapter_quality_control/19_chapter_quality_control.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nImportance of Quality Control: Quality control is essential for ensuring products meet customer expectations and can enhance a company’s competitiveness. Tools like control charts, SPC, and acceptance sampling are critical.\nStatistical Process Control (SPC): SPC uses control charts to monitor and maintain process stability by identifying common and assignable causes of variation.\nControl Charts: Used to monitor different quality metrics, such as the mean (\\(\\bar{X}\\)), variability (R-chart), and defect proportion (p-chart). Control limits are established to detect out-of-control conditions and require adjustments only for assignable causes.\nAcceptance Sampling: Provides a practical method for lot quality decisions without inspecting every item. It balances producer’s and consumer’s risks by setting limits on acceptable defect rates.\nMultiple Sampling Plans: These plans, including the two-stage acceptance sampling plan, can minimize sample size while maintaining error controls, improving efficiency over single-stage plans."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#overview",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#overview",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nProblem formulation.\nChance Events and States of Nature.\nPayoff tables.\nDecision trees.\n\n\n\nDecision Making with Probabilities.\n\nExpected Value Approach\nExpected Value of Perfect Information (EVPI)\nExpected Value with Sample Information (EVSI)\n\nBayes’ theorem to compute branch probabilities for decision trees."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#problem-formulation",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#problem-formulation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Problem Formulation",
    "text": "Problem Formulation\n\nThe first step in the decision analysis process is problem formulation.\n\nIt can be a verbal statement of the problem. Then, we identify the decision alternatives, the uncertain future events, referred to as chance events, and the consequences associated with each decision alternative and each chance event outcome."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-pittsburgh-development-corporation-pdc",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-pittsburgh-development-corporation-pdc",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Pittsburgh Development Corporation (PDC)",
    "text": "Example: Pittsburgh Development Corporation (PDC)\n\nPittsburgh Development Corporation (PDC) has purchased land to develop a luxury condominium complex with individual units priced between $300,000 and $1,400,000.\n\nDecision Problem: PDC’s primary decision challenge is to determine the optimal size of the condominium project that will maximize profits, considering the uncertainty of future demand for these units.\nProject Options: PDC has commissioned architectural plans for three potential project sizes:\n\nSmall Complex: 30 condominiums\nMedium Complex: 60 condominiums\nLarge Complex: 90 condominiums\n\nKey Considerations: The financial viability and success of the project depend on:\n\nThe selected project size (small, medium, or large)\nFuture demand for the condominiums, an uncertain factor influencing profitability\n\n\n\nPDC must analyze both the scale of the project and demand uncertainty to make an informed, profitable decision."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-pdcs-decision-alternatives",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-pdcs-decision-alternatives",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: PDC’s Decision Alternatives",
    "text": "Example: PDC’s Decision Alternatives\nIt is clear that the decision is to select the best size for the condominium complex.\n\nPDC has the following three decision alternatives:\n\\[\nd_1 = \\text{a small complex with 30 condominiums}\n\\]\n\\[\nd_2 = \\text{a medium complex with 60 condominiums}\n\\]\n\\[\nd_3 = \\text{a large complex with 90 condominiums}\n\\]"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#chance-events-and-states-of-nature-in-decision-analysis",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#chance-events-and-states-of-nature-in-decision-analysis",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Chance Events and States of Nature in Decision Analysis",
    "text": "Chance Events and States of Nature in Decision Analysis\nIn decision analysis, uncertain future events like demand levels are known as chance events.\nThe possible outcomes of these events are called states of nature, with only one state ultimately occurring."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-states-of-nature-for-the-pdc-project",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-states-of-nature-for-the-pdc-project",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: States of Nature for the PDC Project",
    "text": "Example: States of Nature for the PDC Project\n\nA key factor in PDC’s decision-making process is the uncertainty surrounding demand for the condominiums.\n\nFor PDC’s condominium project, the two relevant states of nature regarding demand are:\n\\[\ns_1 = \\text{Strong demand for the condominiums}\n\\]\n\\[\ns_2 = \\text{Weak demand for the condominiums}\n\\]\nBy defining these states of nature, PDC can structure its decision-making process to account for demand uncertainty and evaluate the impact of each demand scenario on project profitability."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-sequence-of-decisions-and-consequences",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-sequence-of-decisions-and-consequences",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Sequence of Decisions and Consequences",
    "text": "Example: Sequence of Decisions and Consequences\nThe decision analysis process follows a structured sequence to ensure optimal decision-making:\n\nSelect a Decision Alternative: Management begins by choosing a project size (complex size) from the available options (small, medium, or large complex).\nState of Nature Occurs: After the decision, an uncertain demand scenario—known as the state of nature—will materialize. In this case, it could be: Strong Demand or Weak Demand.\nObserve the Consequence: Based on the chosen project size and the actual demand, a consequence will occur. For PDC, the consequence is represented as profit or loss from the project.\n\n\nBy following this sequence, PDC can better understand potential outcomes and make informed decisions to maximize profitability."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#payoff-tables-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#payoff-tables-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Payoff Tables",
    "text": "Payoff Tables\nTo determine the optimal complex size, PDC must evaluate the consequence of each decision alternative under different demand scenarios. For the purpose of choosing the best Payoff, we use Payoff Tables.\n\nKey Concepts\nPayoff: The result (profit or loss) from a specific combination of a decision alternative (complex size) and a state of nature (demand).\nPayoff Table: A table that organizes payoffs for all possible combinations of decision alternatives and states of nature, providing a structured view of potential outcomes.\n\n\nBy analyzing the payoff table, PDC can make an informed decision that aligns with their profitability goals.\n\n\nGiven the three decision alternatives and the two states of nature, which complex size should PDC choose?"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-payoff-table",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-payoff-table",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Payoff Table",
    "text": "Example: Payoff Table\n\nThe payoff table will show projected profits (in millions of dollars) for each combination of decision alternative (complex size) and state of nature (demand).\n\n\n\n\n\n\n\n\nDecision Alternative / State of Nature\nStrong Demand(\\(s_1\\))\nWeak Demand(\\(s_2\\))\n\n\nSmall complex (\\(d_1\\))\n8\n7\n\n\nMedium complex (\\(d_2\\))\n14\n5\n\n\nLarge complex (\\(d_3\\))\n20\n-9"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-interpreting-payoffs",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-interpreting-payoffs",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Interpreting Payoffs",
    "text": "Example: Interpreting Payoffs\nSince PDC’s goal is to maximize profit, profit is used as the payoff measure in this case.\n\nNotation: We use \\(V_{ij}\\) to denote the payoff for decision alternative \\(i\\) under state of nature \\(j\\).\n\n\nFrom the Payoff Table:\n\n\\(V_{31} = 20\\): A payoff of $20 million occurs if PDC builds a large complex (\\(d_3\\)) and demand is strong (\\(s_1\\)).\n\\(V_{32} = -9\\): A loss of $9 million occurs if PDC builds a large complex (\\(d_3\\)) and demand is weak (\\(s_2\\)).\n\n\n\nThese payoff values help PDC evaluate the potential financial outcomes of each decision under different demand scenarios, guiding them toward the most profitable choice."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-trees-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-trees-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Trees",
    "text": "Decision Trees\n\n\n\nA decision tree graphically shows the sequential nature of the decision-making process.\n\nThe Figure presents a decision tree for the PDC problem, demonstrating the natural or logical progression that will occur over time.\n\nFirst, PDC must make a decision regarding the size of the condominium complex (\\(d_1\\), \\(d_2\\), or \\(d_3\\)).\nThen, after the decision is implemented, either state of nature \\(s_1\\) or \\(s_2\\) will occur.\n\n\n\nThe number at each end point of the tree indicates the payoff associated with a particular sequence. For example:\n\nThe topmost payoff of 8 indicates an $8 million profit if PDC constructs a small condominium complex (\\(d_1\\)) and demand is strong (\\(s_1\\)).\nThe next payoff of 7 indicates an anticipated profit of $7 million if PDC constructs a small condominium complex (\\(d_1\\)) and demand is weak (\\(s_2\\)).\n\n\n\nThus, the decision tree shows the sequences of decision alternatives and states of nature, providing the six possible payoffs."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#explanation-of-the-decision-tree",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#explanation-of-the-decision-tree",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Explanation of the Decision Tree",
    "text": "Explanation of the Decision Tree\n\n\n\nThe decision tree has four nodes, numbered 1–4, representing decisions and chance events:\n\nDecision nodes (squares): Represent points where a decision must be made. Node 1 is the decision node.\nChance nodes (circles): Represent points where the outcome depends on chance. Nodes 2, 3, and 4 are chance nodes.\nBranches: Branches leaving the decision node correspond to decision alternatives (small, medium, or large complex).\nBranches: Branches leaving each chance node correspond to the states of nature (strong or weak demand).\nPayoffs: are shown at the end of each branch.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can the decision maker use the information in the payoff table or the decision tree to select the best decision alternative?"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-making-with-probabilities-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-making-with-probabilities-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Making with Probabilities",
    "text": "Decision Making with Probabilities\n\nWith the decision alternatives and the states of nature established, our next step is to determine the probabilities for each state of nature.\nThere are several methods to assign probabilities:\n\nClassical method: Based on logical analysis, typically used when outcomes are equally likely.\nRelative frequency method: Uses historical or empirical data to estimate probabilities.\nSubjective method: Relies on expert judgment or intuition when data is scarce or unavailable.\n\n\nWith these probabilities in hand, we apply the expected value approach to calculate the expected payoff for each decision alternative by weighting possible outcomes according to their probabilities.\n\n\nThe decision alternative with the highest expected value is then identified as the best or recommended choice for the problem."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-approach",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-approach",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value Approach",
    "text": "Expected Value Approach\n\nThe probabilities for the states of nature must satisfy the basic requirements for assigning probabilities.\nWe begin by defining the expected value of a decision alternative. Let:\n\n\\(N\\) = the number of states of nature\n\\(P(s_j)\\) = the probability of state of nature \\(s_j\\)\n\n\nBecause one and only one of the \\(N\\) states of nature can occur, the probabilities must satisfy two conditions:\n\nNon-negativity: \\[\nP(s_j) \\geq 0 \\quad \\text{for all states of nature}\n\\]\nTotal probability: \\[\n\\sum_{j=1}^{N} P(s_j) = P(s_1) + P(s_2) + \\cdots + P(s_N) = 1\n\\]"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-calculation",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-calculation",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value Calculation",
    "text": "Expected Value Calculation\nThe expected value (EV) of decision alternative \\(d_i\\) is as follows:\n\\[\n\\text{EV}(d_i) = \\sum_{j=1}^{N} P(s_j) V_{ij}\n\\]\nwhere:\n\n\\(V_{ij}\\) = the value of the payoff for decision alternative \\(d_i\\) and state of nature \\(s_j\\).\n\n\nIn words, the expected value of a decision alternative is the sum of weighted payoffs for the decision alternative. The weight for a payoff is the probability of the associated state of nature and therefore the probability that the payoff will occur."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-calculation-of-expected-value",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-calculation-of-expected-value",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Calculation of Expected Value",
    "text": "Example: Calculation of Expected Value\n\nPDC is optimistic about the potential demand for its luxury high-rise condominium complex. Based on this optimism, PDC assigns subjective probabilities to represent the likelihood of each demand scenario:\n\nProbability of strong demand (\\(s_1\\)): \\(P(s_1) = 0.8\\)\nProbability of weak demand (\\(s_2\\)): \\(P(s_2) = 0.2\\)\n\n\nUsing these probabilities and the associated payoffs, we calculate the expected value (EV) for each decision alternative:\n\nSmall complex (\\(d_1\\)): \\[\n\\text{EV}(d_1) = 0.8(8) + 0.2(7) = 7.8\n\\]\nMedium complex (\\(d_2\\)): \\[\n\\text{EV}(d_2) = 0.8(14) + 0.2(5) = 12.2\n\\]\nLarge complex (\\(d_3\\)): \\[\n\\text{EV}(d_3) = 0.8(20) + 0.2(-9) = 14.2\n\\]\n\n\n\nBy comparing the expected values, we find that the large complex (\\(d_3\\)) yields the highest expected value of $14.2 million. Therefore, according to the expected value approach, the large complex is the recommended decision."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#applying-the-expected-value-approach-using-decision-trees",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#applying-the-expected-value-approach-using-decision-trees",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Applying the Expected Value Approach Using Decision Trees",
    "text": "Applying the Expected Value Approach Using Decision Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe calculations required to identify the decision alternative with the best expected value can be conveniently carried out on a decision tree.\nWorking backward through the decision tree, we first compute the expected value at each chance node; that is, at each chance node, we weight each possible payoff by its probability of occurrence. By doing so, we obtain the expected values for nodes 2, 3, and 4."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-of-expected-value-approach-using-decision-trees",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-of-expected-value-approach-using-decision-trees",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary of Expected Value Approach Using Decision Trees",
    "text": "Summary of Expected Value Approach Using Decision Trees\n\nThe decision maker controls the choice at each decision node aiming to maximize the expected profit. At decision node 1, the expected value approach identifies the best decision alternative as:\n\nDecision \\(d_3\\): Constructing the large condominium complex, which yields an expected value of $14.2 million.\n\n\nThis recommendation of \\(d_3\\) is consistent with both the decision tree analysis and the expected value calculations derived from the payoff table.\nUsing the expected value approach in conjunction with the decision tree provides a clear and structured framework for selecting the optimal decision based on projected profits."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-perfect-information-evpi-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-perfect-information-evpi-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value of Perfect Information (EVPI)",
    "text": "Expected Value of Perfect Information (EVPI)\n\n\nWhat if PDC has the opportunity to conduct a market research study to gauge buyer interest in the condominium project?\n\n\nThis study could provide valuable information to refine the probability assessments for the different states of nature (e.g., strong or weak demand).\n\n\nTo understand the potential value of this information, we assume the study could yield perfect information — meaning PDC would know, with certainty, which state of nature will occur before making a decision.\n\n\nGiven this perfect information, we can develop an optimal decision strategy.\n\n\nWe can quantify how much PDC should be willing to pay for this perfect information by calculating the Expected Value of Perfect Information (EVPI)."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#payoff-table-for-the-pdc-condominium-project-millions",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#payoff-table-for-the-pdc-condominium-project-millions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Payoff Table for the PDC Condominium Project ($ Millions)",
    "text": "Payoff Table for the PDC Condominium Project ($ Millions)\n\n\n\n\n\n\n\n\n\nDecision Alternative\nStrong Demand \\(s_1\\)\nWeak Demand \\(s_2\\)\n\n\n\n\nSmall complex, \\(d_1\\)\n8\n7\n\n\nMedium complex, \\(d_2\\)\n14\n5\n\n\nLarge complex, \\(d_3\\)\n20\n-9"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy-with-perfect-information",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy-with-perfect-information",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Strategy with Perfect Information",
    "text": "Decision Strategy with Perfect Information\n\nWith perfect information about the states of nature, PDC can tailor its decision strategy to maximize profits based on certainty about demand:\n\nIf PDC knows with certainty that strong demand (\\(s_1\\)) will occur:\n\n\nThe optimal decision is to select the large complex (\\(d_3\\)), resulting in a payoff of $20 million.\n\n\nIf PDC knows with certainty that weak demand (\\(s_2\\)) will occur:\n\n\nThe optimal decision is to select the small complex (\\(d_1\\)), resulting in a payoff of $7 million.\n\n\nThis decision strategy demonstrates the value of perfect information by showing how it would enable PDC to make the most profitable choice in each scenario.\n\n\n\nWhat is the expected value for this decision strategy?"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-this-decision-strategy",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-this-decision-strategy",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value of this Decision Strategy",
    "text": "Expected Value of this Decision Strategy\n\nWhat is the expected value for this decision strategy?\n\n\nTo compute the expected value with perfect information, we return to the original probabilities for the states of nature:\n\n\\(P(s_1) = 0.8\\)\n\\(P(s_2) = 0.2\\)\n\n\n\nSo, the expected value of the decision strategy based on perfect information is:\n\\[\n0.8(20) + 0.2(7) = 17.4\n\\]\nWe refer to the expected value of $17.4 million as the expected value with perfect information (EVwPI)."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-perfect-information-evpi-2",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-perfect-information-evpi-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value of Perfect Information (EVPI)",
    "text": "Expected Value of Perfect Information (EVPI)\nIn general, the expected value of perfect information (EVPI) is computed as follows:\n\\[\n\\text{EVPI} = | \\text{EVwPI} - \\text{EVwoPI} |\n\\]\nwhere:\n\nEVPI = expected value of perfect information\nEVwPI = expected value with perfect information about the states of nature\nEVwoPI = expected value without perfect information about the states of nature\n\n\n\nNote the role of the absolute value. For minimization problems, information helps reduce or lower cost; thus, the expected value with perfect information is less than or equal to the expected value without perfect information. In this case, EVPI is the magnitude of the difference between EVwPI and EVwoPI."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-expected-value-of-perfect-information-evpi",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-expected-value-of-perfect-information-evpi",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Expected Value of Perfect Information (EVPI)",
    "text": "Example: Expected Value of Perfect Information (EVPI)\nThe expected value with perfect information is $17.4 million, and the expected value without perfect information (EVwoPI) is $14.2 million. Therefore, the expected value of perfect information (EVPI) is:\n\\[\n\\text{EVPI} = 17.4 - 14.2 = 3.2\n\\]\n\nIn other words, $3.2 million represents the additional expected value that can be obtained if perfect information were available about the states of nature.\n\n\nGiven the EVPI of $3.2 million, PDC might seriously consider a market survey as a way to obtain more information about the states of nature."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-analysis-with-sample-information-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-analysis-with-sample-information-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Analysis with Sample Information",
    "text": "Decision Analysis with Sample Information\nAdditional information is often obtained through sample information, which provides insights into the likely states of nature. Common sources of sample information include:\n\nRaw material sampling\nProduct testing\nMarket research studies\n\n\nUsing sample information, decision makers can update their prior probability estimates. These revised probabilities, adjusted based on new data, are referred to as posterior probabilities."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#market-research-study-for-pdc",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#market-research-study-for-pdc",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Market Research Study for PDC",
    "text": "Market Research Study for PDC\nLet’s assume that PDC manager is considering a six-month market research study designed to learn more about potential market acceptance of the PDC condominium project.\nShe anticipates that the market research study will provide one of the following two results:\n\nFavorable report: A significant number of the individuals contacted express interest in purchasing a PDC condominium.\nUnfavorable report: Very few of the individuals contacted express interest in purchasing a PDC condominium."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-tree-with-market-research-study",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-tree-with-market-research-study",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Tree with Market Research Study",
    "text": "Decision Tree with Market Research Study\n\n\nThe decision tree for the PDC problem with sample information shows the logical sequence for the decisions and the chance events.\n\nPDC’s management must decide whether the market research should be conducted.\nIf it is conducted, PDC’s management must be prepared to make a decision about the size of the condominium project if:\n\nthe market research report is favorable or\nthe market research report is unfavorable."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#analysis-of-decision-tree-structure",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#analysis-of-decision-tree-structure",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Analysis of Decision Tree Structure",
    "text": "Analysis of Decision Tree Structure\n\n\n\nIn the decision tree diagram:\n\nSquares represent decision nodes, where PDC actively makes a choice.\nCircles represent chance nodes, where the outcome is determined by probability rather than a decision.\n\n\nKey Nodes in the Decision Tree\n\nDecision Node 1:\n\nPDC must decide whether to conduct a market research study.\n\nChance Node 2:\n\nIf the study is conducted, the outcome—favorable or unfavorable report—is determined by chance, as PDC has no control over this result.\n\nDecision Node 3:\n\nIf the report is favorable, PDC must choose the size of the complex (small, medium, or large) based on this information.\n\nDecision Node 4:\n\nIf the report is unfavorable, PDC again chooses the size of the complex (small, medium, or large), this time with the knowledge of an unfavorable market assessment.\n\nChance Nodes 6 to 14:\n\nThese nodes represent the final demand outcomes (strong or weak) for each decision path. Here, the actual state of nature—strong demand or weak demand—is determined by chance."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#probabilities-for-market-research-study",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#probabilities-for-market-research-study",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Probabilities for Market Research Study",
    "text": "Probabilities for Market Research Study\n\n\n\nPDC developed the following branch probabilities.\n\nIf the market research study is undertaken,\n\\[\nP(\\text{Favorable report}) = P(F) = 0.77\n\\]\n\\[\nP(\\text{Unfavorable report}) = P(U) = 0.23\n\\]\nIf the market research report is favorable,\n\\[\nP(\\text{Strong demand given a favorable report}) = P(s_1 | F) = 0.94\n\\]\n\\[\nP(\\text{Weak demand given a favorable report}) = P(s_2 | F) = 0.06\n\\]\nIf the market research report is unfavorable,\n\\[\nP(\\text{Strong demand given an unfavorable report}) = P(s_1 | U) = 0.35\n\\]\n\\[\nP(\\text{Weak demand given an unfavorable report}) = P(s_2 | U) = 0.65\n\\]\nIf the market research report is not undertaken, the prior probabilities are applicable:\n\\[\nP(\\text{Strong demand}) = P(s_1) = 0.80\n\\]\n\\[\nP(\\text{Weak demand}) = P(s_2) = 0.20\n\\]"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Strategy",
    "text": "Decision Strategy\n\nThe approach used to determine the optimal decision strategy is based on a backward pass through the decision tree using the following steps:\n\nAt chance nodes, compute the expected value by multiplying the payoff at the end of each branch by the corresponding branch probability.\nAt decision nodes, select the decision branch that leads to the best expected value. This expected value becomes the expected value at the decision node."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#decision-strategy-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Decision Strategy",
    "text": "Decision Strategy\n\n\n\n\n\nStarting the backward pass calculations by computing the expected values at chance nodes 6 to 14 provides the following results:\n\n\\[\n\\begin{align*}\n\\text{EV(Node 6)} &= 0.94(8) + 0.06(7) = 7.94 \\\\\n\\text{EV(Node 7)} &= 0.94(14) + 0.06(5) = 13.46 \\\\\n\\text{EV(Node 8)} &= 0.94(20) + 0.06(-9) = 18.26 \\\\\n\\text{EV(Node 9)} &= 0.35(8) + 0.65(7) = 7.35 \\\\\n\\text{EV(Node 10)} &= 0.35(14) + 0.65(5) = 8.15 \\\\\n\\text{EV(Node 11)} &= 0.35(20) + 0.65(-9) = 1.15 \\\\\n\\text{EV(Node 12)} &= 0.80(8) + 0.20(7) = 7.80 \\\\\n\\text{EV(Node 13)} &= 0.80(14) + 0.20(5) = 12.20 \\\\\n\\text{EV(Node 14)} &= 0.80(20) + 0.20(-9) = 14.20 \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#selecting-best-decisions",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#selecting-best-decisions",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting Best Decisions",
    "text": "Selecting Best Decisions\n\n\n\nNext, move to decision nodes 3, 4, and 5. For each of these nodes, select the decision alternative branch that leads to the best expected value.\n\nAt node 3, the choices are:\n\nSmall complex branch: EV(Node 6) = 7.94\nMedium complex branch: EV(Node 7) = 13.46\nLarge complex branch: EV(Node 8) = 18.26\n\n\n\nThe best decision is the large complex, with EV(Node 3) = 18.26.\n\nAt node 4, we select the best expected value from nodes 9, 10, and 11. The best decision is the medium complex, with EV(Node 4) = 8.15.\nFor node 5, we select the best expected value from nodes 12, 13, and 14. The best decision is the large complex, with EV(Node 5) = 14.20.\n\n\n\nPDC Decision Tree After Choosing Best Decisions at Nodes 3, 4, and 5"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#selecting-best-decisions-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#selecting-best-decisions-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Selecting Best Decisions",
    "text": "Selecting Best Decisions\n\n\n\n\nThe expected value at chance node 2 can now be computed as follows:\n\\[\n\\begin{align*}\n\\text{EV(Node 2)} &= 0.77 \\times \\text{EV(Node 3)} + 0.23 \\times \\text{EV(Node 4)} \\\\\n&= 0.77(18.26) + 0.23(8.15) = 15.93\n\\end{align*}\n\\]\nThis calculation reduces the decision tree to one involving only the two decision branches from node 1.\n\nPDC Decision Tree Reduced to Two Decision Branches"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#final-decision-strategy",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#final-decision-strategy",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Final Decision Strategy",
    "text": "Final Decision Strategy\n\n\n\n\nAt decision node 1, PDC can determine the optimal choice by comparing the expected values from nodes 2 and 5.\n\nThe highest expected value is 15.93, which supports the decision to conduct the market research study.\n\n\nThe Optimal Decision Strategy for PDC is to conduct the market research study and then carry out the following decision strategy:\n\nIf the market research is favorable: Construct the large condominium complex to maximize profits (\\(d_3\\)).\nIf the market research is unfavorable: Construct the medium condominium complex as a safer choice under lower demand expectations (\\(d_2\\)).\n\n\n\nPDC Decision Tree After Choosing Best Decisions at Nodes 3, 4, and 5"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-of-decision-tree-analysis",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-of-decision-tree-analysis",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary of Decision Tree Analysis",
    "text": "Summary of Decision Tree Analysis\n\nWork backward through the tree to calculate expected values at each chance node.\nSelect the optimal decision branch at each decision node based on these expected values.\nThe resulting sequence of optimal branches forms the final decision strategy for maximizing expected profit.\n\n\nBy systematically calculating expected values and selecting the best options at each node, PDC arrives at a data-driven decision strategy that adapts based on the information received from the market research study."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-sample-information-formula",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#expected-value-of-sample-information-formula",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Expected Value of Sample Information Formula",
    "text": "Expected Value of Sample Information Formula\n\\[\n\\text{EVSI} = |\\text{EVwSI} - \\text{EVwoSI}|\n\\]\nwhere:\n\nEVSI = expected value of sample information\nEVwSI = expected value with sample information about the states of nature\nEVwoSI = expected value without sample information about the states of nature\n\n\n\nNote the role of the absolute value. For minimization problems, the expected value with sample information is always less than or equal to the expected value without sample information."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-expected-value-of-sample-information-evsi",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#example-expected-value-of-sample-information-evsi",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Example: Expected Value of Sample Information (EVSI)",
    "text": "Example: Expected Value of Sample Information (EVSI)\nThe expected value associated with the market research study is $15.93.\nBefore, we saw that the best expected value if the market research study is not undertaken is $14.20. Thus, we can conclude that the difference is:\n\\[\n\\text{EVSI} = 15.93 - 14.20 = 1.73\n\\]\nthat is the expected value of sample information (EVSI).\nTo conclude, conducting the market research study adds $1.73 million to the PDC expected value."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#computing-branch-probabilities-using-bayes-theorem-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#computing-branch-probabilities-using-bayes-theorem-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Computing Branch Probabilities Using Bayes’ Theorem",
    "text": "Computing Branch Probabilities Using Bayes’ Theorem\n\nThe branch probabilities for the PDC decision tree chance nodes were specified in the problem description, with no computations initially required.\n\n\nPDC developed the following branch probabilities.\n\nIf the market research study is undertaken,\n\\[\nP(\\text{Favorable report}) = P(F) = 0.77\n\\]\n\\[\nP(\\text{Unfavorable report}) = P(U) = 0.23\n\\]\nIf the market research report is favorable,\n\\[\nP(\\text{Strong demand given a favorable report}) = P(s_1 | F) = 0.94\n\\]\n\\[\nP(\\text{Weak demand given a favorable report}) = P(s_2 | F) = 0.06\n\\]\nIf the market research report is unfavorable,\n\\[\nP(\\text{Strong demand given an unfavorable report}) = P(s_1 | U) = 0.35\n\\]\n\\[\nP(\\text{Weak demand given an unfavorable report}) = P(s_2 | U) = 0.65\n\\]\nIf the market research report is not undertaken, the prior probabilities are applicable:\n\\[\nP(\\text{Strong demand}) = P(s_1) = 0.80\n\\]\n\\[\nP(\\text{Weak demand}) = P(s_2) = 0.20\n\\]"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#computing-branch-probabilities-using-bayes-theorem-2",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#computing-branch-probabilities-using-bayes-theorem-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Computing Branch Probabilities Using Bayes’ Theorem",
    "text": "Computing Branch Probabilities Using Bayes’ Theorem\nNow, we will see how Bayes’ theorem can be used to compute branch probabilities for decision trees.\nBayes’ theorem is expressed as:\n\\[\nP(s_j | F) = \\frac{P(F | s_j) \\cdot P(s_j)}{P(F)}\n\\]\nwhere:\n\n\\(P(s_j | F)\\) is the posterior probability of state \\(s_j\\) given a favorable report \\(F\\).\n\\(P(F | s_j)\\) is the conditional probability of receiving a favorable report given state \\(s_j\\).\n\\(P(s_j)\\) is the prior probability of state \\(s_j\\).\n\\(P(F)\\) is the total probability of receiving a favorable report."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#bayes-theorem-in-decision-trees",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#bayes-theorem-in-decision-trees",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Bayes’ Theorem in Decision Trees",
    "text": "Bayes’ Theorem in Decision Trees\n\n\n\nDefinitions\nLet:\n\n\\(F\\): Favorable market research report\n\\(U\\): Unfavorable market research report\n\\(s_1\\): Strong demand (state of nature 1)\n\\(s_2\\): Weak demand (state of nature 2)\n\n\nKey Branch Probabilities\n\nAt Chance Node 2: Determine probabilities of a favorable or unfavorable report: \\(P(F)\\) and \\(P(U)\\).\nAt Chance Nodes 6, 7, and 8: Calculate posterior probabilities for demand given a favorable report:\n\n\\(P(s_1 | F)\\): Probability of strong demand given a favorable report.\n\\(P(s_2 | F)\\): Probability of weak demand given a favorable report.\n\n\n\n\n\n\n\nAt Chance Nodes 9, 10, and 11: Calculate posterior probabilities for demand given an unfavorable report:\n\n\\(P(s_1 | U)\\): Probability of strong demand given an unfavorable report.\n\\(P(s_2 | U)\\): Probability of weak demand given an unfavorable report.\n\nAt Chance Nodes 12, 13, and 14: Use prior probabilities \\(P(s_1)\\) and \\(P(s_2)\\) if the market research study is not conducted.\n\n\n\nThe PDC Decision Tree"
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#conditional-probability-calculations",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#conditional-probability-calculations",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Conditional Probability Calculations",
    "text": "Conditional Probability Calculations\n\nTo perform the probability computations, we need two main sets of probabilities:\n\nPrior Probabilities: PDC’s initial assessment of the likelihood of each state of nature:\n\n\\(P(s_1)\\): Probability of strong demand.\n\\(P(s_2)\\): Probability of weak demand.\n\n\n\n\nConditional Probabilities: Likelihood of specific market research outcomes (sample information) given each state of nature.\n\n\n\nFavorable Report:\n\n\\(P(F | s_1)\\): Probability of favorable report given strong demand.\n\\(P(F | s_2)\\): Probability of favorable report given weak demand.\n\nUnfavorable Report:\n\n\\(P(U | s_1)\\): Probability of unfavorable report given strong demand.\n\\(P(U | s_2)\\): Probability of unfavorable report given weak demand.\n\n\n\n\nThese conditional probabilities allow us to use Bayes’ theorem to update prior probabilities based on market research outcomes, leading to more informed decision-making."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#market-research-probabilities-table",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#market-research-probabilities-table",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Market Research Probabilities Table",
    "text": "Market Research Probabilities Table\n\nWe assume that the following assessments are available for these conditional probabilities. This is the key piece of information that we use to compute the branches probabilities.\n\n\n\n\n\n\n\n\n\nState of Nature / Market Research\nFavorable, \\(F\\)\nUnfavorable, \\(U\\)\n\n\n\n\nStrong demand, \\(s_1\\)\n\\(P(F|s_1) = 0.90\\)\n\\(P(U|s_1) = 0.10\\)\n\n\nWeak demand, \\(s_2\\)\n\\(P(F|s_2) = 0.25\\)\n\\(P(U|s_2) = 0.75\\)\n\n\n\n\nNote: These probability assessments provide a reasonable degree of confidence in the market research study:\n\nIf \\(s_1\\) is true, \\(P(F|s_1) = 0.90\\) and \\(P(U|s_1) = 0.10\\).\nIf \\(s_2\\) is true, \\(P(F|s_2) = 0.25\\) and \\(P(U|s_2) = 0.75\\)."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#tabular-computation-branch-probabilities-for-favorable-report",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#tabular-computation-branch-probabilities-for-favorable-report",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tabular Computation: Branch Probabilities for Favorable Report",
    "text": "Tabular Computation: Branch Probabilities for Favorable Report\n\n\n\nSteps for Tabular Probability Computation\n\nStep 1: In column 1, enter the states of nature. In column 2, enter the prior probabilities. In column 3, enter the conditional probabilities of a favorable report \\(F\\) given each state of nature.\nStep 2: In column 4, compute the joint probabilities by multiplying the prior probability values in column 2 by the corresponding conditional probabilities in column 3.\nStep 3: Sum the joint probabilities in column 4 to obtain the probability of a favorable report, \\(P(F)\\).\nStep 4: Divide each joint probability in column 4 by \\(P(F) = 0.77\\) to obtain the revised or posterior probabilities, \\(P(s_1|F)\\) and \\(P(s_2|F)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Nature \\(s_j\\)\nPrior Probabilities \\(P(s_j)\\)\nConditional Probabilities \\(P(F | s_j)\\)\nJoint Probabilities \\(P(F \\cap s_j)\\)\nPosterior Probabilities \\(P(s_j | F)\\)\n\n\n\n\n\\(s_1\\)\n0.8\n0.90\n0.72\n0.94\n\n\n\\(s_2\\)\n0.2\n0.25\n0.05\n0.06\n\n\nTotal\n1.0\n\n\\(P(F) = 0.77\\)\n1.00\n\n\n\n\nThe Table shows that the probability of obtaining a favorable report is \\(P(F) = 0.77\\).\n\nIn addition:\n\n\\(P(s_1|F) = 0.94\\)\n\\(P(s_2|F) = 0.06\\)\n\n\n\nNote that a favorable report prompts a posterior probability of 0.94 that demand will be strong (\\(s_1\\))."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#tabular-computation-branch-probabilities-for-unfavorable-report",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#tabular-computation-branch-probabilities-for-unfavorable-report",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Tabular Computation: Branch Probabilities for Unfavorable Report",
    "text": "Tabular Computation: Branch Probabilities for Unfavorable Report\n\nThe tabular probability computation procedure must be repeated for each possible sample information outcome.\n\n\nSteps for Tabular Probability Computation\n\nStep 1: In column 1, enter the states of nature. In column 2, enter the prior probabilities. In column 3, enter the conditional probabilities of an unfavorable report \\(U\\) given each state of nature.\nStep 2: In column 4, compute the joint probabilities by multiplying the prior probability values in column 2 by the corresponding conditional probabilities in column 3.\nStep 3: Sum the joint probabilities in column 4 to obtain the probability of an unfavorable report, \\(P(U)\\).\nStep 4: Divide each joint probability in column 4 by \\(P(U) = 0.23\\) to obtain the revised or posterior probabilities, \\(P(s_1|U)\\) and \\(P(s_2|U)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nStates of Nature \\(s_j\\)\nPrior Probabilities \\(P(s_j)\\)\nConditional Probabilities \\(P(U | s_j)\\)\nJoint Probabilities \\(P(U \\cap s_j)\\)\nPosterior Probabilities \\(P(s_j | U)\\)\n\n\n\n\n\\(s_1\\)\n0.8\n0.10\n0.08\n0.35\n\n\n\\(s_2\\)\n0.2\n0.75\n0.15\n0.65\n\n\nTotal\n1.0\n\n\\(P(U) = 0.23\\)\n1.00\n\n\n\nThe probability of an unfavorable report is \\(P(U) = 0.23\\).\n\nWith an unfavorable report:\n\n\\(P(s_1|U) = 0.35\\)\n\\(P(s_2|U) = 0.65\\)\nNote that an unfavorable report prompts a posterior probability of 0.65 that demand will be weak (\\(s_2\\))."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#conclusion",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#conclusion",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe resulting posterior probabilities provide a clearer understanding of potential outcomes, enabling PDC to select the optimal path based on updated insights. To recap from previous slides:\n\nThe Optimal Decision Strategy for PDC is to conduct the market research study and then carry out the following decision strategy, with EV = 15.93:\n\nIf the market research is favorable: Construct the large condominium complex to maximize profits (\\(d_3\\)) with EV = 18.26.\nIf the market research is unfavorable: Construct the medium condominium complex as a safer choice under lower demand expectations (\\(d_2\\)) with EV = 8.15."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-1",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-1",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\nSome key takeaways from this session:\n\nDecision Analysis Structure: A structured approach using payoff tables and decision trees allows for organized, strategic decision-making, particularly in uncertain conditions.\nExpected Value Approach: By calculating expected values across different states of nature, we can identify optimal decisions based on probabilistic outcomes, maximizing potential profits.\nValue of Information:\n\nExpected Value of Perfect Information (EVPI): Quantifies the benefit of having certainty about future states, guiding decision-makers on how much to invest in obtaining information.\nExpected Value of Sample Information (EVSI): Measures the potential benefit of additional data (e.g., market research), providing insight on the value of partial or imperfect information.\n\nBayes’ Theorem in Decision Trees: Allows for updating probabilities based on new evidence (e.g., favorable or unfavorable reports) to make more informed, dynamic decisions.\nDecision Trees with Probabilities: Decision trees not only display possible decisions and outcomes but, with added probabilities, allow for more precise expected value calculations.\nOptimal Decision Strategy: Analyzing decision trees and recalculating with updated probabilities leads to a final strategy that maximizes expected outcomes based on the best available information."
  },
  {
    "objectID": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-2",
    "href": "lecture_slides/20_chapter_decision_analysis/20_chapter_decision_analysis.html#summary-2",
    "title": " MGMT 30500: Business Statistics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nPurpose\nExample\n\n\n\n\nPayoff\nQuantitative value of profit/loss for a decision-state combination\nUsed to evaluate and compare decision alternatives\nBuilding a large complex yields $20 million if demand is high.\n\n\nResults\nActual effects observed after decision and state of nature occur\nEvaluate real-world effectiveness of decision\nMedium complex yields a realized profit of $14 million.\n\n\nOutputs\nImmediate, measurable consequences from the decision\nProvide operational metrics\nNumber of units built, time to completion, occupancy rate.\n\n\nOutcomes\nBroader end results, often aligned with goals/objectives\nAssess decision’s success in achieving goals\nProfit, market reputation, customer satisfaction, future demand"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#overview",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "XXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#what-is-statistical-learning",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#what-is-statistical-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#notation",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#what-is-fx-good-for",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#is-there-an-ideal-fx",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\n\n\n\n\n\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\nA good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#the-regression-function-fx",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\nAlso defined for a vector \\(\\mathbf{X}\\); for example, \\[\n  f(\\mathbf{x})\n  = f(x_1, x_2, x_3)\n  = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\nIdeal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error—even if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is typically a distribution of possible \\(Y\\) values.\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#how-to-estimate-f",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#the-curse-of-dimensionality",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#parametric-and-structured-models",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#parametric-and-structured-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\nLinear model:\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#summary-1",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#nearest-neighbor-observations",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\n\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#simulated-example",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\n\n\n\n\n\n\n\n\n\nSimulated example. Red points are simulated values for income from the model:\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#linear-regression-fit",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\n\n\n\n\n\n\n\n\n\nLinear regression model fit to the simulated data:\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#flexible-regression-model-fit",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\n\n\n\n\n\n\n\n\n\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. Here we use a technique called a thin-plate spline to fit a flexible surface.\nWe control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#overfitting",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\n\n\n\n\n\n\n\n\n\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#some-trade-offs",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso\n\nIntermediate: Least squares, Generalized Additive Models, Trees\n\nHigh flexibility: Bagging, Boosting, Support Vector Machines"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#assessing-model-accuracy",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#assessing-model-accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\nBlack curve is the truth. Red curve on right is MSE on the test set (\\(\\text{MSE}_{Te}\\)), grey curve is MSE on the training set (\\(\\text{MSE}_{Tr}\\)).\nOrange, blue, green curves/squares correspond to fits of different flexibilities."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#smoother-truth-example",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#smoother-truth-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoother Truth Example",
    "text": "Smoother Truth Example\n\n\n\n\n\n\n\n\n\nHere the truth is smoother, so smoother fits and linear models perform well."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#noisy-wiggly-truth-example",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#noisy-wiggly-truth-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Noisy, Wiggly Truth Example",
    "text": "Noisy, Wiggly Truth Example\n\n\n\n\n\n\n\n\n\nHere the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off-1",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off-examples",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#bias-variance-trade-off-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off (Examples)",
    "text": "Bias-Variance Trade-off (Examples)\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#classification-problems-1",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#nearest-neighbor-averaging",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#classification-some-details",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-k-10",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\n\n\n\n\n\n\n\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-error-rates",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\nThis figure illustrates how training errors (teal curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line (around 0.15) might indicate a baseline or reference error rate for comparison."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models-linear-model",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models: Linear model",
    "text": "Comparison of Models: Linear model\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models-1",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html#comparison-of-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#xxx",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#xxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXX?",
    "text": "XXX?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nQuestions we might ask:\n\n\n\n\nIs there a relationship between advertising budget and sales?\nHow strong is the relationship between advertising budget and sales?\nWhich media contribute to sales?\nHow accurately can we predict future sales?\nIs the relationship linear?\nIs there synergy among the advertising media?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data",
    "text": "Advertising Data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-parameters-by-least-squares",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of Parameters by Least Squares",
    "text": "Estimation of Parameters by Least Squares\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)-th value of \\(X\\). Then:\n\n\\[\n  RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\n\nThe least squares approach chooses \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS:\n\n\\[\n  \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing — Continued",
    "text": "Hypothesis Testing — Continued\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#the-woes-of-interpreting-regression-coefficients",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#the-woes-of-interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Woes of Interpreting Regression Coefficients",
    "text": "The Woes of Interpreting Regression Coefficients\n\nRegression coefficient estimates the expected change in \\(Y\\) per unit change in \\(X_j\\), holding other predictors fixed.\nExample 1:\n\\(Y =\\) Total change in your pocket, \\(X_1 =\\) Number of coins, \\(X_2 =\\) Denominations.\nExample 2:\n\\(Y =\\) Number of tackles in a season, \\(W\\) = weight, \\(H\\) = height:\n\n\\[\n    \\hat{Y} = b_0 + 0.50W - 0.10H\n\\]\nWhat does \\(\\beta_2 &lt; 0\\) mean?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#two-quotes-by-famous-statisticians",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#two-quotes-by-famous-statisticians",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Two Quotes by Famous Statisticians",
    "text": "Two Quotes by Famous Statisticians\n\n“Essentially, all models are wrong, but some are useful.”\n— George Box\n\n\n“The only way to find out what will happen when a complex system is disturbed is to disturb the system, not merely to observe it passively.”\n— Fred Mosteller and John Tukey"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection — Continued",
    "text": "Model Selection — Continued\n\nWe will discuss other criterias, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#other-considerations-in-the-regression-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#other-considerations-in-the-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Considerations in the Regression Model",
    "text": "Other Considerations in the Regression Model"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-scatterplot-matrix",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-scatterplot-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data Scatterplot Matrix",
    "text": "Credit Card Data Scatterplot Matrix"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors Continued",
    "text": "Qualitative Predictors Continued\n\nBoth variables can be used in the regression equation to obtain the model:\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\n\\]\nWhere:\n\n\\(\\beta_0 + \\beta_1 + \\epsilon_i\\): if \\(i\\)-th person is Asian\n\\(\\beta_0 + \\beta_2 + \\epsilon_i\\): if \\(i\\)-th person is Caucasian\n\\(\\beta_0 + \\epsilon_i\\): if \\(i\\)-th person is African American (baseline)\nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data — Continued",
    "text": "Credit Card Data — Continued\nResults for gender model:\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nGender [Female]\n19.73\n46.05\n0.429\n0.6690"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the ethnicity variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is Asian} \\\\\n      0 & \\text{if i-th person is not Asian}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is Caucasian} \\\\\n      0 & \\text{if i-th person is not Caucasian}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-continued-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors Continued",
    "text": "Qualitative Predictors Continued\n\nBoth variables can be used in the regression equation to obtain the model:\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\n\\]\nWhere:\n\n\\(\\beta_0 + \\beta_1 + \\epsilon_i\\): if \\(i\\)-th person is Asian\n\\(\\beta_0 + \\beta_2 + \\epsilon_i\\): if \\(i\\)-th person is Caucasian\n\\(\\beta_0 + \\epsilon_i\\): if \\(i\\)-th person is African American (baseline)\nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(Asian\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(Caucasian\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares Asian to African American and that’s not significant. Likewise, the Caucasian to African-American is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#extensions-of-the-linear-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#extensions-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions of the Linear Model",
    "text": "Extensions of the Linear Model\nRemoving the additive assumption: interactions and nonlinearity\nInteractions:\n\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\nFor example, the linear model:\n\n\\[ \\widehat{\\text{sales}} = \\beta\\_0 + \\beta\\_1 \\times \\text{TV} + \\beta\\_2 \\times \\text{radio} + \\beta\\_3 \\times \\text{newspaper} \\]\nstates that the average effect on sales of a one-unit increase in TV is always (_1), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions — continued",
    "text": "Interactions — continued\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy-continued",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy — Continued",
    "text": "Hierarchy — Continued\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects — their meaning is changed.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values (e.g., gender, ethnicity).\nCategorical predictors can be represented using factor variables.\nQualitative variables: Gender, Student (Student Status), Status (Marital Status), Ethnicity."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#overview",
    "href": "lecture_slides/04_classification/04_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#xxxx",
    "href": "lecture_slides/04_classification/04_classification.html#xxxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXXX",
    "text": "XXXX"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classification",
    "href": "lecture_slides/04_classification/04_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "href": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\nLeft plot: Probability of Default using Linear Regression:\n\nOrange marks indicate the response \\(Y\\), either 0 or 1.\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\n\nRight plot: Probability of Default using Logistic Regression:\n\nLogistic regression seems well-suited to the task.\n\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1. Linear regression does not estimate \\(\\Pr(Y = 1|X)\\) well. Logistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "href": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant [Euler’s number].\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\nThis likelihood gives the probability of the observed zeros and ones in the data. We pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood of the observed data.\nMost statistical packages can fit linear logistic regression models by maximum likelihood. In R, we use the glm function.\nLogistic Regression Coefficients\nCoefficient | Std. Error | Z-statistic | P-value |\n|-||-|-| | Intercept | -10.6513 | 0.3612 | -29.5 | &lt; 0.0001 | | balance | 0.0055 | 0.0002 | 24.9 | &lt; 0.0001 |"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#making-predictions",
    "href": "lecture_slides/04_classification/04_classification.html#making-predictions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nWhat is our estimated probability of default for someone with a balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent[Yes]\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\nLogistic Regression Coefficients\n                  | Coefficient | Std. Error | Z-statistic | P-value |\n|——————-|————-|————|———– -|———-| | Intercept | -10.8690 | 0.4923 | -22.08 | &lt; 0.0001 | | balance | 0.0057 | 0.0002 | 24.74 | &lt; 0.0001 | | income | 0.0030 | 0.0082 | 0.37 | 0.7115 | | student[Yes] | -0.6468 | 0.2362 | -2.74 | 0.0062 |\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding",
    "href": "lecture_slides/04_classification/04_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-south-african-heart-disease",
    "href": "lecture_slides/04_classification/04_classification.html#example-south-african-heart-disease",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: South African Heart Disease",
    "text": "Example: South African Heart Disease\n\n160 cases of MI (myocardial infarction) and 302 controls (all male in age range 15–64), from Western Cape, South Africa, in the early 80s.\nOverall prevalence very high in this region: 5.1%.\nMeasurements on seven predictors (risk factors), shown in a scatterplot matrix.\nGoal is to identify relative strengths and directions of risk factors.\nThis was part of an intervention study aimed at educating the public on healthier diets."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#scatterplot-matrix-of-south-african-heart-disease-data",
    "href": "lecture_slides/04_classification/04_classification.html#scatterplot-matrix-of-south-african-heart-disease-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scatterplot Matrix of South African Heart Disease Data",
    "text": "Scatterplot Matrix of South African Heart Disease Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplot matrix of the South African Heart Disease data.\nThe response is color-coded:\n\nThe cases (MI) are red.\nThe controls are turquoise.\n\nfamhist is a binary variable, with 1 indicating family history of MI."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#case-control-sampling-and-logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#case-control-sampling-and-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Case-control Sampling and Logistic Regression",
    "text": "Case-control Sampling and Logistic Regression\n\nIn South African data, there are 160 cases, 302 controls — \\(\\tilde{\\pi} = 0.35\\) are cases. Yet the prevalence of MI in this region is \\(\\pi = 0.05\\).\nWith case-control samples, we can estimate the regression parameters \\(\\beta_j\\) accurately (if our model is correct); the constant term \\(\\beta_0\\) is incorrect.\nWe can correct the estimated intercept by a simple transformation:\n\n\\[\n\\hat{\\beta}_0^* = \\hat{\\beta}_0 + \\log\\left(\\frac{\\pi}{1-\\pi}\\right) - \\log\\left(\\frac{\\tilde{\\pi}}{1-\\tilde{\\pi}}\\right)\n\\]\n\nOften cases are rare, and we take them all; up to five times that number of controls is sufficient. See the next frame."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#diminishing-returns-in-unbalanced-binary-data",
    "href": "lecture_slides/04_classification/04_classification.html#diminishing-returns-in-unbalanced-binary-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diminishing Returns in Unbalanced Binary Data",
    "text": "Diminishing Returns in Unbalanced Binary Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling more controls than cases reduces the variance of the parameter estimates.\nHowever, after a ratio of about 5 to 1, the variance reduction flattens out."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-more-than-two-classes",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-more-than-two-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with More than Two Classes",
    "text": "Logistic Regression with More than Two Classes\nSo far, we have discussed logistic regression with two classes. It is easily generalized to more than two classes. One version (used in the R package glmnet) has the symmetric form:\n\\[\n\\Pr(Y = k \\mid X) = \\frac{e^{\\beta_{0k} + \\beta_{1k}X_1 + \\cdots + \\beta_{pk}X_p}}{\\sum_{\\ell=1}^{K} e^{\\beta_{0\\ell} + \\beta_{1\\ell}X_1 + \\cdots + \\beta_{p\\ell}X_p}}\n\\]\nHere there is a linear function for each class.\n(The mathier students will recognize that some cancellation is possible, and only \\(K - 1\\) linear functions are needed as in 2-class logistic regression.)\nMulticlass logistic regression is also referred to as multinomial regression."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\n\n\\[\n\\pi_1 = 0.5, \\quad \\pi_2 = 0.5\n\\]\n\n\\[\n\\pi_1 = 0.3, \\quad \\pi_2 = 0.7\n\\]\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_k f_k(x)\\).\nOn the right, we favor the pink class — the decision boundary has shifted to the left."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify at the value \\(X = x\\), we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\n\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]\n(See if you can show this.)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nExample with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "href": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere\n\\[\n\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\nis the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\nDensity:\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n\\]\nDiscriminant function:\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\n\\]\nDespite its complex form:\n\\[\n\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\n\\]\nis a linear function."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\nHere \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\).\nThe dashed lines are known as the Bayes decision boundaries. Were they known, they would yield the fewest misclassification errors, among all possible classifiers."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#fishers-iris-data",
    "href": "lecture_slides/04_classification/04_classification.html#fishers-iris-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fisher’s Iris Data",
    "text": "Fisher’s Iris Data\n\n\n4 variables\n3 species\n50 samples/class\n\n🟦 Setosa\n🟧 Versicolor\n🟩 Virginica"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#fishers-discriminant-plot",
    "href": "lecture_slides/04_classification/04_classification.html#fishers-discriminant-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fisher’s Discriminant Plot",
    "text": "Fisher’s Discriminant Plot\n\n\nWhen there are \\(K\\) classes, linear discriminant analysis (LDA) can be visualized exactly in a \\(K - 1\\)-dimensional plot.\nLDA classifies points based on their proximity to centroids in the discriminant space.\nFor \\(K &gt; 3\\), the best two-dimensional plane for visualization can be identified to illustrate the discriminant rule effectively."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\nOnce we have estimates \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "href": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nTrue Default Status\n\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\\((23 + 252) / 10000\\) errors — a 2.75% misclassification rate!\n\nSome caveats:\n\nThis is training error, and we may be overfitting. Not a big concern here since \\(n = 10000\\) and \\(p = 2\\)!\nIf we classified to the prior — always to class No in this case — we would make \\(333 / 10000\\) errors, or only 3.33%.\nOf the true No’s, we make \\(23 / 9667 = 0.2\\%\\) errors; of the true Yes’s, we make \\(252 / 333 = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#types-of-errors",
    "href": "lecture_slides/04_classification/04_classification.html#types-of-errors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\n\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "href": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "href": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC plot displays both simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\n\nKey Insight: Because the (_k) are different for each class, the quadratic terms matter significantly.\n\nInterpretation\n\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nIt is more flexible than LDA but requires more data to estimate class-specific (_k) accurately."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naive-bayes",
    "href": "lecture_slides/04_classification/04_classification.html#naive-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes Assumptions\n\\[\n\\delta_k(x) \\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right]\n\\] \\[\n= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative):\n\nIf \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\n\n\n**Key Point Despite strong assumptions, naive Bayes often produces good classification results."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\nFor a two-class problem, one can show that for LDA: \\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\n\nSo it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nKey Differences:\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\) (known as discriminative learning).\nLDA uses the full likelihood based on \\(\\text{Pr}(X, Y)\\) (known as generative learning).\nDespite these differences, in practice, the results are often very similar.\n\nFootnote: Logistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#summary",
    "href": "lecture_slides/04_classification/04_classification.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nLogistic regression is very popular for classification, especially when \\(K = 2\\).\nLDA is useful when \\(n\\) is small, or the classes are well separated, and Gaussian assumptions are reasonable. Also useful when \\(K &gt; 2\\).\nNaive Bayes is useful when \\(p\\) is very large.\nSee Section 4.5 for some comparisons of logistic regression, LDA, and KNN."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#new-topics-in-chapter-four",
    "href": "lecture_slides/04_classification/04_classification.html#new-topics-in-chapter-four",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "New Topics in Chapter Four",
    "text": "New Topics in Chapter Four\nIn this section, we cover three topics in Chapter 4 that are new in the second edition of ISLR: - Multinomial logistic regression. - Naïve Bayes classification. - Generalized linear models."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nThere is a redundancy here; we really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example is given in Chapter 10, where we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nOften useful if some classes are well separated — a situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example:\n\\[\nX_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\n\\]\nWe estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category. Example to follow.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\nDensity estimates for class \\(k=1\\)\n\n\\[\nx^* = (0.4, 1.5, 1)\n\\]\n\\[\n\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\n\\]\n\\[\n\\hat{f}_{11}(0.4) = 0.368,\\ \\hat{f}_{12}(1.5) = 0.484,\\ \\hat{f}_{13}(1) = 0.226\n\\]\nDensity estimates for class \\(k=2\\)\n\\[\n\\hat{f}_{21}(0.4) = 0.030,\\ \\hat{f}_{22}(1.5) = 0.130,\\ \\hat{f}_{23}(1) = 0.616\n\\]\nProbabilities:\n\\[\nP(Y=1 \\mid X = x^*) = 0.944\n\\]\n\\[\nP(Y=2 \\mid X = x^*) = 0.056\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\\[\n= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]\n\\[\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\\[\n= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model takes the form of a generalized additive model from Chapter 7."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit[cloudy/misty]\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit[light rain/snow]\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit[heavy rain/snow]\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#meanvariance-relationship",
    "href": "lecture_slides/04_classification/04_classification.html#meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mean/Variance Relationship",
    "text": "Mean/Variance Relationship\n\n\n\n\n\n\n\n\nPlot Description\nObservation\n\n\n\n\nLeft Plot\nVariance mostly increases with the mean.\n\n\nRight Plot\nLog transformation of bikers alleviates variance issues but introduces problems like wrong scale for predictions and zero counts.\n\n\n\nAdditional Notes\n\n10% of linear model predictions are negative! (not shown here).\nLog transformation (log(bikers)) helps with variance issues but may introduce new complications."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "href": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\n\nPoisson distribution is useful for modeling counts:\n\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\n\nMean/variance relationship:\n\n\\[\n  \\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\n\\]\ni.e., there is a mean/variance dependence.\nModel with Covariates\n\nWe model:\n\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\nOr equivalently:\n\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nKey Features\n\nAutomatic positivity:\nThe model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression on Bikeshare Data",
    "text": "Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit[cloudy/misty]\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit[light rain/snow]\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit[heavy rain/snow]\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n:::\n\nBy Hour\n:::\nNotes\n\nOverdispersion:\n\nIn this case, the variance is somewhat larger than the mean — a situation known as overdispersion.\nAs a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nWe have covered three GLMs in this course: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are:\n\nLinear: \\(\\eta(\\mu) = \\mu\\),\nLogistic: \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\),\nPoisson: \\(\\eta(\\mu) = \\log(\\mu)\\).\n\n\nAdditional Notes\n\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include:\n\nGamma\nNegative-binomial\nInverse Gaussian\nand more."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#summary-2",
    "href": "lecture_slides/04_classification/04_classification.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#section",
    "href": "lecture_slides/04_classification/04_classification.html#section",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "LDA classifies all but 3 of the 150 training samples correctly."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXXX",
    "text": "XXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-validation and the Bootstrap",
    "text": "Cross-validation and the Bootstrap\n\nIn this section we discuss two resampling methods: cross-validation and the bootstrap.\nThese methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nIn contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training- versus Test-Set Performance",
    "text": "Training- versus Test-Set Performance"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Prediction-Error Estimates",
    "text": "More on Prediction-Error Estimates\n\nBest solution: a large designated test set. Often not available.\nSome methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. These include the Cp statistic, AIC, and BIC. They are discussed elsewhere in this course.\nHere we instead consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held-out observations."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error. This is typically assessed using MSE in the case of a quantitative response and misclassification rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\nA random splitting into two halves:\nLeft part is the training set, and the right part is the validation set."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Automobile Data",
    "text": "Example: Automobile Data\n\nWant to compare linear vs higher-order polynomial terms in a linear regression.\nWe randomly split the 392 observations into two sets:\n\nA training set containing 196 of the data points.\nA validation set containing the remaining 196 observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft panel shows single split; right panel shows multiple splits."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Drawbacks of Validation Set Approach",
    "text": "Drawbacks of Validation Set Approach\n\nThe validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nIn the validation approach, only a subset of the observations — those that are included in the training set rather than in the validation set — are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. Why?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\n\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 5\\) here)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Details",
    "text": "The Details\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSetting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Nice Special Case!",
    "text": "A Nice Special Case!\n\nWith least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\n\\[\n  \\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2,\n\\]\nwhere \\(\\hat{y}_i\\) is the \\(i\\)-th fitted value from the original least-squares fit, and \\(h_i\\) is the leverage (diagonal of the “hat” matrix; see book for details). This is like the ordinary MSE, except the \\(i\\)-th residual is divided by \\(1 - h_i\\).\n\nLOOCV is sometimes useful, but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated, and hence their average can have high variance.\nA better choice is \\(K = 5\\) or \\(10\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Auto Data Revisited",
    "text": "Auto Data Revisited"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "True and Estimated Test MSE for the Simulated Data",
    "text": "True and Estimated Test MSE for the Simulated Data"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Issues with Cross-Validation",
    "text": "Other Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation: Right and Wrong",
    "text": "Cross-Validation: Right and Wrong\n\nConsider a simple classifier applied to some two-class data:\n\nStarting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with the class labels.\nWe then apply a classifier such as logistic regression, using only these 100 predictors.\n\n\nHow do we estimate the test set performance of this classifier?\nCan we apply cross-validation in step 2, forgetting about step 1?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NO!",
    "text": "NO!\n\nThis would ignore the fact that in Step 1, the procedure has already seen the labels of the training data, and made use of them. This is a form of training and must be included in the validation process.\nIt is easy to simulate realistic data with the class labels independent of the outcome, so that true test error = 50%, but the CV error estimate that ignores Step 1 is zero! Try to do this yourself.\nWe have seen this error made in many high-profile genomics papers."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Wrong and Right Way",
    "text": "The Wrong and Right Way\n\nWrong: Apply cross-validation in step 2.\nRight: Apply cross-validation to steps 1 and 2."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Wrong Way",
    "text": "Wrong Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Right Way",
    "text": "Right Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Where Does the Name Come From?",
    "text": "Where Does the Name Come From?\n\nThe use of the term bootstrap derives from the phrase to pull oneself up by one’s bootstraps, widely thought to be based on one of the eighteenth-century The Surprising Adventures of Baron Munchausen by Rudolph Erich Raspe:\n\nThe Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.\n\nIt is not the same as the term bootstrap used in computer science, meaning to “boot” a computer from a set of core instructions, though the derivation is similar."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Simple Example",
    "text": "A Simple Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize:\n\n\\[\n  \\text{Var}(\\alpha X + (1 - \\alpha) Y).\n\\]\n\nOne can show that the value that minimizes the risk is given by:\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\] where \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nBut the values of \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), and \\(\\sigma_{XY}\\) are unknown.\nWe can compute estimates for these quantities, \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\), and \\(\\hat{\\sigma}_{XY}\\), using a data set that contains measurements for \\(X\\) and \\(Y\\).\nWe can then estimate the value of \\(\\alpha\\) that minimizes the variance of our investment using:\n\n\\[\n  \\hat{\\alpha} = \\frac{\\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 - 2\\hat{\\sigma}_{XY}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nEach panel displays 100 simulated returns for investments X and Y. From left to right and top to bottom, the resulting estimates for \\(\\alpha\\) are 0.576, 0.532, 0.657, and 0.651."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nTo estimate the standard deviation of \\(\\hat{\\alpha}\\), we repeated the process of simulating 100 paired observations of \\(X\\) and \\(Y\\), and estimating \\(\\alpha\\) 1,000 times.\nWe thereby obtained 1,000 estimates for \\(\\alpha\\), which we can call \\(\\hat{\\alpha}_1, \\hat{\\alpha}_2, \\ldots, \\hat{\\alpha}_{1000}\\).\nThe left-hand panel of the Figure on slide 29 displays a histogram of the resulting estimates.\nFor these simulations, the parameters were set to:\n\n\\[\n  \\sigma_X^2 = 1, \\, \\sigma_Y^2 = 1.25, \\, \\sigma_{XY} = 0.5,\n\\] and so we know that the true value of \\(\\alpha\\) is 0.6 (indicated by the red line)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nThe mean over all 1,000 estimates for \\(\\alpha\\) is:\n\n\\[\n  \\bar{\\alpha} = \\frac{1}{1000} \\sum_{r=1}^{1000} \\hat{\\alpha}_r = 0.5996,\n\\]\nvery close to \\(\\alpha = 0.6\\), and the standard deviation of the estimates is:\n\\[\n  \\sqrt{\\frac{1}{1000 - 1} \\sum_{r=1}^{1000} (\\hat{\\alpha}_r - \\bar{\\alpha})^2} = 0.083.\n\\]\n\nThis gives us a very good idea of the accuracy of \\(\\hat{\\alpha}\\): \\(\\text{SE}(\\hat{\\alpha}) \\approx 0.083\\).\nSo roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately 0.08, on average."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nLeft: A histogram of the estimates of \\(\\alpha\\) obtained by generating 1,000 simulated data sets from the true population.\nCenter: A histogram of the estimates of \\(\\alpha\\) obtained from 1,000 bootstrap samples from a single data set.\nRight: The estimates of \\(\\alpha\\) displayed in the left and center panels are shown as boxplots. In each panel, the pink line indicates the true value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now Back to the Real World",
    "text": "Now Back to the Real World\n\nThe procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.\nHowever, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.\nRather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.\nEach of these “bootstrap data sets” is created by sampling with replacement, and is the same size as our original dataset. As a result, some observations may appear more than once in a given bootstrap data set and some not at all."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example with Just 3 Observations",
    "text": "Example with Just 3 Observations\n\n\n\n\nObs\nX\nY\n\n\n\n\n1\n4.3\n2.4\n\n\n2\n2.1\n1.1\n\n\n3\n5.3\n2.8\n\n\n\nOriginal Data (\\(Z\\))\n\\[\nZ^{*1} \\quad \\text{Bootstrap Data 1:} \\quad\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Obs} & X   & Y   \\\\\n\\hline\n3          & 5.3 & 2.8 \\\\\n1          & 4.3 & 2.4 \\\\\n3          & 5.3 & 2.8 \\\\\n\\hline\n\\end{array}\n\\]\n\\[\nZ^{*2} \\quad \\text{Bootstrap Data 2:} \\quad\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Obs} & X   & Y   \\\\\n\\hline\n2          & 2.1 & 1.1 \\\\\n3          & 5.3 & 2.8 \\\\\n1          & 4.3 & 2.4 \\\\\n\\hline\n\\end{array}\n\\]\n\\[\nZ^{*B} \\quad \\text{Bootstrap Data $B$:} \\quad\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Obs} & X   & Y   \\\\\n\\hline\n2          & 2.1 & 1.1 \\\\\n2          & 2.1 & 1.1 \\\\\n1          & 4.3 & 2.4 \\\\\n\\hline\n\\end{array}\n\\]\n\nA graphical illustration of the bootstrap approach on a small sample containing \\(n = 3\\) observations.\nEach bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set.\nEach bootstrap data set is used to obtain an estimate of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bootstrap Standard Error",
    "text": "Bootstrap Standard Error\n\nDenoting the first bootstrap data set by \\(Z^{*1}\\), we use \\(Z^{*1}\\) to produce a new bootstrap estimate for \\(\\alpha\\), which we call \\(\\hat{\\alpha}^{*1}\\).\nThis procedure is repeated \\(B\\) times for some large value of \\(B\\) (say 100 or 1000), in order to produce \\(B\\) different bootstrap data sets, \\(Z^{*1}, Z^{*2}, \\ldots, Z^{*B}\\), and \\(B\\) corresponding \\(\\alpha\\) estimates, \\(\\hat{\\alpha}^{*1}, \\hat{\\alpha}^{*2}, \\ldots, \\hat{\\alpha}^{*B}\\).\nWe estimate the standard error of these bootstrap estimates using the formula:\n\n\\[\nSE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B - 1} \\sum_{r=1}^B (\\hat{\\alpha}^{*r} - \\bar{\\alpha}^{*})^2}.\n\\]\n\nThis serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. See center and right panels of Figure on slide 29. Bootstrap results are in blue.\nFor this example \\(SE_B(\\hat{\\alpha}) = 0.087\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A General Picture for the Bootstrap",
    "text": "A General Picture for the Bootstrap\n\n\n\nReal World - Population: \\(P\\) - Random Sampling → Data: \\(Z = (z_1, z_2, \\ldots, z_n)\\) - Estimate: \\(f(Z)\\)\n\nBootstrap World - Estimated Population: \\(\\hat{P}\\) - Random Sampling → Bootstrap Dataset: \\(Z^* = (z^*_1, z^*_2, \\ldots, z^*_n)\\) - Bootstrap Estimate: \\(f(Z^*)\\)"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap in General",
    "text": "The Bootstrap in General\n\nIn more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.\nFor example, if the data is a time series, we can’t simply sample the observations with replacement (why not?).\nWe can instead create blocks of consecutive observations and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Uses of the Bootstrap",
    "text": "Other Uses of the Bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the figure on slide 29, the 5% and 95% quantiles of the 1,000 values is (0.43, 0.72).\nThis represents an approximate 90% confidence interval for the true α. How do we interpret this confidence interval?\nThe above interval is called a Bootstrap Percentile confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. Can you prove this?\nThis will cause the bootstrap to seriously underestimate the true prediction error. Why?\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Overlap",
    "text": "Removing the Overlap\n\nCan partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.\nBut the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation",
    "text": "Pre-validation\n\nIn microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.\nComparing them on the same dataset that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.\nPre-validation can be used to make a fairer comparison between the two sets of predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Motivating Example",
    "text": "Motivating Example\nAn example of this problem arose in the paper of van’t Veer et al. Nature (2002). Their microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer. There are 44 cases in the good prognosis group and 34 in the poor prognosis group. A “microarray” predictor was constructed as follows:\n\n70 genes were selected, having the largest absolute correlation with the 78 class labels.\nUsing these 70 genes, a nearest-centroid classifier \\(C(x)\\) was constructed.\nApplying the classifier to the 78 microarrays gave a dichotomous predictor \\(z_i = C(x_i)\\) for each case \\(i\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nPre-validated\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Idea behind Pre-validation",
    "text": "Idea behind Pre-validation\n\nDesigned for comparison of adaptively derived predictors to fixed, pre-defined predictors.\nThe idea is to form a “pre-validated” version of the adaptive predictor: specifically, a “fairer” version that hasn’t “seen” the response \\(y\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation Process",
    "text": "Pre-validation Process"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation in Detail for This Example",
    "text": "Pre-validation in Detail for This Example\n\nDivide the cases up into \\(K = 13\\) equal-sized parts of 6 cases each.\nSet aside one of the parts. Using only the data from the other 12 parts:\n\nSelect the features having an absolute correlation of at least 0.3 with the class labels.\nForm a nearest centroid classification rule.\n\nUse the rule to predict the class labels for the 13th part.\nRepeat steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor \\(\\tilde{z}_i\\) for each of the 78 cases.\nFit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap versus Permutation Tests",
    "text": "The Bootstrap versus Permutation Tests\n\nBootstrap:\n\nSamples from the estimated population and uses the results to estimate standard errors and confidence intervals.\n\nPermutation Methods:\n\nSample from an estimated null distribution for the data.\nUsed to estimate p-values and False Discovery Rates for hypothesis tests.\n\nBootstrap for Null Hypothesis Testing:\n\nCan test a null hypothesis in simple situations.\nExample: If \\(\\theta = 0\\) is the null hypothesis, check whether the confidence interval for \\(\\theta\\) contains zero.\n\nAdapting Bootstrap for Null Distribution:\n\nCan adapt bootstrap to sample from a null distribution.\nSee Efron and Tibshirani, An Introduction to the Bootstrap (1993), Chapter 16.\nHowever, there is no real advantage over permutation tests."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#overview",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#linear-model-selection-and-regularization",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#linear-model-selection-and-regularization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\n\nRecall the linear model\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. In the lectures covering Chapter 7 of the text, we generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#in-praise-of-linear-models",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#in-praise-of-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In praise of linear models!",
    "text": "In praise of linear models!\n\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\nHence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why consider alternatives to least squares?",
    "text": "Why consider alternatives to least squares?\n\nPrediction Accuracy: especially when \\(p &gt; n\\), to control the variance.\nModel Interpretability: By removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing feature selection."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#three-classes-of-methods",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#three-classes-of-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three classes of methods",
    "text": "Three classes of methods\n\nSubset Selection. We identify a subset of the \\(p\\) predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\nShrinkage. We fit a model involving all \\(p\\) predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance and can also perform variable selection.\nDimension Reduction. We project the \\(p\\) predictors into a \\(M\\)-dimensional subspace, where \\(M &lt; p\\). This is achieved by computing \\(M\\) different linear combinations, or projections, of the variables. Then these \\(M\\) projections are used as predictors to fit a linear regression model by least squares."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#subset-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#example---credit-data-set",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#example---credit-data-set",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example - Credit data set",
    "text": "Example - Credit data set\n\n\nFor each possible model containing a subset of the ten predictors in the Credit data set, the RSS and \\(R^2\\) are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nThough the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#extensions-to-other-models",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#extensions-to-other-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions to other models",
    "text": "Extensions to other models\n\nAlthough we have presented best subset selection here for least squares regression, the same ideas apply to other types of models, such as logistic regression.\nThe deviance—negative two times the maximized log-likelihood—plays the role of RSS for a broader class of models."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#stepwise-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\). Why not?\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#forward-stepwise-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nIn particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#in-detail",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In Detail",
    "text": "In Detail\nForward Stepwise Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#more-on-forward-stepwise-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#more-on-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Forward Stepwise Selection",
    "text": "More on Forward Stepwise Selection\n\nComputational advantage over best subset selection is clear.\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors. Why not? Give an example."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n# Variables\nBest subset\nForward stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#backward-stepwise-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#backward-stepwise-selection-details",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#backward-stepwise-selection-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: details",
    "text": "Backward Stepwise Selection: details\nBackward Stepwise Selection\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all \\(p\\) predictors.\nFor \\(k = p, p - 1, \\ldots, 1\\):\n\n2.1 Consider all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k - 1\\) predictors.\n2.2 Choose the best among these \\(k\\) models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#more-on-backward-stepwise-selection",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#more-on-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Backward Stepwise Selection",
    "text": "More on Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#choosing-the-optimal-model",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#choosing-the-optimal-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#estimating-test-error-two-approaches",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#estimating-test-error-two-approaches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating test error: two approaches",
    "text": "Estimating test error: two approaches\n\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nWe can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures.\nWe illustrate both approaches next."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)\n\nThese techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.\nThe next figure displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-1",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#now-for-some-details",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#now-for-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now for some details",
    "text": "Now for some details\n\nMallow’s \\(C_p\\): \\[\nC_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\n\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement.\n\nThe AIC criterion is defined for a large class of models fit by maximum likelihood:\n\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent. Prove this."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-on-bic",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-on-bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details on BIC",
    "text": "Details on BIC\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#adjusted-r2",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nFor a least squares model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as \\[\n\\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n - d - 1)}{\\text{TSS}/(n - 1)}.\n\\]\n\nwhere TSS is the total sum of squares.\n\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{\\text{RSS}}{n - d - 1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{\\text{RSS}}{n - d - 1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#validation-and-cross-validation",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-2",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-previous-figure",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10\\) folds. In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. What is the rationale for this?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#shrinkage-methods",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize \\[\n\\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression: continued",
    "text": "Ridge regression: continued\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-3",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-previous-figure-1",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge Regression: Scaling of Predictors",
    "text": "Ridge Regression: Scaling of Predictors\n\nThe standard least squares coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\nThe Bias-Variance Tradeoff\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients. Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\). The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses an \\(\\ell_1\\) (pronounced “ell 1”) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso: Continued",
    "text": "The Lasso: Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#example-credit-dataset",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#example-credit-dataset",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Dataset",
    "text": "Example: Credit Dataset"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?\nOne can show that the lasso and ridge regression coefficient estimates solve the problems:\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} |\\beta_j| \\leq s\n\\]\nand\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s,\n\\]\nrespectively."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso-picture",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#the-lasso-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso Picture",
    "text": "The Lasso Picture"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression: continued",
    "text": "Comparing the Lasso and Ridge Regression: continued\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data, except that now only two predictors are related to the response.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#conclusions",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-4",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#credit-data-example-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\nCross-validation errors for ridge regression\n\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\).\n\n\n\n\nCoefficient estimates as function of lambda\n\n\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#simulated-data-example",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#simulated-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated data example",
    "text": "Simulated data example\n\nLeft: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set.\nRight: The corresponding lasso coefficient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods-details",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Details",
    "text": "Dimension Reduction Methods: Details\n\nLet \\(Z_1, Z_2, \\ldots, Z_M\\) represent \\(M &lt; p\\) linear combinations of our original \\(p\\) predictors. That is, \\[\nZ_m = \\sum_{j=1}^p \\phi_{mj} X_j \\quad \\text{(1)}\n\\] for some constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\).\nWe can then fit the linear regression model, \\[\ny_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\quad \\text{(2)}\n\\] using ordinary least squares.\nNote that in model (2), the regression coefficients are given by \\(\\theta_0, \\theta_1, \\ldots, \\theta_M\\). If the constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\) are chosen wisely, then such dimension reduction approaches can often outperform OLS regression."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#dimension-reduction-methods-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Continued",
    "text": "Dimension Reduction Methods: Continued\n\nNotice that from definition (1), \\[\n\\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where \\[\n\\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#principal-components-regression",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nA subset of the advertising data. Left: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. Right: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued-1",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued-2",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#pictures-of-pca-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#application-to-principal-components-regression",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#application-to-principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application to Principal Components Regression",
    "text": "Application to Principal Components Regression\n\nPCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively. Left: Simulated data. Right: Simulated data."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#choosing-the-number-of-directions-m",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#choosing-the-number-of-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Directions \\(M\\)",
    "text": "Choosing the Number of Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#partial-least-squares",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#partial-least-squares-continued",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#partial-least-squares-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares: Continued",
    "text": "Partial Least Squares: Continued\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-partial-least-squares",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#details-of-partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Partial Least Squares",
    "text": "Details of Partial Least Squares\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#summary",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nResearch into methods that give sparsity, such as the lasso, is an especially hot area.\nLater, we will return to sparsity in more detail, and will describe related approaches such as the elastic net."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods copy/06_model_selection.html#summary-2",
    "href": "lecture_slides/05_resampling_methods copy/06_model_selection.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nModel selection methods are an essential tool for data analysis, especially for big datasets involving many predictors.\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#overview",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Beyond Linearity",
    "text": "Moving Beyond Linearity\nThe truth is never linear!\nOr almost never!\nBut often the linearity assumption is good enough.\nWhen it’s not…\n\npolynomials,\n\nstep functions,\n\nsplines,\n\nlocal regression, and\n\ngeneralized additive models\n\noffer a lot of flexibility, without losing the ease and interpretability of linear models."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\nDegree-4 Polynomial"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details",
    "text": "Details\n\nCreate new variables \\(X_1 = X, \\, X_2 = X^2\\), etc., and then treat as multiple linear regression.\nNot really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\):\n\\[\n\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 + \\hat{\\beta}_2 x_0^2 + \\hat{\\beta}_3 x_0^3 + \\hat{\\beta}_4 x_0^4.\n\\]\nSince \\(\\hat{f}(x_0)\\) is a linear function of the \\(\\hat{\\beta}_\\ell\\), can get a simple expression for pointwise-variances ([(x_0)]) at any value \\(x_0\\). In the figure, we have computed the fit and pointwise standard errors on a grid of values for \\(x_0\\). We show \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{se}[\\hat{f}(x_0)]\\).\nWe either fix the degree \\(d\\) at some reasonably low value or use cross-validation to choose \\(d\\)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details continued",
    "text": "Details continued\n\nLogistic regression follows naturally. For example, in the figure we model\n\\[\n\\text{Pr}(y_i &gt; 250 \\mid x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}.\n\\]\nTo get confidence intervals, compute upper and lower bounds on on the logit scale, and then invert to get on the probability scale.\nCan do separately on several variables—just stack the variables into one matrix, and separate out the pieces afterwards (see GAMs later).\nCaveat: Polynomials have notorious tail behavior — very bad for extrapolation.\nCan fit using \\(y \\sim \\text{poly}(x, \\text{degree} = 3)\\) in formula."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\nAnother way of creating transformations of a variable — cut the variable into distinct regions.\n\\[\nC_1(X) = I(X &lt; 35), \\quad C_2(X) = I(35 \\leq X &lt; 50), \\dots, C_3(X) = I(X \\geq 65)\n\\]\nPiecewise Constant"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step functions continued",
    "text": "Step functions continued\n\nEasy to work with. Creates a series of dummy variables representing each group.\nUseful way of creating interactions that are easy to interpret. For example, interaction effect of \\(\\text{Year}\\) and \\(\\text{Age}\\):\n\\[\nI(\\text{Year} &lt; 2005) \\cdot \\text{Age}, \\quad I(\\text{Year} \\geq 2005) \\cdot \\text{Age}\n\\] would allow for different linear functions in each age category.\nIn R: I(year &lt; 2005) or cut(age, c(18, 25, 40, 65, 90)).\nChoice of cutpoints or knots can be problematic. For creating nonlinearities, smoother alternatives such as splines are available."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#piecewise-polynomials",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#piecewise-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Piecewise Polynomials",
    "text": "Piecewise Polynomials\n\nInstead of a single polynomial in \\(X\\) over its whole domain, we can rather use different polynomials in regions defined by knots. E.g. (see figure)\n\\[\ny_i =\n\\begin{cases}\n\\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c; \\\\\n\\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\geq c.\n\\end{cases}\n\\]\nBetter to add constraints to the polynomials, e.g., continuity.\nSplines have the “maximum” amount of continuity."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Splines Visualization",
    "text": "Splines Visualization"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines",
    "text": "Linear Splines\nA linear spline with knots at \\(\\xi_k, \\, k = 1, \\dots, K\\) is a piecewise linear polynomial continuous at each knot.\nWe can represent this model as\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+1} b_{K+1}(x_i) + \\epsilon_i,\n\\]\nwhere the \\(b_k\\) are basis functions.\n\\[\nb_1(x_i) = x_i\n\\]\n\\[\nb_{k+1}(x_i) = (x_i - \\xi_k)_+, \\quad k = 1, \\dots, K\n\\]\nHere the \\(()_+\\) means positive part; i.e.,\n\\[\n(x_i - \\xi_k)_+ =\n\\begin{cases}\nx_i - \\xi_k & \\text{if } x_i &gt; \\xi_k \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines Visualization",
    "text": "Linear Splines Visualization"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines",
    "text": "Cubic Splines\nA cubic spline with knots at \\(\\xi_k, \\, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power basis functions:\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i,\n\\]\n\\[\nb_1(x_i) = x_i, \\quad b_2(x_i) = x_i^2, \\quad b_3(x_i) = x_i^3, \\quad b_{k+3}(x_i) = (x_i - \\xi_k)_+^3, \\quad k = 1, \\dots, K\n\\]\nwhere\n\\[\n(x_i - \\xi_k)_+^3 =\n\\begin{cases}\n(x_i - \\xi_k)^3 & \\text{if } x_i &gt; \\xi_k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines Visualization",
    "text": "Cubic Splines Visualization"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\nA natural cubic spline extrapolates linearly beyond the boundary knots. This adds \\(4 = 2 \\times 2\\) extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#fitting-splines-in-r",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#fitting-splines-in-r",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Splines in R",
    "text": "Fitting Splines in R\nFitting splines in R is easy: bs(x, ...) for any degree splines, and ns(x, ...) for natural cubic splines, in package splines.\nNatural Cubic Spline"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#knot-placement",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#knot-placement",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Knot Placement",
    "text": "Knot Placement\n\nOne strategy is to decide \\(K\\), the number of knots, and then place them at appropriate quantiles of the observed \\(X\\).\nA cubic spline with \\(K\\) knots has \\(K + 4\\) parameters or degrees of freedom.\nA natural spline with \\(K\\) knots has \\(K\\) degrees of freedom.\n\n\nComparison of a degree-14 polynomial and a natural cubic spline, each with 15df.\nns(age, df=14)\npoly(age, deg=14)"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\nThis section is a little bit mathematical 🛠️\nConsider this criterion for fitting a smooth function \\(g(x)\\) to some data:\n\\[\n\\text{minimize}_{g \\in S} \\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\n\nThe first term is RSS, and tries to make \\(g(x)\\) match the data at each \\(x_i\\).\nThe second term is a roughness penalty and controls how wiggly \\(g(x)\\) is. It is modulated by the tuning parameter \\(\\lambda \\geq 0\\).\n\nThe smaller \\(\\lambda\\), the more wiggly the function, eventually interpolating \\(y_i\\) when \\(\\lambda = 0\\).\nAs \\(\\lambda \\to \\infty\\), the function \\(g(x)\\) becomes linear."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines Continued",
    "text": "Smoothing Splines Continued\nThe solution is a natural cubic spline, with a knot at every unique value of \\(x_i\\). The roughness penalty still controls the roughness via \\(\\lambda\\).\nSome details:\n\nSmoothing splines avoid the knot-selection issue, leaving a single \\(\\lambda\\) to be chosen.\nThe algorithmic details are too complex to describe here. In R, the function smooth.spline() will fit a smoothing spline.\nThe vector of \\(n\\) fitted values can be written as \\(\\hat{g}_\\lambda = \\mathbf{S}_\\lambda \\mathbf{y}\\), where \\(\\mathbf{S}_\\lambda\\) is a \\(n \\times n\\) matrix (determined by the \\(x_i\\) and \\(\\lambda\\)).\nThe effective degrees of freedom are given by\n\n\\[\n  \\text{df}_\\lambda = \\sum_{i=1}^n \\{ \\mathbf{S}_\\lambda \\}_{ii}.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued-choosing-lambda",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued-choosing-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines Continued — Choosing \\(\\lambda\\)",
    "text": "Smoothing Splines Continued — Choosing \\(\\lambda\\)\n\nWe can specify \\(\\text{df}\\) rather than \\(\\lambda\\)!\nIn R: smooth.spline(age, wage, df = 10)\nThe leave-one-out (LOO) cross-validated error is given by\n\n\\[\n  \\text{RSS}_{\\text{cv}}(\\lambda) = \\sum_{i=1}^n \\left( y_i - \\hat{g}_\\lambda^{(-i)}(x_i) \\right)^2 = \\sum_{i=1}^n \\left[ \\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2.\n\\]\nIn R: smooth.spline(age, wage)"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Spline",
    "text": "Smoothing Spline"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Local Regression",
    "text": "Local Regression\n\nWith a sliding weight function, we fit separate linear fits over the range of \\(X\\) by weighted least squares.\nSee text for more details, and loess() function in R."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nAllows for flexible nonlinearities in several variables, but retains the additive structure of linear models.\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gam-details",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gam-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAM Details",
    "text": "GAM Details\n\nCan fit a GAM simply using, e.g., natural splines:\nlm(wage ~ ns(year, df = 5) + ns(age, df = 5) + education)\nCoefficients are not that interesting; fitted functions are. The previous plot was produced using plot.gam.\nCan mix terms — some linear, some nonlinear — and use anova() to compare models.\nCan use smoothing splines or local regression as well:\ngam(wage ~ s(year, df = 5) + lo(age, span = 0.5) + education)\nGAMs are additive, although low-order interactions can be included in a natural way using, e.g., bivariate smoothers or interactions of the form:\nns(age, df = 5):ns(year, df = 5)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gams-for-classification",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gams-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAMs for Classification",
    "text": "GAMs for Classification\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p).\n\\]\n\ngam(I(wage &gt; 250) ~ year + s(age, df = 5) + education, family = binomial)"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#overview",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nHere we describe tree-based methods for regression and classification.\nThese involve stratifying or segmenting the predictor space into a number of simple regions.\nSince the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision-tree methods."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pros-and-cons",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pros-and-cons",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pros and Cons",
    "text": "Pros and Cons\n\nTree-based methods are simple and useful for interpretation.\nHowever, they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.\nHence we also discuss bagging, random forests, and boosting. These methods grow multiple trees which are then combined to yield a single consensus prediction.\nCombining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Basics of Decision Trees",
    "text": "The Basics of Decision Trees\n\nDecision trees can be applied to both regression and classification problems.\nWe first consider regression problems, and then move on to classification."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-data-how-would-you-stratify-it",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-data-how-would-you-stratify-it",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball salary data: how would you stratify it?",
    "text": "Baseball salary data: how would you stratify it?\nSalary is color-coded from low (blue, green) to high (yellow, red)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#decision-tree-for-these-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#decision-tree-for-these-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Decision tree for these data",
    "text": "Decision tree for these data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nFor the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year.\nAt a given internal node, the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(\\text{Years} &lt; 4.5\\), and the right-hand branch corresponds to \\(\\text{Years} \\geq 4.5\\).\nThe tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\n\nOverall, the tree stratifies or segments the players into three regions of predictor space: \\(R_1 = \\{X \\ | \\ \\text{Years} &lt; 4.5\\}\\), \\(R_2 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} &lt; 117.5\\}\\), and \\(R_3 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} \\geq 117.5\\}\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#terminology-for-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#terminology-for-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Terminology for Trees",
    "text": "Terminology for Trees\n\nIn keeping with the tree analogy, the regions \\(R_1\\), \\(R_2\\), and \\(R_3\\) are known as terminal nodes.\nDecision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nIn the Hitters tree, the two internal nodes are indicated by the text \\(\\text{Years} &lt; 4.5\\) and \\(\\text{Hits} &lt; 117.5\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#interpretation-of-results",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#interpretation-of-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nYears is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players.\nGiven that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary.\nBut among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and players who made more Hits last year tend to have higher salaries.\nSurely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of the tree-building process",
    "text": "Details of the tree-building process\n\nWe divide the predictor space — that is, the set of possible values for \\(X_1, X_2, \\dots, X_p\\) — into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nIn theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model.\nThe goal is to find boxes \\(R_1, \\dots, R_J\\) that minimize the RSS, given by\n\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^2,\n\\]\nwhere \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)-th box."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nUnfortunately, it is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes.\nFor this reason, we take a top-down, greedy approach that is known as recursive binary splitting.\nThe approach is top-down because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.\nIt is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details— Continued",
    "text": "Details— Continued\n\nWe first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j &lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS.\nNext, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.\nHowever, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.\nAgain, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions\n\nWe predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\nA five-region example of this approach is shown in the next slide."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\nTop Left: A partition of two-dimensional feature space that could not result from recursive binary splitting.\nTop Right: The output of recursive binary splitting on a two-dimensional example.\nBottom Left: A tree corresponding to the partition in the top right panel.\nBottom Right: A perspective plot of the prediction surface corresponding to that tree."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pruning a tree",
    "text": "Pruning a tree\n\nThe process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. Why?\nA smaller tree with fewer splits (that is, fewer regions \\(R_1, \\dots, R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias.\nOne possible alternative to the process described above is to grow the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.\nThis strategy will result in smaller trees, but is too short-sighted: a seemingly worthless split early on in the tree might be followed by a very good split — that is, a split that leads to a large reduction in RSS later on."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pruning a tree— continued",
    "text": "Pruning a tree— continued\n\nA better strategy is to grow a very large tree \\(T_0\\), and then prune it back in order to obtain a subtree.\nCost complexity pruning — also known as weakest link pruning — is used to do this.\nWe consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\), there corresponds a subtree \\(T \\subset T_0\\) such that\n\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i : x_i \\in R_m} \\left( y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]\nis as small as possible. Here \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\), \\(R_m\\) is the rectangle (i.e., the subset of predictor space) corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the mean of the training observations in \\(R_m\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#choosing-the-best-subtree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#choosing-the-best-subtree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the best subtree",
    "text": "Choosing the best subtree\n\nThe tuning parameter \\(\\alpha\\) controls a trade-off between the subtree’s complexity and its fit to the training data.\nWe select an optimal value \\(\\hat{\\alpha}\\) using cross-validation.\nWe then return to the full data set and obtain the subtree corresponding to \\(\\hat{\\alpha}\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-tree-algorithm",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-tree-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: tree algorithm",
    "text": "Summary: tree algorithm\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse K-fold cross-validation to choose \\(\\alpha\\). For each \\(k = 1, \\dots, K\\):\n3.1 Repeat Steps 1 and 2 on the \\(\\frac{K-1}{K}\\)-th fraction of the training data, excluding the \\(k\\)-th fold. 3.2 Evaluate the mean squared prediction error on the data in the left-out \\(k\\)-th fold, as a function of \\(\\alpha\\).\nAverage the results, and pick \\(\\alpha\\) to minimize the average error.\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued\n\nFirst, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set.\nWe then built a large regression tree on the training data and varied \\(\\alpha\\) in order to create subtrees with different numbers of terminal nodes.\nFinally, we performed six-fold cross-validation in order to estimate the cross-validated MSE of the trees as a function of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nVery similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of classification trees",
    "text": "Details of classification trees\n\nJust as in the regression setting, we use recursive binary splitting to grow a classification tree.\nIn the classification setting, RSS cannot be used as a criterion for making the binary splits.\nA natural alternative to RSS is the classification error rate. This is simply the fraction of the training observations in that region that do not belong to the most common class:\n\n\\[\nE = 1 - \\max_k(\\hat{p}_{mk}).\n\\]\nHere \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)-th region that are from the \\(k\\)-th class.\n\nHowever, classification error is not sufficiently sensitive for tree-growing, and in practice, two other measures are preferable."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gini-index-and-deviance",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gini-index-and-deviance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gini index and Deviance",
    "text": "Gini index and Deviance\n\nThe Gini index is defined by\n\n\\[\nG = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk}),\n\\]\na measure of total variance across the \\(K\\) classes. The Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one.\n\nFor this reason, the Gini index is referred to as a measure of node purity — a small value indicates that a node contains predominantly observations from a single class.\nAn alternative to the Gini index is cross-entropy, given by\n\n\\[\nD = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}.\n\\]\n\nIt turns out that the Gini index and the cross-entropy are very similar numerically."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: heart data",
    "text": "Example: heart data\n\nThese data contain a binary outcome HD for 303 patients who presented with chest pain.\nAn outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease.\nThere are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.\nCross-validation yields a tree with six terminal nodes. See next figure."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Trees Versus Linear Models",
    "text": "Trees Versus Linear Models\n\nTop Row: True linear boundary; Bottom row: true non-linear boundary.\nLeft column: Linear model; Right column: Tree-based model."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advantages and Disadvantages of Trees",
    "text": "Advantages and Disadvantages of Trees\nXXX Check original slide for arrows XXX\n\nTrees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\nSome people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.\nTrees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\nTrees can easily handle qualitative predictors without the need to create dummy variables.\nUnfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\n\nHowever, by aggregating many decision trees, the predictive performance of trees can be substantially improved. We introduce these concepts next."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.\nRecall that given a set of \\(n\\) independent observations \\(Z_1, \\dots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean \\(\\bar{Z}\\) of the observations is given by \\(\\sigma^2 / n\\).\nIn other words, averaging a set of observations reduces variance. Of course, this is not practical because we generally do not have access to multiple training sets."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging— continued",
    "text": "Bagging— continued\n\nInstead, we can bootstrap, by taking repeated samples from the (single) training data set.\nIn this approach, we generate \\(B\\) different bootstrapped training data sets. We then train our method on the \\(b\\)-th bootstrapped training set in order to get \\(\\hat{f}^*_b(x)\\), the prediction at a point \\(x\\). We then average all the predictions to obtain\n\n\\[\n\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^*_b(x).\n\\]\nThis is called bagging."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging classification trees",
    "text": "Bagging classification trees\n\nThe above prescription applied to regression trees.\nFor classification trees: for each test observation, we record the class predicted by each of the \\(B\\) trees, and take a majority vote: the overall prediction is the most commonly occurring class among the \\(B\\) predictions."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-the-heart-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-the-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging the heart data",
    "text": "Bagging the heart data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\nBagging and random forest results for the Heart data.\n\nThe test error (black and orange) is shown as a function of \\(B\\), the number of bootstrapped training sets used.\nRandom forests were applied with \\(m = \\sqrt{p}\\).\nThe dashed line indicates the test error resulting from a single classification tree.\nThe green and blue traces show the OOB error, which in this case is considerably lower."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#out-of-bag-error-estimation",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#out-of-bag-error-estimation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Out-of-Bag Error Estimation",
    "text": "Out-of-Bag Error Estimation\n\nIt turns out that there is a very straightforward way to estimate the test error of a bagged model.\nRecall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.\nThe remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.\nWe can predict the response for the \\(i\\)th observation using each of the trees in which that observation was OOB. This will yield around \\(B/3\\) predictions for the \\(i\\)th observation, which we average.\nThis estimate is essentially the LOO cross-validation error for bagging, if \\(B\\) is large."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Random Forests",
    "text": "Random Forests\n\nRandom forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. This reduces the variance when we average the trees.\nAs in bagging, we build a number of decision trees on bootstrapped training samples.\nBut when building these decision trees, each time a split in a tree is considered, a random selection of \\(m\\) predictors is chosen as split candidates from the full set of \\(p\\) predictors. The split is allowed to use only one of those \\(m\\) predictors.\nA fresh selection of \\(m\\) predictors is taken at each split, and typically we choose \\(m \\approx \\sqrt{p}\\) — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Gene Expression Data",
    "text": "Example: Gene Expression Data\n\nWe applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients.\nThere are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions.\nEach of the patient samples has a qualitative label with 15 different levels: either normal or one of 14 different types of cancer.\nWe use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.\nWe randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables \\(m\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results-gene-expression-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results: Gene Expression Data",
    "text": "Results: Gene Expression Data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-3",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nResults from random forests for the fifteen-class gene expression data set with \\(p = 500\\) predictors.\nThe test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of \\(m\\), the number of predictors available for splitting at each interior tree node.\nRandom forests (\\(m &lt; p\\)) lead to a slight improvement over bagging (\\(m = p\\)). A single classification tree has an error rate of 45.7%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting",
    "text": "Boosting\n\nLike bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. We only discuss boosting for decision trees.\nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\nNotably, each tree is built on a bootstrap data set, independent of the other trees.\nBoosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting Algorithm for Regression Trees",
    "text": "Boosting Algorithm for Regression Trees\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set.\nFor \\(b = 1, 2, \\dots, B\\), repeat:\n2.1 Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits (\\(d + 1\\) terminal nodes) to the training data \\((X, r)\\).\n2.2 Update \\(\\hat{f}\\) by adding in a shrunken version of the new tree:\n\n\\[\n   \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x).\n\\]\n 2.3 Update the residuals,\n\\[\n   r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i).\n\\]\n\nOutput the boosted model,\n\n\\[\n\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x).\n\\]"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the idea behind this procedure?",
    "text": "What is the idea behind this procedure?\n\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly.\nGiven the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.\nEach of these trees can be rather small, with just a few terminal nodes, determined by the parameter \\(d\\) in the algorithm.\nBy fitting small trees to the residuals, we slowly improve \\(\\hat{f}\\) in areas where it does not perform well. The shrinkage parameter \\(\\lambda\\) slows the process down even further, allowing more and different shaped trees to attack the residuals."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-for-classification",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting for classification",
    "text": "Boosting for classification\n\nBoosting for classification is similar in spirit to boosting for regression, but is a bit more complex. We will not go into detail here, nor do we in the text book.\nStudents can learn about the details in Elements of Statistical Learning, chapter 10.\nThe R package gbm (gradient boosted models) handles a variety of regression and classification problems."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gene-expression-data-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gene-expression-data-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gene expression data continued",
    "text": "Gene expression data continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-4",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\nThe test error is displayed as a function of the number of trees. For the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tuning-parameters-for-boosting",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tuning-parameters-for-boosting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tuning Parameters for Boosting",
    "text": "Tuning Parameters for Boosting\n\nThe number of trees \\(B\\). Unlike bagging and random forests, boosting can overfit if \\(B\\) is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select \\(B\\).\nThe shrinkage parameter \\(\\lambda\\). A small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small \\(\\lambda\\) can require using a very large value of \\(B\\) in order to achieve good performance.\nThe number of splits \\(d\\) in each tree, which controls the complexity of the boosted ensemble. Often \\(d = 1\\) works well, in which case each tree is a stump, consisting of a single split and resulting in an additive model. More generally \\(d\\) is the interaction depth, and controls the interaction order of the boosted model, since \\(d\\) splits can involve at most \\(d\\) variables."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-regression-example",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-regression-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Regression Example",
    "text": "Another Regression Example\n\nFrom Elements of Statistical Learning, chapter 15."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-classification-example",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-classification-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Classification Example",
    "text": "Another Classification Example\n\nFrom Elements of Statistical Learning, chapter 15."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Variable Importance Measure",
    "text": "Variable Importance Measure\n\nFor bagged/RF regression trees:\n\nRecord the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees.\nA large value indicates an important predictor.\n\nFor bagged/RF classification trees:\n\nAdd up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all \\(B\\) trees.\n\n\n\nVariable Importance Plot for the Heart Data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nDecision trees are simple and interpretable models for regression and classification.\nHowever, they are often not competitive with other methods in terms of prediction accuracy.\nBagging, random forests, and boosting are effective methods for improving the prediction accuracy of trees:\n\nThey work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees.\n\nRandom forests and boosting are among the state-of-the-art methods for supervised learning, though their results can be difficult to interpret."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#overview",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nHere we approach the two-class classification problem in a direct way:\nWe try and find a plane that separates the classes in feature space.\nIf we cannot, we get creative in two ways:\n\nWe soften what we mean by “separates”, and\nWe enrich and enlarge the feature space so that separation is possible."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#what-is-a-hyperplane",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#what-is-a-hyperplane",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a Hyperplane?",
    "text": "What is a Hyperplane?\n\nA hyperplane in \\(p\\) dimensions is a flat affine subspace of dimension \\(p - 1\\).\nIn general, the equation for a hyperplane has the form\n\n\\[\n  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p = 0\n\\]\n\nIn \\(p = 2\\) dimensions, a hyperplane is a line.\nIf \\(\\beta_0 = 0\\), the hyperplane goes through the origin; otherwise, it does not.\nThe vector \\(\\beta = (\\beta_1, \\beta_2, \\dots, \\beta_p)\\) is called the normal vector — it points in a direction orthogonal to the surface of the hyperplane."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hyperplane in 2 Dimensions",
    "text": "Hyperplane in 2 Dimensions"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Separating Hyperplanes",
    "text": "Separating Hyperplanes\n\n\nIf \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\), then \\(f(X) &gt; 0\\) for points on one side of the hyperplane, and \\(f(X) &lt; 0\\) for points on the other.\nIf we code the colored points as \\(Y_i = +1\\) for blue and \\(Y_i = -1\\) for mauve, then if \\(Y_i \\cdot f(X_i) &gt;0\\) for all \\(i\\), \\(f(X) = 0\\) defines a separating hyperplane."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#maximal-margin-classifier",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#maximal-margin-classifier",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier\nAmong all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.\nConstrained optimization problem:\n\\[\n\\text{maximize } M \\quad \\beta_0, \\beta_1, \\dots, \\beta_p\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\beta_j^2 = 1,\n\\] \\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M \\quad \\text{for all } i = 1, \\dots, N.\n\\]\n\nThis can be rephrased as a convex quadratic program and solved efficiently. The function svm() in the e1071 package solves this problem efficiently."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#non-separable-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#non-separable-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-separable Data",
    "text": "Non-separable Data\n\nThe data on the left are not separable by a linear boundary.\nThis is often the case, unless \\(N &lt; p\\)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#noisy-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#noisy-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Noisy Data",
    "text": "Noisy Data\n\nSometimes the data are separable, but noisy. This can lead to a poor solution for the maximal-margin classifier.\nThe support vector classifier maximizes a soft margin."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\n\n\\[\n\\text{maximize } M \\quad \\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\beta_j^2 = 1,\n\\]\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\epsilon_i),\n\\]\n\\[\n\\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n \\epsilon_i \\leq C.\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C\\) is a Regularization Parameter",
    "text": "\\(C\\) is a Regularization Parameter\n\nThis layout demonstrates the effect of the regularization parameter \\(C\\) on the SVM classifier with four subplots, each representing a different \\(C\\) value. Adjustments to annotations or layout can be made as needed. Let me know if further modifications are required!"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Boundary Can Fail",
    "text": "Linear Boundary Can Fail\n\nSometimes a linear boundary simply won’t work, no matter what value of \\(C\\).\nThe example on the left is such a case.\nWhat to do?"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#feature-expansion",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#feature-expansion",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Feature Expansion",
    "text": "Feature Expansion\n\nEnlarge the space of features by including transformations; e.g. \\(X_1^2, X_1^3, X_1X_2, X_1X_2^2, \\dots\\). Hence go from a \\(p\\)-dimensional space to a \\(M &gt; p\\)-dimensional space.\nFit a support-vector classifier in the enlarged space.\nThis results in non-linear decision boundaries in the original space.\n\nExample:\nSuppose we use \\((X_1, X_2, X_1^2, X_2^2, X_1X_2)\\) instead of just \\((X_1, X_2)\\). Then the decision boundary would be of the form:\n\\[\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1X_2 = 0\n\\]\nThis leads to nonlinear decision boundaries in the original space (quadratic conic sections)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#cubic-polynomials",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#cubic-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Polynomials",
    "text": "Cubic Polynomials\nHere we use a basis expansion of cubic polynomials:\n\nFrom 2 variables to 9.\nThe support-vector classifier in the enlarged space solves the problem in the lower-dimensional space.\n\n\n\\[\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\beta_6 X_1^3 + \\beta_7 X_2^3 + \\beta_8 X_1^2 X_2 + \\beta_9 X_1 X_2^2 = 0\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nonlinearities and Kernels",
    "text": "Nonlinearities and Kernels\n\nPolynomials (especially high-dimensional ones) get wild rather fast.\nThere is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers — through the use of kernels.\nBefore we discuss these, we must understand the role of inner products in support-vector classifiers."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products-and-support-vectors",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products-and-support-vectors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Inner Products and Support Vectors",
    "text": "Inner Products and Support Vectors\n\\[\n\\langle x_i, x_{i'} \\rangle = \\sum_{j=1}^p x_{ij} x_{i'j} \\quad \\text{— inner product between vectors}\n\\]\n\nThe linear support vector classifier can be represented as:\n\n\\[\n  f(x) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle x, x_i \\rangle \\quad \\text{— $n$ parameters}\n\\]\n\nTo estimate the parameters \\(\\alpha_1, \\dots, \\alpha_n\\) and \\(\\beta_0\\), all we need are the \\(\\binom{n}{2}\\) inner products \\(\\langle x_i, x_{i'} \\rangle\\) between all pairs of training observations.\n\nIt turns out that most of the \\(\\hat{\\alpha}_i\\) can be zero:\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i \\langle x, x_i \\rangle\n\\]\n\\(\\mathbf{S}\\) is the support set of indices \\(i\\) such that \\(\\hat{\\alpha}_i &gt; 0\\). [See slide 8]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#kernels-and-support-vector-machines",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#kernels-and-support-vector-machines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Kernels and Support Vector Machines",
    "text": "Kernels and Support Vector Machines\n\nIf we can compute inner products between observations, we can fit a SV classifier. Can be quite abstract!\nSome special kernel functions can do this for us. E.g., \\[\nK(x_i, x_{i'}) = \\left(1 + \\sum_{j=1}^p x_{ij} x_{i'j}\\right)^d\n\\] computes the inner products needed for \\(d\\)-dimensional polynomials — \\(\\binom{p+d}{d}\\) basis functions!\nTry it for \\(p = 2\\) and \\(d = 2\\).\nThe solution has the form: \\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i K(x, x_i).\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#radial-kernel",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#radial-kernel",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Radial Kernel",
    "text": "Radial Kernel\n\\[\nK(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 \\right).\n\\]\n\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i K(x, x_i).\n\\]\n\nImplicit feature space; very high dimensional.\nControls variance by squashing down most dimensions severely."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\n\nROC curve is obtained by changing the threshold \\(t\\) in \\(\\hat{f}(X) &gt; t\\), and recording false positive and true positive rates as \\(t\\) varies. Here we see ROC curves on training data."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-continued-heart-test-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-continued-heart-test-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued: Heart Test Data",
    "text": "Example Continued: Heart Test Data"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "SVMs: More Than 2 Classes?",
    "text": "SVMs: More Than 2 Classes?\nThe SVM as defined works for \\(K = 2\\) classes. What do we do if we have \\(K &gt; 2\\) classes?\nOVA\nOne versus All. Fit \\(K\\) different 2-class SVM classifiers \\(\\hat{f}_k(x)\\), \\(k = 1, \\dots, K\\); each class versus the rest. Classify \\(x^*\\) to the class for which \\(\\hat{f}_k(x^*)\\) is largest.\nOVO\nOne versus One. Fit all \\(\\binom{K}{2}\\) pairwise classifiers \\(\\hat{f}_{k\\ell}(x)\\). Classify \\(x^*\\) to the class that wins the most pairwise competitions.\nWhich to choose? If \\(K\\) is not too large, use OVO."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector versus Logistic Regression?",
    "text": "Support Vector versus Logistic Regression?\nWith \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\), we can rephrase support-vector classifier optimization as:\n\\[\n\\text{minimize}_{\\beta_0, \\beta_1, \\dots, \\beta_p} \\left\\{ \\sum_{i=1}^n \\max \\big[ 0, 1 - y_i f(x_i) \\big] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]\n\nThis has the form loss plus penalty.\n\nThe loss is known as the hinge loss.\n\nVery similar to the loss in logistic regression (negative log-likelihood)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#which-to-use-svm-or-logistic-regression",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#which-to-use-svm-or-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Which to Use: SVM or Logistic Regression",
    "text": "Which to Use: SVM or Logistic Regression\n\nWhen classes are (nearly) separable, SVM does better than LR. So does LDA.\nWhen not, LR (with ridge penalty) and SVM are very similar.\nIf you wish to estimate probabilities, LR is the choice.\nFor nonlinear boundaries, kernel SVMs are popular. Can use kernels with LR and LDA as well, but computations are more expensive."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#summary",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nNeural networks became popular in the 1980s.\nLots of successes, hype, and great conferences: NeurIPS, Snowbird.\nThen along came SVMs, Random Forests, and Boosting in the 1990s, and Neural Networks took a back seat.\nRe-emerged around 2010 as Deep Learning.\nBy 2020s, very dominant and successful.\nPart of success due to vast improvements in computing power, larger training sets, and software: TensorFlow and PyTorch.\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\\[\nf(X) = \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j).\n\\]\nDiagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#details",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details",
    "text": "Details\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer.\n\\(g(z)\\) is called the activation function. Popular examples are the sigmoid and rectified linear (ReLU), shown in the figure.\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: MNIST Digits",
    "text": "Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#details-of-output-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#details-of-output-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Output Layer",
    "text": "Details of Output Layer\n\nLet \\(Z_m = \\beta_{m0} + \\sum_{\\ell=1}^{K_2} \\beta_{m\\ell} A_\\ell^{(2)}\\), \\(m = 0, 1, \\ldots, 9\\), be 10 linear combinations of activations at the second layer.\nOutput activation function encodes the softmax function:\n\n\\[\nf_m(X) = \\Pr(Y = m \\mid X) = \\frac{e^{Z_m}}{\\sum_{\\ell=0}^{9} e^{Z_\\ell}}.\n\\]\n\nWe fit the model by minimizing the negative multinomial log-likelihood (or cross-entropy):\n\n\\[\n-\\sum_{i=1}^{n} \\sum_{m=0}^{9} y_{im} \\log(f_m(x_i)).\n\\]\n\n\\(y_{im}\\) is 1 if the true class for observation \\(i\\) is \\(m\\), else 0 — i.e., one-hot encoded."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#results",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\n\n\n\nMethod\nTest Error\n\n\n\n\nNeural Network + Ridge Regularization\n2.3%\n\n\nNeural Network + Dropout Regularization\n1.8%\n\n\nMultinomial Logistic Regression\n7.2%\n\n\nLinear Discriminant Analysis\n12.7%\n\n\n\n\nEarly success for neural networks in the 1990s.\nWith so many parameters, regularization is essential.\nSome details of regularization and fitting will come later.\nVery overworked problem — best reported rates are \\(&lt; 0.5\\%\\)!\nHuman error rate is reported to be around \\(0.2\\%\\), or 20 of the 10K test images."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolutional Neural Network — CNN",
    "text": "Convolutional Neural Network — CNN\n\nMajor success story for classifying images.\nShown are samples from CIFAR100 database: \\(32 \\times 32\\) color natural images, with 100 classes.\n\\(50K\\) training images, \\(10K\\) test images.\nEach image is a three-dimensional array or feature map:\n\\(32 \\times 32 \\times 3\\) array of 8-bit numbers.\nThe last dimension represents the three color channels for red, green, and blue."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#how-cnns-work",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#how-cnns-work",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How CNNs Work",
    "text": "How CNNs Work\n\n\nThe CNN builds up an image in a hierarchical fashion.\nEdges and shapes are recognized and pieced together to form more complex shapes, eventually assembling the target image.\nThis hierarchical construction is achieved using convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-filter",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-filter",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Filter",
    "text": "Convolution Filter\n\\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\n\\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\n\nThe filter is itself an image and represents a small shape, edge, etc.\nWe slide it around the input image, scoring for matches.\nThe scoring is done via dot-products, illustrated above.\nIf the subimage of the input image is similar to the filter, the score is high; otherwise, it is low.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling",
    "text": "Pooling\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#using-pretrained-networks-to-classify-images",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#using-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Using Pretrained Networks to Classify Images",
    "text": "Using Pretrained Networks to Classify Images\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nWe wish to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Lasso versus Neural Network — IMDB Reviews",
    "text": "Lasso versus Neural Network — IMDB Reviews\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\nglmnet was used to fit the lasso model, and is very effective because it can exploit sparsity in the \\(\\mathbf{X}\\) matrix."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\nHandwriting, such as doctor’s notes.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-and-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-and-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN and IMDB Reviews",
    "text": "RNN and IMDB Reviews\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding",
    "text": "Word Embedding\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-on-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-on-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN on IMDB Reviews",
    "text": "RNN on IMDB Reviews\n\nAfter a lot of work, the results are a disappointing 76% accuracy.\nWe then fit a more exotic RNN than the one displayed — a LSTM with long and short term memory. Here \\(A_\\ell\\) receives input from \\(A_{\\ell-1}\\) (short term memory) as well as from a version that reaches further back in time (long term memory). Now we get 87% accuracy, slightly less than the 88% achieved by glmnet.\nThese data have been used as a benchmark for new RNN architectures. The best reported result found at the time of writing (2020) was around 95%. We point to a leaderboard in Section 10.5.1."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#time-series-forecasting",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Time Series Forecasting",
    "text": "Time Series Forecasting"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#new-york-stock-exchange-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#new-york-stock-exchange-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "New-York Stock Exchange Data",
    "text": "New-York Stock Exchange Data\nShown in the previous slide are three daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences\n\\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\nFigure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\] \\[\nR^2 = 0.18 \\text{ for straw man — use yesterday’s value of Log trading volume to predict that of today.}\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\\[\nR^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\n\\]\n\\[\nR^2 = 0.42 \\text{ for RNN model (205 parameters)}\n\\]\n\\[\nR^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\n\\]\n\\[\nR^2 = 0.46 \\text{ for all models if we include } \\textbf{day\\_of\\_week} \\text{ of day being predicted.}\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, the linear model fit by glmnet did as well as the neural network, and better than the RNN.\n\nWe endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\n\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector:\n\\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or\n\\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\]\nwhere \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\nAt each SGD update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests (Chapter 8)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\n\nWhat happened to overfitting and the usual bias-variance trade-off?\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#software",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#software",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Software",
    "text": "Software\n\nWonderful software available for neural networks and deep learning. Tensorflow from Google and PyTorch from Facebook. Both are Python packages.\nIn the Chapter 10 lab we demonstrate tensorflow and keras packages in R, which interface to Python. See textbook and online resources for Rmarkdown and Jupyter notebooks for these and all labs for the second edition of ISLR book.\nThe torch package in R is available as well, and implements the PyTorch dialect. The Chapter 10 lab will be available in this dialect as well; watch the resources page at www.statlearning.com."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#overview",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nXXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUnsupervised vs Supervised Learning:\n\nMost of this course focuses on supervised learning methods such as regression and classification.\nIn that setting we observe both a set of features \\(X_1, X_2, \\ldots, X_p\\) for each object, as well as a response or outcome variable \\(Y\\). The goal is then to predict \\(Y\\) using \\(X_1, X_2, \\ldots, X_p\\).\nHere we instead focus on unsupervised learning, where we observe only the features \\(X_1, X_2, \\ldots, X_p\\). We are not interested in prediction, because we do not have an associated response variable \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-goals-of-unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-goals-of-unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Goals of Unsupervised Learning",
    "text": "The Goals of Unsupervised Learning\n\nThe goal is to discover interesting things about the measurements: is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations?\nWe discuss two methods:\n\nprincipal components analysis, a tool used for data visualization or data pre-processing before supervised techniques are applied, and\nclustering, a broad class of methods for discovering unknown subgroups in data."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-challenge-of-unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-challenge-of-unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Challenge of Unsupervised Learning",
    "text": "The Challenge of Unsupervised Learning\n\nUnsupervised learning is more subjective than supervised learning, as there is no simple goal for the analysis, such as prediction of a response.\nBut techniques for unsupervised learning are of growing importance in a number of fields:\n\nsubgroups of breast cancer patients grouped by their gene expression measurements,\ngroups of shoppers characterized by their browsing and purchase histories,\nmovies grouped by the ratings assigned by movie viewers."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-advantage",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-advantage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another advantage",
    "text": "Another advantage\n\nIt is often easier to obtain unlabeled data — from a lab instrument or a computer — than labeled data, which can require human intervention.\nFor example, it is difficult to automatically assess the overall sentiment of a movie review: is it favorable or not?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\nPCA produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nApart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-details",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis: details",
    "text": "Principal Components Analysis: details\n\nThe first principal component of a set of features \\(X_1, X_2, \\ldots, X_p\\) is the normalized linear combination of the features\n\n\\[\n  Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\n\\]\nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\).\n\nWe refer to the elements \\(\\phi_{11}, \\ldots, \\phi_{p1}\\) as the loadings of the first principal component; together, the loadings make up the principal component loading vector,\n\n\\[\n  \\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})^T.\n\\]\n\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA: example",
    "text": "PCA: example\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component direction, and the blue dashed line indicates the second principal component direction."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computation of Principal Components",
    "text": "Computation of Principal Components\n\nSuppose we have a \\(n \\times p\\) data set \\(\\mathbf{X}\\). Since we are only interested in variance, we assume that each of the variables in \\(\\mathbf{X}\\) has been centered to have mean zero (that is, the column means of \\(\\mathbf{X}\\) are zero).\nWe then look for the linear combination of the sample feature values of the form \\[\nz_{i1} = \\phi_{11}x_{i1} + \\phi_{21}x_{i2} + \\ldots + \\phi_{p1}x_{ip} \\tag{1}\n\\]\nfor \\(i = 1, \\ldots, n\\) that has the largest sample variance, subject to the constraint that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\).\nSince each of the \\(x_{ij}\\) has mean zero, then so does \\(z_{i1}\\) (for any values of \\(\\phi_{j1}\\)). Hence the sample variance of the \\(z_{i1}\\) can be written as\n\n\\[\n  \\frac{1}{n} \\sum_{i=1}^n z_{i1}^2.\n\\]"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computation: continued",
    "text": "Computation: continued\n\nPlugging in (1) the first principal component loading vector solves the optimization problem\n\n\\[\n  \\text{maximize}_{\\phi_{11}, \\ldots, \\phi_{p1}} \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1} x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p \\phi_{j1}^2 = 1.\n\\]\n\nThis problem can be solved via a singular-value decomposition of the matrix \\(\\mathbf{X}\\), a standard technique in linear algebra.\nWe refer to \\(Z_1\\) as the first principal component, with realized values \\(z_{11}, \\ldots, z_{n1}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#geometry-of-pca",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#geometry-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Geometry of PCA",
    "text": "Geometry of PCA\n\nThe loading vector \\(\\phi_1\\) with elements \\(\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1}\\) defines a direction in feature space along which the data vary the most.\nIf we project the \\(n\\) data points \\(x_1, \\ldots, x_n\\) onto this direction, the projected values are the principal component scores \\(z_{11}, \\ldots, z_{n1}\\) themselves."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Further principal components",
    "text": "Further principal components\n\nThe second principal component is the linear combination of \\(X_1, \\ldots, X_p\\) that has maximal variance among all linear combinations that are uncorrelated with \\(Z_1\\).\nThe second principal component scores \\(z_{12}, z_{22}, \\ldots, z_{n2}\\) take the form\n\n\\[\n  z_{i2} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + \\ldots + \\phi_{p2}x_{ip},\n\\]\nwhere \\(\\phi_2\\) is the second principal component loading vector, with elements \\(\\phi_{12}, \\phi_{22}, \\ldots, \\phi_{p2}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Further principal components: continued",
    "text": "Further principal components: continued\n\nIt turns out that constraining \\(Z_2\\) to be uncorrelated with \\(Z_1\\) is equivalent to constraining the direction \\(\\phi_2\\) to be orthogonal (perpendicular) to the direction \\(\\phi_1\\). And so on.\nThe principal component directions \\(\\phi_1, \\phi_2, \\phi_3, \\ldots\\) are the ordered sequence of right singular vectors of the matrix \\(\\mathbf{X}\\), and the variances of the components are \\(\\frac{1}{n}\\) times the squares of the singular values. There are at most \\(\\min(n - 1, p)\\) principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#illustration",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#illustration",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration",
    "text": "Illustration\n\nUSArrests data: For each of the fifty states in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. We also record UrbanPop (the percent of the population in each state living in urban areas).\nThe principal component score vectors have length \\(n = 50\\), and the principal component loading vectors have length \\(p = 4\\).\nPCA was performed after standardizing each variable to have mean zero and standard deviation one."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#usarrests-data-pca-plot",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#usarrests-data-pca-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "USArrests data: PCA plot",
    "text": "USArrests data: PCA plot\n\nThe PCA plot shows the first and second principal components for the USArrests dataset, with arrows indicating the loadings for the variables UrbanPop, Rape, Assault, and Murder. State names are displayed based on their principal component scores."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#figure-details",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#figure-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Figure details",
    "text": "Figure details\nThe first two principal components for the USArrests data.\n\nThe blue state names represent the scores for the first two principal components.\nThe orange arrows indicate the first two principal component loading vectors (with axes on the top and right). For example, the loading for Rape on the first component is 0.54, and its loading on the second principal component is 0.17 [the word Rape is centered at the point (0.54, 0.17)].\nThis figure is known as a biplot, because it displays both the principal component scores and the principal component loadings."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-loadings",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-loadings",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA loadings",
    "text": "PCA loadings\n\n\n\n\nPC1\nPC2\n\n\n\n\nMurder\n0.5358995\n-0.4181809\n\n\nAssault\n0.5831836\n-0.1879856\n\n\nUrbanPop\n0.2781909\n0.8728062\n\n\nRape\n0.5434321\n0.1673186"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Interpretation of Principal Components",
    "text": "Another Interpretation of Principal Components\n\nThe left plot shows a 3D visualization with projections onto the first two principal components, while the right plot displays the data in 2D using the first and second principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-find-the-hyperplane-closest-to-the-observations",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-find-the-hyperplane-closest-to-the-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA find the hyperplane closest to the observations",
    "text": "PCA find the hyperplane closest to the observations\n\nThe first principal component loading vector has a very special property: it defines the line in \\(p\\)-dimensional space that is closest to the \\(n\\) observations (using average squared Euclidean distance as a measure of closeness).\nThe notion of principal components as the dimensions that are closest to the \\(n\\) observations extends beyond just the first principal component.\nFor instance, the first two principal components of a data set span the plane that is closest to the \\(n\\) observations, in terms of average squared Euclidean distance."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scaling of the variables matters",
    "text": "Scaling of the variables matters\n\nIf the variables are in different units, scaling each to have standard deviation equal to one is recommended.\nIf they are in the same units, you might or might not scale the variables."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained",
    "text": "Proportion Variance Explained\n\nTo understand the strength of each component, we are interested in knowing the proportion of variance explained (PVE) by each one.\nThe total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as\n\n\\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{j=1}^p \\frac{1}{n} \\sum_{i=1}^n x_{ij}^2,\n\\]\nand the variance explained by the \\(m\\)-th principal component is\n\\[\n  \\text{Var}(Z_m) = \\frac{1}{n} \\sum_{i=1}^n z_{im}^2.\n\\]\n\nIt can be shown that\n\n\\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{m=1}^M \\text{Var}(Z_m),\n\\]\nwith \\(M = \\min(n-1, p)\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained: continued",
    "text": "Proportion Variance Explained: continued\n\nTherefore, the PVE of the \\(m\\)-th principal component is given by the positive quantity between 0 and 1:\n\n\\[\n  \\frac{\\sum_{i=1}^n z_{im}^2}{\\sum_{j=1}^p \\sum_{i=1}^n x_{ij}^2}.\n\\]\n\nThe PVEs sum to one. We sometimes display the cumulative PVEs."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-many-principal-components-should-we-use",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-many-principal-components-should-we-use",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How many principal components should we use?",
    "text": "How many principal components should we use?\nIf we use principal components as a summary of our data, how many components are sufficient?\n\nNo simple answer to this question, as cross-validation is not available for this purpose.\n\nWhy not?\nWhen could we use cross-validation to select the number of components?\n\nThe “scree plot” on the previous slide can be used as a guide: we look for an “elbow”."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Clustering",
    "text": "Clustering\n\nClustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.\nWe seek a partition of the data into distinct groups so that the observations within each group are quite similar to each other.\nTo make this concrete, we must define what it means for two or more observations to be similar or different.\nIndeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-vs-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-vs-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA vs Clustering",
    "text": "PCA vs Clustering\n\nPCA looks for a low-dimensional representation of the observations that explains a good fraction of the variance.\nClustering looks for homogeneous subgroups among the observations."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-for-market-segmentation",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-for-market-segmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Clustering for Market Segmentation",
    "text": "Clustering for Market Segmentation\n\nSuppose we have access to a large number of measurements (e.g., median household income, occupation, distance from nearest urban area, and so forth) for a large number of people.\nOur goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\nThe task of performing market segmentation amounts to clustering the people in the data set."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#two-clustering-methods",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#two-clustering-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Two clustering methods",
    "text": "Two clustering methods\n\nIn K-means clustering, we seek to partition the observations into a pre-specified number of clusters.\nIn hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to \\(n\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nA simulated data set with 150 observations in 2-dimensional space. Panels show the results of applying K-means clustering with different values of \\(K\\), the number of clusters. The color of each observation indicates the cluster to which it was assigned using the K-means clustering algorithm. Note that there is no ordering of the clusters, so the cluster coloring is arbitrary. These cluster labels were not used in clustering; instead, they are the outputs of the clustering procedure."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of K-means clustering",
    "text": "Details of K-means clustering\nLet \\(C_1, \\ldots, C_K\\) denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n\n\\(C_1 \\cup C_2 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}\\). In other words, each observation belongs to at least one of the \\(K\\) clusters.\n\\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\neq k'\\). In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n\nFor instance, if the \\(i\\)-th observation is in the \\(k\\)-th cluster, then \\(i \\in C_k\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of K-means clustering: continued",
    "text": "Details of K-means clustering: continued\n\nThe idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible.\nThe within-cluster variation for cluster \\(C_k\\) is a measure \\(\\text{WCV}(C_k)\\) of the amount by which the observations within a cluster differ from each other.\nHence, we want to solve the problem:\n\n\\[\n  \\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^K \\text{WCV}(C_k) \\right\\}.\n\\]\n\nIn words, this formula says that we want to partition the observations into \\(K\\) clusters such that the total within-cluster variation, summed over all \\(K\\) clusters, is as small as possible."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-to-define-within-cluster-variation",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-to-define-within-cluster-variation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Define Within-Cluster Variation?",
    "text": "How to Define Within-Cluster Variation?\n\nTypically, we use Euclidean distance:\n\n\\[\n  \\text{WCV}(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2, \\tag{3}\n\\]\nwhere \\(|C_k|\\) denotes the number of observations in the \\(k\\)-th cluster.\n\nCombining (2) and (3) gives the optimization problem that defines K-means clustering:\n\n\\[\n  \\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^K \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 \\right\\}. \\tag{4}\n\\]"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Means Clustering Algorithm",
    "text": "K-Means Clustering Algorithm\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\n2.1 For each of the \\(K\\) clusters, compute the cluster centroid.\nThe \\(k\\)-th cluster centroid is the vector of the \\(p\\) feature means for the observations in the \\(k\\)-th cluster.\n2.2 Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#properties-of-the-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#properties-of-the-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Properties of the Algorithm",
    "text": "Properties of the Algorithm\n\nThis algorithm is guaranteed to decrease the value of the objective (4) at each step. Why? Note that\n\n\\[\n  \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 = 2 \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2,\n\\]\nwhere \\(\\bar{x}_{kj} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x_{ij}\\) is the mean for feature \\(j\\) in cluster \\(C_k\\).\n\nHowever, it is not guaranteed to give the global minimum. Why not?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nTop left: The observations are shown.\nTop center: In Step 1 of the algorithm, each observation is randomly assigned to a cluster.\nTop right: In Step 2(a), the cluster centroids are computed. These are shown as large colored disks. Initially, the centroids are almost completely overlapping because the initial cluster assignments were chosen at random.\nBottom left: In Step 2(b), each observation is assigned to the nearest centroid.\nBottom center: Step 2(a) is once again performed, leading to new cluster centroids.\nBottom right: The results obtained after 10 iterations."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-different-starting-values",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-different-starting-values",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Different Starting Values",
    "text": "Example: Different Starting Values"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nK-means clustering was performed six times on the data from the previous figure with \\(K = 3\\), each time with a different random assignment of the observations in Step 1 of the K-means algorithm.\nAbove each plot is the value of the objective (4).\nThree different local optima were obtained, one of which resulted in a smaller value of the objective and provides better separation between the clusters.\nThose labeled in red all achieved the same best solution, with an objective value of 235.8."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nK-means clustering requires us to pre-specify the number of clusters \\(K\\). This can be a disadvantage (later we discuss strategies for choosing \\(K\\)).\nHierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(K\\).\nIn this section, we describe bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea",
    "text": "Hierarchical Clustering: The Idea\nBuilds a hierarchy in a “bottom-up” fashion…\nStep 1"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 2)",
    "text": "Hierarchical Clustering: The Idea (Step 2)\nMerging Closest Observations"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-3",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 3)",
    "text": "Hierarchical Clustering: The Idea (Step 3)\nMerging Another Closest Pair"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-4",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 4)",
    "text": "Hierarchical Clustering: The Idea (Step 4)\nExpanding the Hierarchy"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-final-step",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-final-step",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Final Step)",
    "text": "Hierarchical Clustering: The Idea (Final Step)\nSingle Cluster Representation"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering Algorithm",
    "text": "Hierarchical Clustering Algorithm\nThe approach in words:\n\nStart with each point in its own cluster.\nIdentify the closest two clusters and merge them.\nRepeat.\nEnds when all points are in a single cluster."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#an-example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#an-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "An Example",
    "text": "An Example\n\n\n45 observations generated in 2-dimensional space.\nIn reality, there are three distinct classes, shown in separate colors.\nHowever, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-of-hierarchical-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-of-hierarchical-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application of Hierarchical Clustering",
    "text": "Application of Hierarchical Clustering\n\n\nThis slide visualizes the application of hierarchical clustering on data.\nEach dendrogram represents different stages or thresholds for determining clusters.\nThe dashed horizontal lines indicate where the data is split into distinct clusters."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nLeft: Dendrogram obtained from hierarchically clustering the data from the previous slide, using complete linkage and Euclidean distance.\nCenter: The dendrogram from the left-hand panel, cut at a height of 9 (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors.\nRight: The dendrogram from the left-hand panel, now cut at a height of 5. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of Linkage",
    "text": "Types of Linkage\n\n\n\n\n\n\n\nLinkage\nDescription\n\n\n\n\nComplete\nMaximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.\n\n\nSingle\nMinimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.\n\n\nAverage\nMean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n\n\nCentroid\nDissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#choice-of-dissimilarity-measure",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#choice-of-dissimilarity-measure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choice of Dissimilarity Measure",
    "text": "Choice of Dissimilarity Measure\n\nSo far, have used Euclidean distance.\nAn alternative is correlation-based distance, which considers two observations to be similar if their features are highly correlated.\nThis is an unusual use of correlation, which is normally computed between variables; here, it is computed between the observation profiles for each pair of observations."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#practical-issues",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#practical-issues",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Practical Issues",
    "text": "Practical Issues\n\nScaling of the variables matters! Should the observations or features first be standardized in some way?\n\nFor instance, maybe the variables should be centered to have mean zero and scaled to have a standard deviation of one.\n\nIn the case of hierarchical clustering:\n\nWhat dissimilarity measure should be used?\nWhat type of linkage should be used?\n\nHow many clusters to choose? (in both K-means or hierarchical clustering):\n\nDifficult problem.\nNo agreed-upon method. See Elements of Statistical Learning, Chapter 13, for more details.\n\nWhich features should we use to drive the clustering?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-breast-cancer-microarray-study",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-breast-cancer-microarray-study",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Breast Cancer Microarray Study",
    "text": "Example: Breast Cancer Microarray Study\n\n“Repeated observation of breast tumor subtypes in independent gene expression data sets;” Sorlie et al., PNAS 2003.\nGene expression measurements for approximately ~8000 genes, for each of 88 breast cancer patients.\nAverage linkage, correlation metric.\nClustered samples using 500 intrinsic genes:\n\nEach woman was measured before and after chemotherapy.\nIntrinsic genes have the smallest within/between variation."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#conclusions",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nUnsupervised learning is important for understanding the variation and grouping structure of a set of unlabeled data, and can be a useful pre-processor for supervised learning.\nIt is intrinsically more difficult than supervised learning because there is no gold standard (like an outcome variable) and no single objective (like test set accuracy).\nIt is an active field of research, with many recently developed tools such as self-organizing maps, independent components analysis, and spectral clustering.\nSee The Elements of Statistical Learning, chapter 14."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-and-missing-values-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-and-missing-values-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Completion and Missing Values",
    "text": "Matrix Completion and Missing Values\n\nIt is often the case that data matrices X have missing entries, often represented by NAs (not available).\nThis is a nuisance, since many of our modeling procedures, such as linear regression and GLMs, require complete data.\nSometimes imputation is the prediction problem! — as in recommender systems.\nOne simple approach is mean imputation — replace missing values for a variable by the mean of the non-missing entries.\nThis ignores the correlations among variables; we should be able to exploit these correlations when imputing missing values.\nWe assume values are missing at random; i.e., the missingness should not be informative.\nWe present an approach based on principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommender Systems",
    "text": "Recommender Systems"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommender Systems",
    "text": "Recommender Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer\nJerry Maguire\nOceans\nRoad to Perdition\nA Fortunate Man\nCatch Me If You Can\nDriving Miss Daisy\nThe Two Popes\nThe Laundromat\nCode 8\nThe Social Network\n…\n\n\n\n\nCustomer 1\n.\n.\n.\n.\n.\n.\n.\n4\n.\n.\n…\n\n\nCustomer 2\n.\n3\n.\n.\n.\n.\n.\n3\n.\n3\n…\n\n\nCustomer 3\n.\n.\n.\n2\n4\n.\n.\n.\n.\n.\n…\n\n\nCustomer 4\n3\n.\n.\n.\n.\n2\n.\n.\n.\n.\n…\n\n\nCustomer 5\n5\n1\n.\n.\n4\n.\n.\n.\n.\n.\n…\n\n\nCustomer 6\n.\n.\n.\n2\n.\n.\n4\n.\n.\n.\n…\n\n\nCustomer 7\n.\n.\n.\n.\n5\n.\n.\n.\n3\n.\n…\n\n\nCustomer 8\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n…\n\n\nCustomer 9\n3\n.\n.\n.\n.\n.\n.\n5\n.\n1\n…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\nNetflix users rate movies they have seen, usually a very small fraction of available movies.\nPredicting missing ratings provides a way to recommend movies to users. Matrix completion is one of the primary tools."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-approximation-via-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-approximation-via-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Approximation via Principal Components",
    "text": "Matrix Approximation via Principal Components\n\nIn Section 12.2.2 we gave an interpretation of principal components in terms of matrix approximation:\n\n\\[\n\\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{j=1}^p \\sum_{i=1}^n \\left( x_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\}.\n\\]\n\n\\(\\mathbf{A}\\) is an \\(n \\times M\\) matrix whose \\((i, m)\\) element is \\(a_{im}\\), and \\(\\mathbf{B}\\) is a \\(p \\times M\\) matrix whose \\((j, m)\\) element is \\(b_{jm}\\).\nIt can be shown that for any value of \\(M\\), the first \\(M\\) principal components provide a solution: \\[\n\\hat{a}_{im} = z_{im} \\quad \\text{and} \\quad \\hat{b}_{jm} = \\phi_{jm}.\n\\]\nBut what to do if the matrix has missing elements?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-via-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-via-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Completion via Principal Components",
    "text": "Matrix Completion via Principal Components\nWe pose instead a modified version of the approximation criterion:\n\\[\n\\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{(i,j) \\in \\mathcal{O}} \\left( x_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\},\n\\]\nwhere \\(\\mathcal{O}\\) is the set of all observed pairs of indices \\((i, j)\\), a subset of the possible \\(n \\times p\\) pairs.\nOnce we solve this problem:\n\nWe can estimate a missing observation \\(x_{ij}\\) using:\n\n\\[\n  \\hat{x}_{ij} = \\sum_{m=1}^M \\hat{a}_{im}\\hat{b}_{jm},\n\\]\nwhere \\(\\hat{a}_{im}\\) and \\(\\hat{b}_{jm}\\) are the \\((i,m)\\) and \\((j,m)\\) elements of the solution matrices \\(\\mathbf{\\hat{A}}\\) and \\(\\mathbf{\\hat{B}}\\).\n\nWe can (approximately) recover the \\(M\\) principal component scores and loadings, as if data were complete."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#iterative-algorithm-for-matrix-completion",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#iterative-algorithm-for-matrix-completion",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Iterative Algorithm for Matrix Completion",
    "text": "Iterative Algorithm for Matrix Completion\n\nInitialize: create a complete data matrix \\(\\tilde{\\mathbf{X}}\\) by filling in the missing values using mean imputation.\nRepeat: steps (a)–(c) until the objective in (c) fails to decrease:\n\n\n\n\n\n\n\\[\n      \\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{j=1}^p \\sum_{i=1}^n \\left( \\tilde{x}_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\},\n\\]\n  by computing the principal components of $\\tilde{\\mathbf{X}}$.\n\n- (b) For each missing entry $(i, j) \\notin \\mathcal{O}$, set:\n\\[\n      \\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{a}_{im} \\hat{b}_{jm}.\n\\]\n- (c) Compute the objective:\n\\[\n      \\sum_{(i,j) \\in \\mathcal{O}} \\left( x_{ij} - \\sum_{m=1}^M \\hat{a}_{im} \\hat{b}_{jm} \\right)^2.\n\\]\n\nReturn the estimated missing entries \\(\\tilde{x}_{ij}\\), \\((i, j) \\notin \\mathcal{O}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: USAarrests Data",
    "text": "Example: USAarrests Data\n\nHere \\(\\mathbf{X}\\) has 50 rows (states) and four columns: Murder, Assault, Rape, and UrbanPop.\n\nWe selected 20 states at random, and for each, we selected one of the variables at random, and set its value to NA.\nUsed \\(M = 1\\) principal component in the algorithm.\nCorrelation: 0.63 between original and imputed values."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example — Continued",
    "text": "Example — Continued\n\nThe USAarrests data has only four variables, which is on the low end for this method to work well. For this reason, for this demonstration we randomly set at most one variable per state to be missing and only used \\(M = 1\\) principal component.\nIn general, in order to apply this algorithm, we must select \\(M\\), the number of principal components to use for the imputation.\nOne approach is to randomly set to NA some elements that were actually observed, and select \\(M\\) based on how well those known values are recovered. This is closely related to the validation-set approach seen in Chapter 5.\nsoftImpute package in R implements matrix completion algorithms and can manage Netflix-scale matrices."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/02_statistical_learning/02_statistical_learning.html",
    "href": "lecture_slides/02_statistical_learning/02_statistical_learning.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "XXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html",
    "href": "lecture_slides/01_introduction/01_introduction.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Introductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nXXX\n\n\n\n\n\n\n\nthis lecture draws on materials from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nIt is your turn! - 5 minutes\n\n\n\nPresent yourself to your left/right colleague and tell her/him what are the current two main passions in your life."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\nMaterials:\n\nBrightspace\nCourse Webpage\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-supervised-learning-problem",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-supervised-learning-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Supervised Learning Problem",
    "text": "The Supervised Learning Problem\n\n\n\nStarting point:\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples.\nObjective is more fuzzy:\n\nFind groups of samples that behave similarly.\nFind features that behave similarly.\nFind linear combinations of features with the most variation.\n\nDifficult to know how well you are doing.\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Netflix Prize",
    "text": "The Netflix Prize\n\n\n\nCompetition started in October 2006. Training data is ratings for 18,000 movies by 400,000 Netflix customers, each rating between 1 and 5.\nTraining data is very sparse—about 98% missing.\nObjective is to predict the rating for a set of 1 million customer-movie pairs that are missing in the training data.\nNetflix’s original algorithm achieved a root MSE of 0.953.\nThe first team to achieve a 10% improvement wins one million dollars.\nIs this a supervised or unsupervised problem?"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#statistical-learning-versus-machine-learning",
    "href": "lecture_slides/01_introduction/01_introduction.html#statistical-learning-versus-machine-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Statistical Learning versus Machine Learning",
    "text": "Statistical Learning versus Machine Learning\n\n\n\nMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap—both fields focus on supervised and unsupervised problems:\n\nMachine learning has a greater emphasis on large scale applications and prediction accuracy.\nStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\nThe distinction has become more blurred, with significant cross-fertilization.\nMachine learning has the upper hand in Marketing!"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html",
    "href": "lecture_slides/06_model_selection/06_model_selection.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "XXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "XXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\n\nRecall the linear model\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. In the lectures covering Chapter 7 of the text, we generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In praise of linear models!",
    "text": "In praise of linear models!\n\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\nHence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why consider alternatives to least squares?",
    "text": "Why consider alternatives to least squares?\n\nPrediction Accuracy: especially when \\(p &gt; n\\), to control the variance.\nModel Interpretability: By removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing feature selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three classes of methods",
    "text": "Three classes of methods\n\nSubset Selection. We identify a subset of the \\(p\\) predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\nShrinkage. We fit a model involving all \\(p\\) predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance and can also perform variable selection.\nDimension Reduction. We project the \\(p\\) predictors into a \\(M\\)-dimensional subspace, where \\(M &lt; p\\). This is achieved by computing \\(M\\) different linear combinations, or projections, of the variables. Then these \\(M\\) projections are used as predictors to fit a linear regression model by least squares."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example - Credit data set",
    "text": "Example - Credit data set\n\n\n\n\n\n\n\n\n\n\nFor each possible model containing a subset of the ten predictors in the Credit data set, the RSS and \\(R^2\\) are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nThough the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions to other models",
    "text": "Extensions to other models\n\nAlthough we have presented best subset selection here for least squares regression, the same ideas apply to other types of models, such as logistic regression.\nThe deviance—negative two times the maximized log-likelihood—plays the role of RSS for a broader class of models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\). Why not?\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nIn particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In Detail",
    "text": "In Detail\nForward Stepwise Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Forward Stepwise Selection",
    "text": "More on Forward Stepwise Selection\n\nComputational advantage over best subset selection is clear.\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors. Why not? Give an example."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n# Variables\nBest subset\nForward stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: details",
    "text": "Backward Stepwise Selection: details\nBackward Stepwise Selection\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all \\(p\\) predictors.\nFor \\(k = p, p - 1, \\ldots, 1\\):\n\n2.1 Consider all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k - 1\\) predictors.\n2.2 Choose the best among these \\(k\\) models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Backward Stepwise Selection",
    "text": "More on Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating test error: two approaches",
    "text": "Estimating test error: two approaches\n\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nWe can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures.\nWe illustrate both approaches next."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)\n\nThese techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.\nThe next figure displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now for some details",
    "text": "Now for some details\n\nMallow’s \\(C_p\\): \\[\nC_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\n\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement.\n\nThe AIC criterion is defined for a large class of models fit by maximum likelihood:\n\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent. Prove this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details on BIC",
    "text": "Details on BIC\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nFor a least squares model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as \\[\n\\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n - d - 1)}{\\text{TSS}/(n - 1)}.\n\\]\n\nwhere TSS is the total sum of squares.\n\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{\\text{RSS}}{n - d - 1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{\\text{RSS}}{n - d - 1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10\\) folds. In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. What is the rationale for this?"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize \\[\n\\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression: continued",
    "text": "Ridge regression: continued\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge Regression: Scaling of Predictors",
    "text": "Ridge Regression: Scaling of Predictors\n\nThe standard least squares coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\nThe Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients. Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\). The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses an \\(\\ell_1\\) (pronounced “ell 1”) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso: Continued",
    "text": "The Lasso: Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Dataset",
    "text": "Example: Credit Dataset"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?\nOne can show that the lasso and ridge regression coefficient estimates solve the problems:\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} |\\beta_j| \\leq s\n\\]\nand\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s,\n\\]\nrespectively."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso Picture",
    "text": "The Lasso Picture"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\n\n\n\n\n\n\n\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression: continued",
    "text": "Comparing the Lasso and Ridge Regression: continued\n\n\n\n\n\n\n\n\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data, except that now only two predictors are related to the response.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\nCross-validation errors for ridge regression\n\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\).\n\n\n\n\nCoefficient estimates as function of lambda\n\n\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated data example",
    "text": "Simulated data example\n\n\n\n\n\n\n\n\n\nLeft: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set.\nRight: The corresponding lasso coefficient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Details",
    "text": "Dimension Reduction Methods: Details\n\nLet \\(Z_1, Z_2, \\ldots, Z_M\\) represent \\(M &lt; p\\) linear combinations of our original \\(p\\) predictors. That is, \\[\nZ_m = \\sum_{j=1}^p \\phi_{mj} X_j \\quad \\text{(1)}\n\\] for some constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\).\nWe can then fit the linear regression model, \\[\ny_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\quad \\text{(2)}\n\\] using ordinary least squares.\nNote that in model (2), the regression coefficients are given by \\(\\theta_0, \\theta_1, \\ldots, \\theta_M\\). If the constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\) are chosen wisely, then such dimension reduction approaches can often outperform OLS regression."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Continued",
    "text": "Dimension Reduction Methods: Continued\n\nNotice that from definition (1), \\[\n\\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where \\[\n\\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\n\n\n\n\n\n\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\n\n\n\n\n\n\n\n\nA subset of the advertising data. Left: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. Right: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\n\n\n\n\n\n\n\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\n\n\n\n\n\n\n\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application to Principal Components Regression",
    "text": "Application to Principal Components Regression\n\n\n\n\n\n\n\n\n\nPCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively. Left: Simulated data. Right: Simulated data."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Directions \\(M\\)",
    "text": "Choosing the Number of Directions \\(M\\)\n\n\n\n\n\n\n\n\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares: Continued",
    "text": "Partial Least Squares: Continued\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Partial Least Squares",
    "text": "Details of Partial Least Squares\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nResearch into methods that give sparsity, such as the lasso, is an especially hot area.\nLater, we will return to sparsity in more detail, and will describe related approaches such as the elastic net."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nModel selection methods are an essential tool for data analysis, especially for big datasets involving many predictors.\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "XXXX\n\n\n\nXXX"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#xxx-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#xxx-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXX?",
    "text": "XXX?"
  },
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_purdue_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): [ISLP] James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_purdue_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples.\nObjective is more fuzzy:\n\nFind groups of samples that behave similarly.\nFind features that behave similarly.\nFind linear combinations of features with the most variation.\n\nDifficult to know how well we are doing.\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#smoother-truth-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#smoother-truth-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoother Truth Example",
    "text": "Smoother Truth Example\n\nHere the truth is smoother, so smoother fits and linear models perform well."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#noisy-wiggly-truth-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#noisy-wiggly-truth-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Noisy, Wiggly Truth Example",
    "text": "Noisy, Wiggly Truth Example\n\nHere the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off (Examples)",
    "text": "Bias-Variance Trade-off (Examples)\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point (\\((0, 0)\\)).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point (\\((0, 0)\\)).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality.\n\n\n\n\n\n\n\nParametric and Structured Models\nThe linear model is a key example of a parametric model:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\).\n\n\n\nComparison of Models\n\n\n\nComparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases.\n\n\n\n\n\nSimulated Example\n\nSimulated example. Red points are simulated values for income from the model:\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface.\n\n\n\nLinear Regression Fit\n\nLinear regression model fit to the simulated data:\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]\n\n\n\nFlexible Regression Model Fit\n\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. Here we use a technique called a thin-plate spline to fit a flexible surface.\nWe control the roughness of the fit.\n\n\n\nOverfitting\n\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\nThe fitted model makes no errors on the training data! This is known as overfitting.\n\n\n\nSome Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors.\n\n\n\n\nFlexibility vs. Interpretability\nTrade-offs between flexibility and interpretability:\n\n\nHigh interpretability: Subset selection, Lasso\n\nIntermediate: Least squares, Generalized Additive Models, Trees\n\nHigh flexibility: Bagging, Boosting, Support Vector Machines\n\n\n\n\nAssessing Model Accuracy\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]\n\n\n\n\nBias-Variance Trade-off\n\n\nBlack curve is the truth. Red curve on right is MSE on the test set (\\(\\text{MSE}_{Te}\\)), grey curve is MSE on the training set (\\(\\text{MSE}_{Tr}\\)).\nOrange, blue, green curves/squares correspond to fits of different flexibilities.\n\n\n\n\nSmoother Truth Example\n\nHere the truth is smoother, so smoother fits and linear models perform well.\n\n\n\nNoisy, Wiggly Truth Example\n\nHere the truth is wiggly and the noise is low. More flexible fits perform the best.\n\n\n\nBias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off.\n\n\n\n\n\nBias-Variance Trade-off (Examples)\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE.\n\n\n\n\n\n\nClassification Problems\n\n\n\nClassification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\).\n\n\n\n\n\n\nIdeal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]\n\n\n\n\n\nNearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\).\n\n\n\n\nClassification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models.\n\n\n\n\n\nExample: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier.\n\n\n\nKNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother.\n\n\n\nKNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit.\n\n\n\nKNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\nThis figure illustrates how training errors (teal curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line (around 0.15) might indicate a baseline or reference error rate for comparison.\n\n\n\n\n\nSummary\n\n\n\nSummary\n\n\nXXX\n\n\n\n\n\nThank you!\n\n\n\nPredictive Analytics"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between males and females, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person is female} \\\\\n0 & \\text{if } i\\text{th person is male}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person is female} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person is male.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for gender model:\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nGender \\(Female\\)\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, contrary to popular wisdom, females don’t generally have a higher credit card balance than males."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is Asian} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is Caucasian}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is African American (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#getting-started",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#getting-started",
    "title": "Introduction to Python",
    "section": "Getting Started",
    "text": "Getting Started\nTo run the labs in this book, you will need two things:\n\nAn installation of Python3, which is the specific version of Python used in the labs.\nAccess to Jupyter, a very popular Python interface that runs code through a file called a notebook.\n\nYou can download and install Python3 by following the instructions available at anaconda.com.\nThere are a number of ways to get access to Jupyter. Here are just a few:\n\nUsing Google’s Colaboratory service: colab.research.google.com/.\nUsing JupyterHub, available at jupyter.org/hub.\nUsing your own jupyter installation. Installation instructions are available at jupyter.org/install.\n\nPlease see the Python resources page on the book website statlearning.com for up-to-date information about getting Python and Jupyter working on your computer.\nYou will need to install the ISLP package, which provides access to the datasets and custom-built functions that we provide. Inside a macOS or Linux terminal type pip install ISLP; this also installs most other packages needed in the labs. The Python resources page has a link to the ISLP documentation website.\nTo run this lab, download the file Ch2-statlearn-lab.ipynb from the Python resources page. Now run the following code at the command line: jupyter lab Ch2-statlearn-lab.ipynb.\nIf you’re using Windows, you can use the start menu to access anaconda, and follow the links. For example, to install ISLP and run this lab, you can run the same code above in an anaconda shell."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#basic-commands",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#basic-commands",
    "title": "Introduction to Python",
    "section": "Basic Commands",
    "text": "Basic Commands\nIn this lab, we will introduce some simple Python commands. For more resources about Python in general, readers may want to consult the tutorial at docs.python.org/3/tutorial/.\nLike most programming languages, Python uses functions to perform operations. To run a function called fun, we type fun(input1,input2), where the inputs (or arguments) input1 and input2 tell Python how to run the function. A function can have any number of inputs. For example, the print() function outputs a text representation of all of its arguments to the console.\n\nprint('fit a model with', 11, 'variables')\n\nfit a model with 11 variables\n\n\nThe following command will provide information about the print() function.\n\nprint?\n\nAdding two integers in Python is pretty intuitive.\n\n3 + 5\n\n8\n\n\nIn Python, textual data is handled using strings. For instance, \"hello\" and 'hello' are strings. We can concatenate them using the addition + symbol.\n\n\"hello\" + \" \" + \"world\"\n\n'hello world'\n\n\nA string is actually a type of sequence: this is a generic term for an ordered list. The three most important types of sequences are lists, tuples, and strings.\nWe introduce lists now.\nThe following command instructs Python to join together the numbers 3, 4, and 5, and to save them as a list named x. When we type x, it gives us back the list.\n\nx = [3, 4, 5]\nx\n\n[3, 4, 5]\n\n\nNote that we used the brackets [] to construct this list.\nWe will often want to add two sets of numbers together. It is reasonable to try the following code, though it will not produce the desired results.\n\ny = [4, 9, 7]\nx + y\n\n[3, 4, 5, 4, 9, 7]\n\n\nThe result may appear slightly counterintuitive: why did Python not add the entries of the lists element-by-element? In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the behavior that we saw earlier when we entered \"hello\" + \" \" + \"world\".\nThis example reflects the fact that Python is a general-purpose programming language. Much of Python’s data-specific functionality comes from other packages, notably numpy and pandas. In the next section, we will introduce the numpy package. See docs.scipy.org/doc/numpy/user/quickstart.html for more information about numpy."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#introduction-to-numerical-python",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#introduction-to-numerical-python",
    "title": "Introduction to Python",
    "section": "Introduction to Numerical Python",
    "text": "Introduction to Numerical Python\nAs mentioned earlier, this book makes use of functionality that is contained in the numpy library, or package. A package is a collection of modules that are not necessarily included in the base Python distribution. The name numpy is an abbreviation for numerical Python.\nTo access numpy, we must first import it.\n\nimport numpy as np \n\nIn the previous line, we named the numpy module np; an abbreviation for easier referencing.\nIn numpy, an array is a generic term for a multidimensional set of numbers. We use the np.array() function to define x and y, which are one-dimensional arrays, i.e. vectors.\n\nx = np.array([3, 4, 5])\ny = np.array([4, 9, 7])\n\nNote that if you forgot to run the import numpy as np command earlier, then you will encounter an error in calling the np.array() function in the previous line. The syntax np.array() indicates that the function being called is part of the numpy package, which we have abbreviated as np.\nSince x and y have been defined using np.array(), we get a sensible result when we add them together. Compare this to our results in the previous section, when we tried to add two lists without using numpy.\n\nx + y\n\narray([ 7, 13, 12])\n\n\nIn numpy, matrices are typically represented as two-dimensional arrays, and vectors as one-dimensional arrays. {While it is also possible to create matrices using np.matrix(), we will use np.array() throughout the labs in this book.} We can create a two-dimensional array as follows.\n\nx = np.array([[1, 2], [3, 4]])\nx\n\narray([[1, 2],\n       [3, 4]])\n\n\nThe object x has several attributes, or associated objects. To access an attribute of x, we type x.attribute, where we replace attribute with the name of the attribute. For instance, we can access the ndim attribute of x as follows.\n\nx.ndim\n\n2\n\n\nThe output indicates that x is a two-dimensional array.\nSimilarly, x.dtype is the data type attribute of the object x. This indicates that x is comprised of 64-bit integers:\n\nx.dtype\n\ndtype('int64')\n\n\nWhy is x comprised of integers? This is because we created x by passing in exclusively integers to the np.array() function. If we had passed in any decimals, then we would have obtained an array of floating point numbers (i.e. real-valued numbers).\n\nnp.array([[1, 2], [3.0, 4]]).dtype\n\ndtype('float64')\n\n\nTyping fun? will cause Python to display documentation associated with the function fun, if it exists. We can try this for np.array().\n\nnp.array?\n\nThis documentation indicates that we could create a floating point array by passing a dtype argument into np.array().\n\nnp.array([[1, 2], [3, 4]], float).dtype\n\ndtype('float64')\n\n\nThe array x is two-dimensional. We can find out the number of rows and columns by looking at its shape attribute.\n\nx.shape\n\n(2, 2)\n\n\nA method is a function that is associated with an object. For instance, given an array x, the expression x.sum() sums all of its elements, using the sum() method for arrays. The call x.sum() automatically provides x as the first argument to its sum() method.\n\nx = np.array([1, 2, 3, 4])\nx.sum()\n\n10\n\n\nWe could also sum the elements of x by passing in x as an argument to the np.sum() function.\n\nx = np.array([1, 2, 3, 4])\nnp.sum(x)\n\n10\n\n\nAs another example, the reshape() method returns a new array with the same elements as x, but a different shape. We do this by passing in a tuple in our call to reshape(), in this case (2, 3). This tuple specifies that we would like to create a two-dimensional array with \\(2\\) rows and \\(3\\) columns. {Like lists, tuples represent a sequence of objects. Why do we need more than one way to create a sequence? There are a few differences between tuples and lists, but perhaps the most important is that elements of a tuple cannot be modified, whereas elements of a list can be.}\nIn what follows, the \\n character creates a new line.\n\nx = np.array([1, 2, 3, 4, 5, 6])\nprint('beginning x:\\n', x)\nx_reshape = x.reshape((2, 3))\nprint('reshaped x:\\n', x_reshape)\n\nbeginning x:\n [1 2 3 4 5 6]\nreshaped x:\n [[1 2 3]\n [4 5 6]]\n\n\nThe previous output reveals that numpy arrays are specified as a sequence of rows. This is called row-major ordering, as opposed to column-major ordering.\nPython (and hence numpy) uses 0-based indexing. This means that to access the top left element of x_reshape, we type in x_reshape[0,0].\n\nx_reshape[0, 0] \n\n1\n\n\nSimilarly, x_reshape[1,2] yields the element in the second row and the third column of x_reshape.\n\nx_reshape[1, 2] \n\n6\n\n\nSimilarly, x[2] yields the third entry of x.\nNow, let’s modify the top left element of x_reshape. To our surprise, we discover that the first element of x has been modified as well!\n\nprint('x before we modify x_reshape:\\n', x)\nprint('x_reshape before we modify x_reshape:\\n', x_reshape)\nx_reshape[0, 0] = 5\nprint('x_reshape after we modify its top left element:\\n', x_reshape)\nprint('x after we modify top left element of x_reshape:\\n', x)\n\nx before we modify x_reshape:\n [1 2 3 4 5 6]\nx_reshape before we modify x_reshape:\n [[1 2 3]\n [4 5 6]]\nx_reshape after we modify its top left element:\n [[5 2 3]\n [4 5 6]]\nx after we modify top left element of x_reshape:\n [5 2 3 4 5 6]\n\n\nModifying x_reshape also modified x because the two objects occupy the same space in memory.\nWe just saw that we can modify an element of an array. Can we also modify a tuple? It turns out that we cannot — and trying to do so introduces an exception, or error.\n\nmy_tuple = (3, 4, 5)\nmy_tuple[0] = 2\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[23], line 2\n      1 my_tuple = (3, 4, 5)\n----&gt; 2 my_tuple[0] = 2\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nWe now briefly mention some attributes of arrays that will come in handy. An array’s shape attribute contains its dimension; this is always a tuple. The ndim attribute yields the number of dimensions, and T provides its transpose.\n\nx_reshape.shape, x_reshape.ndim, x_reshape.T\n\n((2, 3),\n 2,\n array([[5, 4],\n        [2, 5],\n        [3, 6]]))\n\n\nNotice that the three individual outputs (2,3), 2, and array([[5, 4],[2, 5], [3,6]]) are themselves output as a tuple.\nWe will often want to apply functions to arrays. For instance, we can compute the square root of the entries using the np.sqrt() function:\n\nnp.sqrt(x)\n\narray([2.23606798, 1.41421356, 1.73205081, 2.        , 2.23606798,\n       2.44948974])\n\n\nWe can also square the elements:\n\nx**2\n\narray([25,  4,  9, 16, 25, 36])\n\n\nWe can compute the square roots using the same notation, raising to the power of \\(1/2\\) instead of 2.\n\nx**0.5\n\narray([2.23606798, 1.41421356, 1.73205081, 2.        , 2.23606798,\n       2.44948974])\n\n\nThroughout this book, we will often want to generate random data. The np.random.normal() function generates a vector of random normal variables. We can learn more about this function by looking at the help page, via a call to np.random.normal?. The first line of the help page reads normal(loc=0.0, scale=1.0, size=None). This signature line tells us that the function’s arguments are loc, scale, and size. These are keyword arguments, which means that when they are passed into the function, they can be referred to by name (in any order). {Python also uses positional arguments. Positional arguments do not need to use a keyword. To see an example, type in np.sum?. We see that a is a positional argument, i.e. this function assumes that the first unnamed argument that it receives is the array to be summed. By contrast, axis and dtype are keyword arguments: the position in which these arguments are entered into np.sum() does not matter.} By default, this function will generate random normal variable(s) with mean (loc) \\(0\\) and standard deviation (scale) \\(1\\); furthermore, a single random variable will be generated unless the argument to size is changed.\nWe now generate 50 independent random variables from a \\(N(0,1)\\) distribution.\n\nx = np.random.normal(size=50)\nx\n\narray([ 0.63214394,  0.34288365,  0.85005043, -0.47385094,  0.32864279,\n       -0.78796854,  0.51125391, -1.12947776, -0.71391649, -1.92367418,\n       -0.42609273, -0.7309404 ,  0.83090301, -0.47570241, -0.5852834 ,\n        0.89788488, -1.27638353,  2.59421427,  1.25973331, -0.61486026,\n        1.3979914 ,  0.79368659, -1.14506316, -2.00869422,  0.19502313,\n       -0.45956661,  0.65944298,  0.69723473, -1.09211872,  0.15972909,\n       -0.22530418,  0.32276968,  0.50374192,  0.80506415, -0.48224507,\n       -0.53214642, -1.09662644, -1.52722433, -2.10241781, -0.42403843,\n        1.32310673,  2.00932483, -0.16726754,  2.30113584, -0.68976675,\n       -0.67621251, -0.78371139, -0.62709974, -1.39453964,  0.25195549])\n\n\nWe create an array y by adding an independent \\(N(50,1)\\) random variable to each element of x.\n\ny = x + np.random.normal(loc=50, scale=1, size=50)\n\nThe np.corrcoef() function computes the correlation matrix between x and y. The off-diagonal elements give the correlation between x and y.\n\nnp.corrcoef(x, y)\n\narray([[1.        , 0.71781515],\n       [0.71781515, 1.        ]])\n\n\nIf you’re following along in your own Jupyter notebook, then you probably noticed that you got a different set of results when you ran the past few commands. In particular, each time we call np.random.normal(), we will get a different answer, as shown in the following example.\n\nprint(np.random.normal(scale=5, size=2))\nprint(np.random.normal(scale=5, size=2)) \n\n[-0.97181857  3.2315884 ]\n[ 0.73365791 -7.23683363]\n\n\nIn order to ensure that our code provides exactly the same results each time it is run, we can set a random seed using the np.random.default_rng() function. This function takes an arbitrary, user-specified integer argument. If we set a random seed before generating random data, then re-running our code will yield the same results. The object rng has essentially all the random number generating methods found in np.random. Hence, to generate normal data we use rng.normal().\n\nrng = np.random.default_rng(1303)\nprint(rng.normal(scale=5, size=2))\nrng2 = np.random.default_rng(1303)\nprint(rng2.normal(scale=5, size=2)) \n\n[ 4.09482632 -1.07485605]\n[ 4.09482632 -1.07485605]\n\n\nThroughout the labs in this book, we use np.random.default_rng() whenever we perform calculations involving random quantities within numpy. In principle, this should enable the reader to exactly reproduce the stated results. However, as new versions of numpy become available, it is possible that some small discrepancies may occur between the output in the labs and the output from numpy.\nThe np.mean(), np.var(), and np.std() functions can be used to compute the mean, variance, and standard deviation of arrays. These functions are also available as methods on the arrays.\n\nrng = np.random.default_rng(3)\ny = rng.standard_normal(10)\nnp.mean(y), y.mean()\n\n(-0.1126795190952861, -0.1126795190952861)\n\n\n\nnp.var(y), y.var(), np.mean((y - y.mean())**2)\n\n(2.7243406406465125, 2.7243406406465125, 2.7243406406465125)\n\n\nNotice that by default np.var() divides by the sample size \\(n\\) rather than \\(n-1\\); see the ddof argument in np.var?.\n\nnp.sqrt(np.var(y)), np.std(y)\n\n(1.6505576756498128, 1.6505576756498128)\n\n\nThe np.mean(), np.var(), and np.std() functions can also be applied to the rows and columns of a matrix. To see this, we construct a \\(10 \\times 3\\) matrix of \\(N(0,1)\\) random variables, and consider computing its row sums.\n\nX = rng.standard_normal((10, 3))\nX\n\narray([[ 0.22578661, -0.35263079, -0.28128742],\n       [-0.66804635, -1.05515055, -0.39080098],\n       [ 0.48194539, -0.23855361,  0.9577587 ],\n       [-0.19980213,  0.02425957,  1.54582085],\n       [ 0.54510552, -0.50522874, -0.18283897],\n       [ 0.54052513,  1.93508803, -0.26962033],\n       [-0.24355868,  1.0023136 , -0.88645994],\n       [-0.29172023,  0.88253897,  0.58035002],\n       [ 0.0915167 ,  0.67010435, -2.82816231],\n       [ 1.02130682, -0.95964476, -1.66861984]])\n\n\nSince arrays are row-major ordered, the first axis, i.e. axis=0, refers to its rows. We pass this argument into the mean() method for the object X.\n\nX.mean(axis=0)\n\narray([ 0.15030588,  0.14030961, -0.34238602])\n\n\nThe following yields the same result.\n\nX.mean(0)\n\narray([ 0.15030588,  0.14030961, -0.34238602])"
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#graphics",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#graphics",
    "title": "Introduction to Python",
    "section": "Graphics",
    "text": "Graphics\nIn Python, common practice is to use the library matplotlib for graphics. However, since Python was not written with data analysis in mind, the notion of plotting is not intrinsic to the language. We will use the subplots() function from matplotlib.pyplot to create a figure and the axes onto which we plot our data. For many more examples of how to make plots in Python, readers are encouraged to visit matplotlib.org/stable/gallery/.\nIn matplotlib, a plot consists of a figure and one or more axes. You can think of the figure as the blank canvas upon which one or more plots will be displayed: it is the entire plotting window. The axes contain important information about each plot, such as its \\(x\\)- and \\(y\\)-axis labels, title, and more. (Note that in matplotlib, the word axes is not the plural of axis: a plot’s axes contains much more information than just the \\(x\\)-axis and the \\(y\\)-axis.)\nWe begin by importing the subplots() function from matplotlib. We use this function throughout when creating figures. The function returns a tuple of length two: a figure object as well as the relevant axes object. We will typically pass figsize as a keyword argument. Having created our axes, we attempt our first plot using its plot() method. To learn more about it, type ax.plot?.\n\nfrom matplotlib.pyplot import subplots\nfig, ax = subplots(figsize=(8, 8))\nx = rng.standard_normal(100)\ny = rng.standard_normal(100)\nax.plot(x, y);\n\n\n\n\n\n\n\n\nWe pause here to note that we have unpacked the tuple of length two returned by subplots() into the two distinct variables fig and ax. Unpacking is typically preferred to the following equivalent but slightly more verbose code:\n\noutput = subplots(figsize=(8, 8))\nfig = output[0]\nax = output[1]\n\n\n\n\n\n\n\n\nWe see that our earlier cell produced a line plot, which is the default. To create a scatterplot, we provide an additional argument to ax.plot(), indicating that circles should be displayed.\n\nfig, ax = subplots(figsize=(8, 8))\nax.plot(x, y, 'o');\n\n\n\n\n\n\n\n\nDifferent values of this additional argument can be used to produce different colored lines as well as different linestyles.\nAs an alternative, we could use the ax.scatter() function to create a scatterplot.\n\nfig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='o');\n\n\n\n\n\n\n\n\nNotice that in the code blocks above, we have ended the last line with a semicolon. This prevents ax.plot(x, y) from printing text to the notebook. However, it does not prevent a plot from being produced. If we omit the trailing semi-colon, then we obtain the following output:\n\nfig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='o')\n\n\n\n\n\n\n\n\nIn what follows, we will use trailing semicolons whenever the text that would be output is not germane to the discussion at hand.\nTo label our plot, we make use of the set_xlabel(), set_ylabel(), and set_title() methods of ax.\n\nfig, ax = subplots(figsize=(8, 8))\nax.scatter(x, y, marker='o')\nax.set_xlabel(\"this is the x-axis\")\nax.set_ylabel(\"this is the y-axis\")\nax.set_title(\"Plot of X vs Y\");\n\n\n\n\n\n\n\n\nHaving access to the figure object fig itself means that we can go in and change some aspects and then redisplay it. Here, we change the size from (8, 8) to (12, 3).\n\nfig.set_size_inches(12,3)\nfig\n\n\n\n\n\n\n\n\nOccasionally we will want to create several plots within a figure. This can be achieved by passing additional arguments to subplots(). Below, we create a \\(2 \\times 3\\) grid of plots in a figure of size determined by the figsize argument. In such situations, there is often a relationship between the axes in the plots. For example, all plots may have a common \\(x\\)-axis. The subplots() function can automatically handle this situation when passed the keyword argument sharex=True. The axes object below is an array pointing to different plots in the figure.\n\nfig, axes = subplots(nrows=2,\n                     ncols=3,\n                     figsize=(15, 5))\n\n\n\n\n\n\n\n\nWe now produce a scatter plot with 'o' in the second column of the first row and a scatter plot with '+' in the third column of the second row.\n\naxes[0,1].plot(x, y, 'o')\naxes[1,2].scatter(x, y, marker='+')\nfig\n\n\n\n\n\n\n\n\nType subplots? to learn more about subplots().\nTo save the output of fig, we call its savefig() method. The argument dpi is the dots per inch, used to determine how large the figure will be in pixels.\n\nfig.savefig(\"Figure.png\", dpi=400)\nfig.savefig(\"Figure.pdf\", dpi=200);\n\nWe can continue to modify fig using step-by-step updates; for example, we can modify the range of the \\(x\\)-axis, re-save the figure, and even re-display it.\n\naxes[0,1].set_xlim([-1,1])\nfig.savefig(\"Figure_updated.jpg\")\nfig\n\n\n\n\n\n\n\n\nWe now create some more sophisticated plots. The ax.contour() method produces a contour plot in order to represent three-dimensional data, similar to a topographical map. It takes three arguments:\n\nA vector of x values (the first dimension),\nA vector of y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x,y) coordinates.\n\nTo create x and y, we’ll use the command np.linspace(a, b, n), which returns a vector of n numbers starting at a and ending at b.\n\nfig, ax = subplots(figsize=(8, 8))\nx = np.linspace(-np.pi, np.pi, 50)\ny = x\nf = np.multiply.outer(np.cos(y), 1 / (1 + x**2))\nax.contour(x, y, f);\n\n\n\n\n\n\n\n\nWe can increase the resolution by adding more levels to the image.\n\nfig, ax = subplots(figsize=(8, 8))\nax.contour(x, y, f, levels=45);\n\n\n\n\n\n\n\n\nTo fine-tune the output of the ax.contour() function, take a look at the help file by typing ?plt.contour.\nThe ax.imshow() method is similar to ax.contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts.\n\nfig, ax = subplots(figsize=(8, 8))\nax.imshow(f);"
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#sequences-and-slice-notation",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#sequences-and-slice-notation",
    "title": "Introduction to Python",
    "section": "Sequences and Slice Notation",
    "text": "Sequences and Slice Notation\nAs seen above, the function np.linspace() can be used to create a sequence of numbers.\n\nseq1 = np.linspace(0, 10, 11)\nseq1\n\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n\n\nThe function np.arange() returns a sequence of numbers spaced out by step. If step is not specified, then a default value of \\(1\\) is used. Let’s create a sequence that starts at \\(0\\) and ends at \\(10\\).\n\nseq2 = np.arange(0, 10)\nseq2\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nWhy isn’t \\(10\\) output above? This has to do with slice notation in Python. Slice notation\nis used to index sequences such as lists, tuples and arrays. Suppose we want to retrieve the fourth through sixth (inclusive) entries of a string. We obtain a slice of the string using the indexing notation [3:6].\n\n\"hello world\"[3:6]\n\n'lo '\n\n\nIn the code block above, the notation 3:6 is shorthand for slice(3,6) when used inside [].\n\n\"hello world\"[slice(3,6)]\n\n'lo '\n\n\nYou might have expected slice(3,6) to output the fourth through seventh characters in the text string (recalling that Python begins its indexing at zero), but instead it output the fourth through sixth. This also explains why the earlier np.arange(0, 10) command output only the integers from \\(0\\) to \\(9\\). See the documentation slice? for useful options in creating slices."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#indexing-data",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#indexing-data",
    "title": "Introduction to Python",
    "section": "Indexing Data",
    "text": "Indexing Data\nTo begin, we create a two-dimensional numpy array.\n\nA = np.array(np.arange(16)).reshape((4, 4))\nA\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\n\n\nTyping A[1,2] retrieves the element corresponding to the second row and third column. (As usual, Python indexes from \\(0.\\))\n\nA[1,2]\n\n6\n\n\nThe first number after the open-bracket symbol [ refers to the row, and the second number refers to the column.\n\nIndexing Rows, Columns, and Submatrices\nTo select multiple rows at a time, we can pass in a list specifying our selection. For instance, [1,3] will retrieve the second and fourth rows:\n\nA[[1,3]]\n\narray([[ 4,  5,  6,  7],\n       [12, 13, 14, 15]])\n\n\nTo select the first and third columns, we pass in [0,2] as the second argument in the square brackets. In this case we need to supply the first argument : which selects all rows.\n\nA[:,[0,2]]\n\narray([[ 0,  2],\n       [ 4,  6],\n       [ 8, 10],\n       [12, 14]])\n\n\nNow, suppose that we want to select the submatrix made up of the second and fourth rows as well as the first and third columns. This is where indexing gets slightly tricky. It is natural to try to use lists to retrieve the rows and columns:\n\nA[[1,3],[0,2]]\n\narray([ 4, 14])\n\n\nOops — what happened? We got a one-dimensional array of length two identical to\n\nnp.array([A[1,0],A[3,2]])\n\narray([ 4, 14])\n\n\nSimilarly, the following code fails to extract the submatrix comprised of the second and fourth rows and the first, third, and fourth columns:\n\nA[[1,3],[0,2,3]]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[63], line 1\n----&gt; 1 A[[1,3],[0,2,3]]\n\nIndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (3,) \n\n\n\nWe can see what has gone wrong here. When supplied with two indexing lists, the numpy interpretation is that these provide pairs of \\(i,j\\) indices for a series of entries. That is why the pair of lists must have the same length. However, that was not our intent, since we are looking for a submatrix.\nOne easy way to do this is as follows. We first create a submatrix by subsetting the rows of A, and then on the fly we make a further submatrix by subsetting its columns.\n\nA[[1,3]][:,[0,2]]\n\narray([[ 4,  6],\n       [12, 14]])\n\n\nThere are more efficient ways of achieving the same result.\nThe convenience function np.ix_() allows us to extract a submatrix using lists, by creating an intermediate mesh object.\n\nidx = np.ix_([1,3],[0,2,3])\nA[idx]\n\narray([[ 4,  6,  7],\n       [12, 14, 15]])\n\n\nAlternatively, we can subset matrices efficiently using slices.\nThe slice 1:4:2 captures the second and fourth items of a sequence, while the slice 0:3:2 captures the first and third items (the third element in a slice sequence is the step size).\n\nA[1:4:2,0:3:2]\n\narray([[ 4,  6],\n       [12, 14]])\n\n\nWhy are we able to retrieve a submatrix directly using slices but not using lists? Its because they are different Python types, and are treated differently by numpy. Slices can be used to extract objects from arbitrary sequences, such as strings, lists, and tuples, while the use of lists for indexing is more limited.\n\n\nBoolean Indexing\nIn numpy, a Boolean is a type that equals either True or False (also represented as \\(1\\) and \\(0\\), respectively). The next line creates a vector of \\(0\\)’s, represented as Booleans, of length equal to the first dimension of A.\n\nkeep_rows = np.zeros(A.shape[0], bool)\nkeep_rows\n\narray([False, False, False, False])\n\n\nWe now set two of the elements to True.\n\nkeep_rows[[1,3]] = True\nkeep_rows\n\narray([False,  True, False,  True])\n\n\nNote that the elements of keep_rows, when viewed as integers, are the same as the values of np.array([0,1,0,1]). Below, we use == to verify their equality. When applied to two arrays, the == operation is applied elementwise.\n\nnp.all(keep_rows == np.array([0,1,0,1]))\n\nTrue\n\n\n(Here, the function np.all() has checked whether all entries of an array are True. A similar function, np.any(), can be used to check whether any entries of an array are True.)\nHowever, even though np.array([0,1,0,1]) and keep_rows are equal according to ==, they index different sets of rows! The former retrieves the first, second, first, and second rows of A.\n\nA[np.array([0,1,0,1])]\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7],\n       [0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\n\nBy contrast, keep_rows retrieves only the second and fourth rows of A — i.e. the rows for which the Boolean equals TRUE.\n\nA[keep_rows]\n\narray([[ 4,  5,  6,  7],\n       [12, 13, 14, 15]])\n\n\nThis example shows that Booleans and integers are treated differently by numpy.\nWe again make use of the np.ix_() function to create a mesh containing the second and fourth rows, and the first, third, and fourth columns. This time, we apply the function to Booleans, rather than lists.\n\nkeep_cols = np.zeros(A.shape[1], bool)\nkeep_cols[[0, 2, 3]] = True\nidx_bool = np.ix_(keep_rows, keep_cols)\nA[idx_bool]\n\narray([[ 4,  6,  7],\n       [12, 14, 15]])\n\n\nWe can also mix a list with an array of Booleans in the arguments to np.ix_():\n\nidx_mixed = np.ix_([1,3], keep_cols)\nA[idx_mixed]\n\narray([[ 4,  6,  7],\n       [12, 14, 15]])\n\n\nFor more details on indexing in numpy, readers are referred to the numpy tutorial mentioned earlier."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#loading-data",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#loading-data",
    "title": "Introduction to Python",
    "section": "Loading Data",
    "text": "Loading Data\nData sets often contain different types of data, and may have names associated with the rows or columns. For these reasons, they typically are best accommodated using a data frame. We can think of a data frame as a sequence of arrays of identical length; these are the columns. Entries in the different arrays can be combined to form a row. The pandas library can be used to create and work with data frame objects.\n\nReading in a Data Set\nThe first step of most analyses involves importing a data set into Python.\nBefore attempting to load a data set, we must make sure that Python knows where to find the file containing it. If the file is in the same location as this notebook file, then we are all set. Otherwise, the command os.chdir() can be used to change directory. (You will need to call import os before calling os.chdir().)\nWe will begin by reading in Auto.csv, available on the book website. This is a comma-separated file, and can be read in using pd.read_csv():\n\nimport pandas as pd\nAuto = pd.read_csv('Auto.csv')\nAuto\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n387\n27.0\n4\n140.0\n86\n2790\n15.6\n82\n1\nford mustang gl\n\n\n388\n44.0\n4\n97.0\n52\n2130\n24.6\n82\n2\nvw pickup\n\n\n389\n32.0\n4\n135.0\n84\n2295\n11.6\n82\n1\ndodge rampage\n\n\n390\n28.0\n4\n120.0\n79\n2625\n18.6\n82\n1\nford ranger\n\n\n391\n31.0\n4\n119.0\n82\n2720\n19.4\n82\n1\nchevy s-10\n\n\n\n\n392 rows × 9 columns\n\n\n\nThe book website also has a whitespace-delimited version of this data, called Auto.data. This can be read in as follows:\n\nAuto = pd.read_csv('Auto.data', delim_whitespace=True)\n\nBoth Auto.csv and Auto.data are simply text files. Before loading data into Python, it is a good idea to view it using a text editor or other software, such as Microsoft Excel.\nWe now take a look at the column of Auto corresponding to the variable horsepower:\n\nAuto['horsepower']\n\n0      130.0\n1      165.0\n2      150.0\n3      150.0\n4      140.0\n       ...  \n392    86.00\n393    52.00\n394    84.00\n395    79.00\n396    82.00\nName: horsepower, Length: 397, dtype: object\n\n\nWe see that the dtype of this column is object. It turns out that all values of the horsepower column were interpreted as strings when reading in the data. We can find out why by looking at the unique values.\n\nnp.unique(Auto['horsepower'])\n\narray(['100.0', '102.0', '103.0', '105.0', '107.0', '108.0', '110.0',\n       '112.0', '113.0', '115.0', '116.0', '120.0', '122.0', '125.0',\n       '129.0', '130.0', '132.0', '133.0', '135.0', '137.0', '138.0',\n       '139.0', '140.0', '142.0', '145.0', '148.0', '149.0', '150.0',\n       '152.0', '153.0', '155.0', '158.0', '160.0', '165.0', '167.0',\n       '170.0', '175.0', '180.0', '190.0', '193.0', '198.0', '200.0',\n       '208.0', '210.0', '215.0', '220.0', '225.0', '230.0', '46.00',\n       '48.00', '49.00', '52.00', '53.00', '54.00', '58.00', '60.00',\n       '61.00', '62.00', '63.00', '64.00', '65.00', '66.00', '67.00',\n       '68.00', '69.00', '70.00', '71.00', '72.00', '74.00', '75.00',\n       '76.00', '77.00', '78.00', '79.00', '80.00', '81.00', '82.00',\n       '83.00', '84.00', '85.00', '86.00', '87.00', '88.00', '89.00',\n       '90.00', '91.00', '92.00', '93.00', '94.00', '95.00', '96.00',\n       '97.00', '98.00', '?'], dtype=object)\n\n\nWe see the culprit is the value ?, which is being used to encode missing values.\nTo fix the problem, we must provide pd.read_csv() with an argument called na_values. Now, each instance of ? in the file is replaced with the value np.nan, which means not a number:\n\nAuto = pd.read_csv('Auto.data',\n                   na_values=['?'],\n                   delim_whitespace=True)\nAuto['horsepower'].sum()\n\n40952.0\n\n\nThe Auto.shape attribute tells us that the data has 397 observations, or rows, and nine variables, or columns.\n\nAuto.shape\n\n(397, 9)\n\n\nThere are various ways to deal with missing data. In this case, since only five of the rows contain missing observations, we choose to use the Auto.dropna() method to simply remove these rows.\n\nAuto_new = Auto.dropna()\nAuto_new.shape\n\n(392, 9)\n\n\n\n\nBasics of Selecting Rows and Columns\nWe can use Auto.columns to check the variable names.\n\nAuto = Auto_new # overwrite the previous value\nAuto.columns\n\nIndex(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin', 'name'],\n      dtype='object')\n\n\nAccessing the rows and columns of a data frame is similar, but not identical, to accessing the rows and columns of an array. Recall that the first argument to the [] method is always applied to the rows of the array.\nSimilarly, passing in a slice to the [] method creates a data frame whose rows are determined by the slice:\n\nAuto[:3]\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504.0\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693.0\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436.0\n11.0\n70\n1\nplymouth satellite\n\n\n\n\n\n\n\nSimilarly, an array of Booleans can be used to subset the rows:\n\nidx_80 = Auto['year'] &gt; 80\nAuto[idx_80]\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n338\n27.2\n4\n135.0\n84.0\n2490.0\n15.7\n81\n1\nplymouth reliant\n\n\n339\n26.6\n4\n151.0\n84.0\n2635.0\n16.4\n81\n1\nbuick skylark\n\n\n340\n25.8\n4\n156.0\n92.0\n2620.0\n14.4\n81\n1\ndodge aries wagon (sw)\n\n\n341\n23.5\n6\n173.0\n110.0\n2725.0\n12.6\n81\n1\nchevrolet citation\n\n\n342\n30.0\n4\n135.0\n84.0\n2385.0\n12.9\n81\n1\nplymouth reliant\n\n\n343\n39.1\n4\n79.0\n58.0\n1755.0\n16.9\n81\n3\ntoyota starlet\n\n\n344\n39.0\n4\n86.0\n64.0\n1875.0\n16.4\n81\n1\nplymouth champ\n\n\n345\n35.1\n4\n81.0\n60.0\n1760.0\n16.1\n81\n3\nhonda civic 1300\n\n\n346\n32.3\n4\n97.0\n67.0\n2065.0\n17.8\n81\n3\nsubaru\n\n\n347\n37.0\n4\n85.0\n65.0\n1975.0\n19.4\n81\n3\ndatsun 210 mpg\n\n\n348\n37.7\n4\n89.0\n62.0\n2050.0\n17.3\n81\n3\ntoyota tercel\n\n\n349\n34.1\n4\n91.0\n68.0\n1985.0\n16.0\n81\n3\nmazda glc 4\n\n\n350\n34.7\n4\n105.0\n63.0\n2215.0\n14.9\n81\n1\nplymouth horizon 4\n\n\n351\n34.4\n4\n98.0\n65.0\n2045.0\n16.2\n81\n1\nford escort 4w\n\n\n352\n29.9\n4\n98.0\n65.0\n2380.0\n20.7\n81\n1\nford escort 2h\n\n\n353\n33.0\n4\n105.0\n74.0\n2190.0\n14.2\n81\n2\nvolkswagen jetta\n\n\n355\n33.7\n4\n107.0\n75.0\n2210.0\n14.4\n81\n3\nhonda prelude\n\n\n356\n32.4\n4\n108.0\n75.0\n2350.0\n16.8\n81\n3\ntoyota corolla\n\n\n357\n32.9\n4\n119.0\n100.0\n2615.0\n14.8\n81\n3\ndatsun 200sx\n\n\n358\n31.6\n4\n120.0\n74.0\n2635.0\n18.3\n81\n3\nmazda 626\n\n\n359\n28.1\n4\n141.0\n80.0\n3230.0\n20.4\n81\n2\npeugeot 505s turbo diesel\n\n\n360\n30.7\n6\n145.0\n76.0\n3160.0\n19.6\n81\n2\nvolvo diesel\n\n\n361\n25.4\n6\n168.0\n116.0\n2900.0\n12.6\n81\n3\ntoyota cressida\n\n\n362\n24.2\n6\n146.0\n120.0\n2930.0\n13.8\n81\n3\ndatsun 810 maxima\n\n\n363\n22.4\n6\n231.0\n110.0\n3415.0\n15.8\n81\n1\nbuick century\n\n\n364\n26.6\n8\n350.0\n105.0\n3725.0\n19.0\n81\n1\noldsmobile cutlass ls\n\n\n365\n20.2\n6\n200.0\n88.0\n3060.0\n17.1\n81\n1\nford granada gl\n\n\n366\n17.6\n6\n225.0\n85.0\n3465.0\n16.6\n81\n1\nchrysler lebaron salon\n\n\n367\n28.0\n4\n112.0\n88.0\n2605.0\n19.6\n82\n1\nchevrolet cavalier\n\n\n368\n27.0\n4\n112.0\n88.0\n2640.0\n18.6\n82\n1\nchevrolet cavalier wagon\n\n\n369\n34.0\n4\n112.0\n88.0\n2395.0\n18.0\n82\n1\nchevrolet cavalier 2-door\n\n\n370\n31.0\n4\n112.0\n85.0\n2575.0\n16.2\n82\n1\npontiac j2000 se hatchback\n\n\n371\n29.0\n4\n135.0\n84.0\n2525.0\n16.0\n82\n1\ndodge aries se\n\n\n372\n27.0\n4\n151.0\n90.0\n2735.0\n18.0\n82\n1\npontiac phoenix\n\n\n373\n24.0\n4\n140.0\n92.0\n2865.0\n16.4\n82\n1\nford fairmont futura\n\n\n374\n36.0\n4\n105.0\n74.0\n1980.0\n15.3\n82\n2\nvolkswagen rabbit l\n\n\n375\n37.0\n4\n91.0\n68.0\n2025.0\n18.2\n82\n3\nmazda glc custom l\n\n\n376\n31.0\n4\n91.0\n68.0\n1970.0\n17.6\n82\n3\nmazda glc custom\n\n\n377\n38.0\n4\n105.0\n63.0\n2125.0\n14.7\n82\n1\nplymouth horizon miser\n\n\n378\n36.0\n4\n98.0\n70.0\n2125.0\n17.3\n82\n1\nmercury lynx l\n\n\n379\n36.0\n4\n120.0\n88.0\n2160.0\n14.5\n82\n3\nnissan stanza xe\n\n\n380\n36.0\n4\n107.0\n75.0\n2205.0\n14.5\n82\n3\nhonda accord\n\n\n381\n34.0\n4\n108.0\n70.0\n2245.0\n16.9\n82\n3\ntoyota corolla\n\n\n382\n38.0\n4\n91.0\n67.0\n1965.0\n15.0\n82\n3\nhonda civic\n\n\n383\n32.0\n4\n91.0\n67.0\n1965.0\n15.7\n82\n3\nhonda civic (auto)\n\n\n384\n38.0\n4\n91.0\n67.0\n1995.0\n16.2\n82\n3\ndatsun 310 gx\n\n\n385\n25.0\n6\n181.0\n110.0\n2945.0\n16.4\n82\n1\nbuick century limited\n\n\n386\n38.0\n6\n262.0\n85.0\n3015.0\n17.0\n82\n1\noldsmobile cutlass ciera (diesel)\n\n\n387\n26.0\n4\n156.0\n92.0\n2585.0\n14.5\n82\n1\nchrysler lebaron medallion\n\n\n388\n22.0\n6\n232.0\n112.0\n2835.0\n14.7\n82\n1\nford granada l\n\n\n389\n32.0\n4\n144.0\n96.0\n2665.0\n13.9\n82\n3\ntoyota celica gt\n\n\n390\n36.0\n4\n135.0\n84.0\n2370.0\n13.0\n82\n1\ndodge charger 2.2\n\n\n391\n27.0\n4\n151.0\n90.0\n2950.0\n17.3\n82\n1\nchevrolet camaro\n\n\n392\n27.0\n4\n140.0\n86.0\n2790.0\n15.6\n82\n1\nford mustang gl\n\n\n393\n44.0\n4\n97.0\n52.0\n2130.0\n24.6\n82\n2\nvw pickup\n\n\n394\n32.0\n4\n135.0\n84.0\n2295.0\n11.6\n82\n1\ndodge rampage\n\n\n395\n28.0\n4\n120.0\n79.0\n2625.0\n18.6\n82\n1\nford ranger\n\n\n396\n31.0\n4\n119.0\n82.0\n2720.0\n19.4\n82\n1\nchevy s-10\n\n\n\n\n\n\n\nHowever, if we pass in a list of strings to the [] method, then we obtain a data frame containing the corresponding set of columns.\n\nAuto[['mpg', 'horsepower']]\n\n\n\n\n\n\n\n\nmpg\nhorsepower\n\n\n\n\n0\n18.0\n130.0\n\n\n1\n15.0\n165.0\n\n\n2\n18.0\n150.0\n\n\n3\n16.0\n150.0\n\n\n4\n17.0\n140.0\n\n\n...\n...\n...\n\n\n392\n27.0\n86.0\n\n\n393\n44.0\n52.0\n\n\n394\n32.0\n84.0\n\n\n395\n28.0\n79.0\n\n\n396\n31.0\n82.0\n\n\n\n\n392 rows × 2 columns\n\n\n\nSince we did not specify an index column when we loaded our data frame, the rows are labeled using integers 0 to 396.\n\nAuto.index\n\nIndex([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       387, 388, 389, 390, 391, 392, 393, 394, 395, 396],\n      dtype='int64', length=392)\n\n\nWe can use the set_index() method to re-name the rows using the contents of Auto['name'].\n\nAuto_re = Auto.set_index('name')\nAuto_re\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\nchevrolet chevelle malibu\n18.0\n8\n307.0\n130.0\n3504.0\n12.0\n70\n1\n\n\nbuick skylark 320\n15.0\n8\n350.0\n165.0\n3693.0\n11.5\n70\n1\n\n\nplymouth satellite\n18.0\n8\n318.0\n150.0\n3436.0\n11.0\n70\n1\n\n\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433.0\n12.0\n70\n1\n\n\nford torino\n17.0\n8\n302.0\n140.0\n3449.0\n10.5\n70\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nford mustang gl\n27.0\n4\n140.0\n86.0\n2790.0\n15.6\n82\n1\n\n\nvw pickup\n44.0\n4\n97.0\n52.0\n2130.0\n24.6\n82\n2\n\n\ndodge rampage\n32.0\n4\n135.0\n84.0\n2295.0\n11.6\n82\n1\n\n\nford ranger\n28.0\n4\n120.0\n79.0\n2625.0\n18.6\n82\n1\n\n\nchevy s-10\n31.0\n4\n119.0\n82.0\n2720.0\n19.4\n82\n1\n\n\n\n\n392 rows × 8 columns\n\n\n\n\nAuto_re.columns\n\nIndex(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin'],\n      dtype='object')\n\n\nWe see that the column 'name' is no longer there.\nNow that the index has been set to name, we can access rows of the data frame by name using the {loc[]} method of Auto:\n\nrows = ['amc rebel sst', 'ford torino']\nAuto_re.loc[rows]\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433.0\n12.0\n70\n1\n\n\nford torino\n17.0\n8\n302.0\n140.0\n3449.0\n10.5\n70\n1\n\n\n\n\n\n\n\nAs an alternative to using the index name, we could retrieve the 4th and 5th rows of Auto using the {iloc[]} method:\n\nAuto_re.iloc[[3,4]]\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\namc rebel sst\n16.0\n8\n304.0\n150.0\n3433.0\n12.0\n70\n1\n\n\nford torino\n17.0\n8\n302.0\n140.0\n3449.0\n10.5\n70\n1\n\n\n\n\n\n\n\nWe can also use it to retrieve the 1st, 3rd and and 4th columns of Auto_re:\n\nAuto_re.iloc[:,[0,2,3]]\n\n\n\n\n\n\n\n\nmpg\ndisplacement\nhorsepower\n\n\nname\n\n\n\n\n\n\n\nchevrolet chevelle malibu\n18.0\n307.0\n130.0\n\n\nbuick skylark 320\n15.0\n350.0\n165.0\n\n\nplymouth satellite\n18.0\n318.0\n150.0\n\n\namc rebel sst\n16.0\n304.0\n150.0\n\n\nford torino\n17.0\n302.0\n140.0\n\n\n...\n...\n...\n...\n\n\nford mustang gl\n27.0\n140.0\n86.0\n\n\nvw pickup\n44.0\n97.0\n52.0\n\n\ndodge rampage\n32.0\n135.0\n84.0\n\n\nford ranger\n28.0\n120.0\n79.0\n\n\nchevy s-10\n31.0\n119.0\n82.0\n\n\n\n\n392 rows × 3 columns\n\n\n\nWe can extract the 4th and 5th rows, as well as the 1st, 3rd and 4th columns, using a single call to iloc[]:\n\nAuto_re.iloc[[3,4],[0,2,3]]\n\n\n\n\n\n\n\n\nmpg\ndisplacement\nhorsepower\n\n\nname\n\n\n\n\n\n\n\namc rebel sst\n16.0\n304.0\n150.0\n\n\nford torino\n17.0\n302.0\n140.0\n\n\n\n\n\n\n\nIndex entries need not be unique: there are several cars in the data frame named ford galaxie 500.\n\nAuto_re.loc['ford galaxie 500', ['mpg', 'origin']]\n\n\n\n\n\n\n\n\nmpg\norigin\n\n\nname\n\n\n\n\n\n\nford galaxie 500\n15.0\n1\n\n\nford galaxie 500\n14.0\n1\n\n\nford galaxie 500\n14.0\n1\n\n\n\n\n\n\n\n\n\nMore on Selecting Rows and Columns\nSuppose now that we want to create a data frame consisting of the weight and origin of the subset of cars with year greater than 80 — i.e. those built after 1980. To do this, we first create a Boolean array that indexes the rows. The loc[] method allows for Boolean entries as well as strings:\n\nidx_80 = Auto_re['year'] &gt; 80\nAuto_re.loc[idx_80, ['weight', 'origin']]\n\n\n\n\n\n\n\n\nweight\norigin\n\n\nname\n\n\n\n\n\n\nplymouth reliant\n2490.0\n1\n\n\nbuick skylark\n2635.0\n1\n\n\ndodge aries wagon (sw)\n2620.0\n1\n\n\nchevrolet citation\n2725.0\n1\n\n\nplymouth reliant\n2385.0\n1\n\n\ntoyota starlet\n1755.0\n3\n\n\nplymouth champ\n1875.0\n1\n\n\nhonda civic 1300\n1760.0\n3\n\n\nsubaru\n2065.0\n3\n\n\ndatsun 210 mpg\n1975.0\n3\n\n\ntoyota tercel\n2050.0\n3\n\n\nmazda glc 4\n1985.0\n3\n\n\nplymouth horizon 4\n2215.0\n1\n\n\nford escort 4w\n2045.0\n1\n\n\nford escort 2h\n2380.0\n1\n\n\nvolkswagen jetta\n2190.0\n2\n\n\nhonda prelude\n2210.0\n3\n\n\ntoyota corolla\n2350.0\n3\n\n\ndatsun 200sx\n2615.0\n3\n\n\nmazda 626\n2635.0\n3\n\n\npeugeot 505s turbo diesel\n3230.0\n2\n\n\nvolvo diesel\n3160.0\n2\n\n\ntoyota cressida\n2900.0\n3\n\n\ndatsun 810 maxima\n2930.0\n3\n\n\nbuick century\n3415.0\n1\n\n\noldsmobile cutlass ls\n3725.0\n1\n\n\nford granada gl\n3060.0\n1\n\n\nchrysler lebaron salon\n3465.0\n1\n\n\nchevrolet cavalier\n2605.0\n1\n\n\nchevrolet cavalier wagon\n2640.0\n1\n\n\nchevrolet cavalier 2-door\n2395.0\n1\n\n\npontiac j2000 se hatchback\n2575.0\n1\n\n\ndodge aries se\n2525.0\n1\n\n\npontiac phoenix\n2735.0\n1\n\n\nford fairmont futura\n2865.0\n1\n\n\nvolkswagen rabbit l\n1980.0\n2\n\n\nmazda glc custom l\n2025.0\n3\n\n\nmazda glc custom\n1970.0\n3\n\n\nplymouth horizon miser\n2125.0\n1\n\n\nmercury lynx l\n2125.0\n1\n\n\nnissan stanza xe\n2160.0\n3\n\n\nhonda accord\n2205.0\n3\n\n\ntoyota corolla\n2245.0\n3\n\n\nhonda civic\n1965.0\n3\n\n\nhonda civic (auto)\n1965.0\n3\n\n\ndatsun 310 gx\n1995.0\n3\n\n\nbuick century limited\n2945.0\n1\n\n\noldsmobile cutlass ciera (diesel)\n3015.0\n1\n\n\nchrysler lebaron medallion\n2585.0\n1\n\n\nford granada l\n2835.0\n1\n\n\ntoyota celica gt\n2665.0\n3\n\n\ndodge charger 2.2\n2370.0\n1\n\n\nchevrolet camaro\n2950.0\n1\n\n\nford mustang gl\n2790.0\n1\n\n\nvw pickup\n2130.0\n2\n\n\ndodge rampage\n2295.0\n1\n\n\nford ranger\n2625.0\n1\n\n\nchevy s-10\n2720.0\n1\n\n\n\n\n\n\n\nTo do this more concisely, we can use an anonymous function called a lambda:\n\nAuto_re.loc[lambda df: df['year'] &gt; 80, ['weight', 'origin']]\n\n\n\n\n\n\n\n\nweight\norigin\n\n\nname\n\n\n\n\n\n\nplymouth reliant\n2490.0\n1\n\n\nbuick skylark\n2635.0\n1\n\n\ndodge aries wagon (sw)\n2620.0\n1\n\n\nchevrolet citation\n2725.0\n1\n\n\nplymouth reliant\n2385.0\n1\n\n\ntoyota starlet\n1755.0\n3\n\n\nplymouth champ\n1875.0\n1\n\n\nhonda civic 1300\n1760.0\n3\n\n\nsubaru\n2065.0\n3\n\n\ndatsun 210 mpg\n1975.0\n3\n\n\ntoyota tercel\n2050.0\n3\n\n\nmazda glc 4\n1985.0\n3\n\n\nplymouth horizon 4\n2215.0\n1\n\n\nford escort 4w\n2045.0\n1\n\n\nford escort 2h\n2380.0\n1\n\n\nvolkswagen jetta\n2190.0\n2\n\n\nhonda prelude\n2210.0\n3\n\n\ntoyota corolla\n2350.0\n3\n\n\ndatsun 200sx\n2615.0\n3\n\n\nmazda 626\n2635.0\n3\n\n\npeugeot 505s turbo diesel\n3230.0\n2\n\n\nvolvo diesel\n3160.0\n2\n\n\ntoyota cressida\n2900.0\n3\n\n\ndatsun 810 maxima\n2930.0\n3\n\n\nbuick century\n3415.0\n1\n\n\noldsmobile cutlass ls\n3725.0\n1\n\n\nford granada gl\n3060.0\n1\n\n\nchrysler lebaron salon\n3465.0\n1\n\n\nchevrolet cavalier\n2605.0\n1\n\n\nchevrolet cavalier wagon\n2640.0\n1\n\n\nchevrolet cavalier 2-door\n2395.0\n1\n\n\npontiac j2000 se hatchback\n2575.0\n1\n\n\ndodge aries se\n2525.0\n1\n\n\npontiac phoenix\n2735.0\n1\n\n\nford fairmont futura\n2865.0\n1\n\n\nvolkswagen rabbit l\n1980.0\n2\n\n\nmazda glc custom l\n2025.0\n3\n\n\nmazda glc custom\n1970.0\n3\n\n\nplymouth horizon miser\n2125.0\n1\n\n\nmercury lynx l\n2125.0\n1\n\n\nnissan stanza xe\n2160.0\n3\n\n\nhonda accord\n2205.0\n3\n\n\ntoyota corolla\n2245.0\n3\n\n\nhonda civic\n1965.0\n3\n\n\nhonda civic (auto)\n1965.0\n3\n\n\ndatsun 310 gx\n1995.0\n3\n\n\nbuick century limited\n2945.0\n1\n\n\noldsmobile cutlass ciera (diesel)\n3015.0\n1\n\n\nchrysler lebaron medallion\n2585.0\n1\n\n\nford granada l\n2835.0\n1\n\n\ntoyota celica gt\n2665.0\n3\n\n\ndodge charger 2.2\n2370.0\n1\n\n\nchevrolet camaro\n2950.0\n1\n\n\nford mustang gl\n2790.0\n1\n\n\nvw pickup\n2130.0\n2\n\n\ndodge rampage\n2295.0\n1\n\n\nford ranger\n2625.0\n1\n\n\nchevy s-10\n2720.0\n1\n\n\n\n\n\n\n\nThe lambda call creates a function that takes a single argument, here df, and returns df['year']&gt;80. Since it is created inside the loc[] method for the dataframe Auto_re, that dataframe will be the argument supplied. As another example of using a lambda, suppose that we want all cars built after 1980 that achieve greater than 30 miles per gallon:\n\nAuto_re.loc[lambda df: (df['year'] &gt; 80) & (df['mpg'] &gt; 30),\n            ['weight', 'origin']\n           ]\n\n\n\n\n\n\n\n\nweight\norigin\n\n\nname\n\n\n\n\n\n\ntoyota starlet\n1755.0\n3\n\n\nplymouth champ\n1875.0\n1\n\n\nhonda civic 1300\n1760.0\n3\n\n\nsubaru\n2065.0\n3\n\n\ndatsun 210 mpg\n1975.0\n3\n\n\ntoyota tercel\n2050.0\n3\n\n\nmazda glc 4\n1985.0\n3\n\n\nplymouth horizon 4\n2215.0\n1\n\n\nford escort 4w\n2045.0\n1\n\n\nvolkswagen jetta\n2190.0\n2\n\n\nhonda prelude\n2210.0\n3\n\n\ntoyota corolla\n2350.0\n3\n\n\ndatsun 200sx\n2615.0\n3\n\n\nmazda 626\n2635.0\n3\n\n\nvolvo diesel\n3160.0\n2\n\n\nchevrolet cavalier 2-door\n2395.0\n1\n\n\npontiac j2000 se hatchback\n2575.0\n1\n\n\nvolkswagen rabbit l\n1980.0\n2\n\n\nmazda glc custom l\n2025.0\n3\n\n\nmazda glc custom\n1970.0\n3\n\n\nplymouth horizon miser\n2125.0\n1\n\n\nmercury lynx l\n2125.0\n1\n\n\nnissan stanza xe\n2160.0\n3\n\n\nhonda accord\n2205.0\n3\n\n\ntoyota corolla\n2245.0\n3\n\n\nhonda civic\n1965.0\n3\n\n\nhonda civic (auto)\n1965.0\n3\n\n\ndatsun 310 gx\n1995.0\n3\n\n\noldsmobile cutlass ciera (diesel)\n3015.0\n1\n\n\ntoyota celica gt\n2665.0\n3\n\n\ndodge charger 2.2\n2370.0\n1\n\n\nvw pickup\n2130.0\n2\n\n\ndodge rampage\n2295.0\n1\n\n\nchevy s-10\n2720.0\n1\n\n\n\n\n\n\n\nThe symbol & computes an element-wise and operation. As another example, suppose that we want to retrieve all Ford and Datsun cars with displacement less than 300. We check whether each name entry contains either the string ford or datsun using the str.contains() method of the index attribute of of the dataframe:\n\nAuto_re.loc[lambda df: (df['displacement'] &lt; 300)\n                       & (df.index.str.contains('ford')\n                       | df.index.str.contains('datsun')),\n            ['weight', 'origin']\n           ]\n\n\n\n\n\n\n\n\nweight\norigin\n\n\nname\n\n\n\n\n\n\nford maverick\n2587.0\n1\n\n\ndatsun pl510\n2130.0\n3\n\n\ndatsun pl510\n2130.0\n3\n\n\nford torino 500\n3302.0\n1\n\n\nford mustang\n3139.0\n1\n\n\ndatsun 1200\n1613.0\n3\n\n\nford pinto runabout\n2226.0\n1\n\n\nford pinto (sw)\n2395.0\n1\n\n\ndatsun 510 (sw)\n2288.0\n3\n\n\nford maverick\n3021.0\n1\n\n\ndatsun 610\n2379.0\n3\n\n\nford pinto\n2310.0\n1\n\n\ndatsun b210\n1950.0\n3\n\n\nford pinto\n2451.0\n1\n\n\ndatsun 710\n2003.0\n3\n\n\nford maverick\n3158.0\n1\n\n\nford pinto\n2639.0\n1\n\n\ndatsun 710\n2545.0\n3\n\n\nford pinto\n2984.0\n1\n\n\nford maverick\n3012.0\n1\n\n\nford granada ghia\n3574.0\n1\n\n\ndatsun b-210\n1990.0\n3\n\n\nford pinto\n2565.0\n1\n\n\ndatsun f-10 hatchback\n1945.0\n3\n\n\nford granada\n3525.0\n1\n\n\nford mustang ii 2+2\n2755.0\n1\n\n\ndatsun 810\n2815.0\n3\n\n\nford fiesta\n1800.0\n1\n\n\ndatsun b210 gx\n2070.0\n3\n\n\nford fairmont (auto)\n2965.0\n1\n\n\nford fairmont (man)\n2720.0\n1\n\n\ndatsun 510\n2300.0\n3\n\n\ndatsun 200-sx\n2405.0\n3\n\n\nford fairmont 4\n2890.0\n1\n\n\ndatsun 210\n2020.0\n3\n\n\ndatsun 310\n2019.0\n3\n\n\nford fairmont\n2870.0\n1\n\n\ndatsun 510 hatchback\n2434.0\n3\n\n\ndatsun 210\n2110.0\n3\n\n\ndatsun 280-zx\n2910.0\n3\n\n\ndatsun 210 mpg\n1975.0\n3\n\n\nford escort 4w\n2045.0\n1\n\n\nford escort 2h\n2380.0\n1\n\n\ndatsun 200sx\n2615.0\n3\n\n\ndatsun 810 maxima\n2930.0\n3\n\n\nford granada gl\n3060.0\n1\n\n\nford fairmont futura\n2865.0\n1\n\n\ndatsun 310 gx\n1995.0\n3\n\n\nford granada l\n2835.0\n1\n\n\nford mustang gl\n2790.0\n1\n\n\nford ranger\n2625.0\n1\n\n\n\n\n\n\n\nHere, the symbol | computes an element-wise or operation.\nIn summary, a powerful set of operations is available to index the rows and columns of data frames. For integer based queries, use the iloc[] method. For string and Boolean selections, use the loc[] method. For functional queries that filter rows, use the loc[] method with a function (typically a lambda) in the rows argument."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#for-loops",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#for-loops",
    "title": "Introduction to Python",
    "section": "For Loops",
    "text": "For Loops\nA for loop is a standard tool in many languages that repeatedly evaluates some chunk of code while varying different values inside the code. For example, suppose we loop over elements of a list and compute their sum.\n\ntotal = 0\nfor value in [3,2,19]:\n    total += value\nprint('Total is: {0}'.format(total))\n\nTotal is: 24\n\n\nThe indented code beneath the line with the for statement is run for each value in the sequence specified in the for statement. The loop ends either when the cell ends or when code is indented at the same level as the original for statement. We see that the final line above which prints the total is executed only once after the for loop has terminated. Loops can be nested by additional indentation.\n\ntotal = 0\nfor value in [2,3,19]:\n    for weight in [3, 2, 1]:\n        total += value * weight\nprint('Total is: {0}'.format(total))\n\nTotal is: 144\n\n\nAbove, we summed over each combination of value and weight. We also took advantage of the increment notation in Python: the expression a += b is equivalent to a = a + b. Besides being a convenient notation, this can save time in computationally heavy tasks in which the intermediate value of a+b need not be explicitly created.\nPerhaps a more common task would be to sum over (value, weight) pairs. For instance, to compute the average value of a random variable that takes on possible values 2, 3 or 19 with probability 0.2, 0.3, 0.5 respectively we would compute the weighted sum. Tasks such as this can often be accomplished using the zip() function that loops over a sequence of tuples.\n\ntotal = 0\nfor value, weight in zip([2,3,19],\n                         [0.2,0.3,0.5]):\n    total += weight * value\nprint('Weighted average is: {0}'.format(total))\n\nWeighted average is: 10.8\n\n\n\nString Formatting\nIn the code chunk above we also printed a string displaying the total. However, the object total is an integer and not a string. Inserting the value of something into a string is a common task, made simple using some of the powerful string formatting tools in Python. Many data cleaning tasks involve manipulating and programmatically producing strings.\nFor example we may want to loop over the columns of a data frame and print the percent missing in each column. Let’s create a data frame D with columns in which 20% of the entries are missing i.e. set to np.nan. We’ll create the values in D from a normal distribution with mean 0 and variance 1 using rng.standard_normal() and then overwrite some random entries using rng.choice().\n\nrng = np.random.default_rng(1)\nA = rng.standard_normal((127, 5))\nM = rng.choice([0, np.nan], p=[0.8,0.2], size=A.shape)\nA += M\nD = pd.DataFrame(A, columns=['food',\n                             'bar',\n                             'pickle',\n                             'snack',\n                             'popcorn'])\nD[:3]\n\n\n\n\n\n\n\n\nfood\nbar\npickle\nsnack\npopcorn\n\n\n\n\n0\n0.345584\n0.821618\n0.330437\n-1.303157\nNaN\n\n\n1\nNaN\n-0.536953\n0.581118\n0.364572\n0.294132\n\n\n2\nNaN\n0.546713\nNaN\n-0.162910\n-0.482119\n\n\n\n\n\n\n\n\nfor col in D.columns:\n    template = 'Column \"{0}\" has {1:.2%} missing values'\n    print(template.format(col,\n          np.isnan(D[col]).mean()))\n\nColumn \"food\" has 16.54% missing values\nColumn \"bar\" has 25.98% missing values\nColumn \"pickle\" has 29.13% missing values\nColumn \"snack\" has 21.26% missing values\nColumn \"popcorn\" has 22.83% missing values\n\n\nWe see that the template.format() method expects two arguments {0} and {1:.2%}, and the latter includes some formatting information. In particular, it specifies that the second argument should be expressed as a percent with two decimal digits.\nThe reference docs.python.org/3/library/string.html includes many helpful and more complex examples."
  },
  {
    "objectID": "lecture_slides/01_introduction/02-statlearn-lab.html#additional-graphical-and-numerical-summaries",
    "href": "lecture_slides/01_introduction/02-statlearn-lab.html#additional-graphical-and-numerical-summaries",
    "title": "Introduction to Python",
    "section": "Additional Graphical and Numerical Summaries",
    "text": "Additional Graphical and Numerical Summaries\nWe can use the ax.plot() or ax.scatter() functions to display the quantitative variables. However, simply typing the variable names will produce an error message, because Python does not know to look in the Auto data set for those variables.\n\nfig, ax = subplots(figsize=(8, 8))\nax.plot(horsepower, mpg, 'o');\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[102], line 2\n      1 fig, ax = subplots(figsize=(8, 8))\n----&gt; 2 ax.plot(horsepower, mpg, 'o');\n\nNameError: name 'horsepower' is not defined\n\n\n\n\n\n\n\n\n\n\nWe can address this by accessing the columns directly:\n\nfig, ax = subplots(figsize=(8, 8))\nax.plot(Auto['horsepower'], Auto['mpg'], 'o');\n\n\n\n\n\n\n\n\nAlternatively, we can use the plot() method with the call Auto.plot(). Using this method, the variables can be accessed by name. The plot methods of a data frame return a familiar object: an axes. We can use it to update the plot as we did previously:\n\nax = Auto.plot.scatter('horsepower', 'mpg')\nax.set_title('Horsepower vs. MPG');\n\n\n\n\n\n\n\n\nIf we want to save the figure that contains a given axes, we can find the relevant figure by accessing the figure attribute:\n\nfig = ax.figure\nfig.savefig('horsepower_mpg.png');\n\nWe can further instruct the data frame to plot to a particular axes object. In this case the corresponding plot() method will return the modified axes we passed in as an argument. Note that when we request a one-dimensional grid of plots, the object axes is similarly one-dimensional. We place our scatter plot in the middle plot of a row of three plots within a figure.\n\nfig, axes = subplots(ncols=3, figsize=(15, 5))\nAuto.plot.scatter('horsepower', 'mpg', ax=axes[1]);\n\n\n\n\n\n\n\n\nNote also that the columns of a data frame can be accessed as attributes: try typing in Auto.horsepower.\nWe now consider the cylinders variable. Typing in Auto.cylinders.dtype reveals that it is being treated as a quantitative variable. However, since there is only a small number of possible values for this variable, we may wish to treat it as qualitative. Below, we replace the cylinders column with a categorical version of Auto.cylinders. The function pd.Series() owes its name to the fact that pandas is often used in time series applications.\n\nAuto.cylinders = pd.Series(Auto.cylinders, dtype='category')\nAuto.cylinders.dtype\n\nCategoricalDtype(categories=[3, 4, 5, 6, 8], ordered=False, categories_dtype=int64)\n\n\nNow that cylinders is qualitative, we can display it using the boxplot() method.\n\nfig, ax = subplots(figsize=(8, 8))\nAuto.boxplot('mpg', by='cylinders', ax=ax);\n\n\n\n\n\n\n\n\nThe hist() method can be used to plot a histogram.\n\nfig, ax = subplots(figsize=(8, 8))\nAuto.hist('mpg', ax=ax);\n\n\n\n\n\n\n\n\nThe color of the bars and the number of bins can be changed:\n\nfig, ax = subplots(figsize=(8, 8))\nAuto.hist('mpg', color='red', bins=12, ax=ax);\n\n\n\n\n\n\n\n\nSee Auto.hist? for more plotting options.\nWe can use the pd.plotting.scatter_matrix() function to create a scatterplot matrix to visualize all of the pairwise relationships between the columns in a data frame.\n\npd.plotting.scatter_matrix(Auto);\n\n\n\n\n\n\n\n\nWe can also produce scatterplots for a subset of the variables.\n\npd.plotting.scatter_matrix(Auto[['mpg',\n                                 'displacement',\n                                 'weight']]);\n\n\n\n\n\n\n\n\nThe describe() method produces a numerical summary of each column in a data frame.\n\nAuto[['mpg', 'weight']].describe()\n\n\n\n\n\n\n\n\nmpg\nweight\n\n\n\n\ncount\n392.000000\n392.000000\n\n\nmean\n23.445918\n2977.584184\n\n\nstd\n7.805007\n849.402560\n\n\nmin\n9.000000\n1613.000000\n\n\n25%\n17.000000\n2225.250000\n\n\n50%\n22.750000\n2803.500000\n\n\n75%\n29.000000\n3614.750000\n\n\nmax\n46.600000\n5140.000000\n\n\n\n\n\n\n\nWe can also produce a summary of just a single column.\n\nAuto['cylinders'].describe()\nAuto['mpg'].describe()\n\ncount    392.000000\nmean      23.445918\nstd        7.805007\nmin        9.000000\n25%       17.000000\n50%       22.750000\n75%       29.000000\nmax       46.600000\nName: mpg, dtype: float64\n\n\nTo exit Jupyter, select File / Shut Down."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Binder\n\n\n\n\nWe import our standard libraries at this top level.\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\n\n\n\nThroughout this lab we will introduce new functions and libraries. However, we will import them here to emphasize these are the new code objects in this lab. Keeping imports near the top of a notebook makes the code more readable, since scanning the first few lines tells us what libraries are used.\n\nimport statsmodels.api as sm\n\nWe will provide relevant details about the functions below as they are needed.\nBesides importing whole modules, it is also possible to import only a few items from a given module. This will help keep the namespace clean. We will use a few specific objects from the statsmodels package which we import here.\n\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\nAs one of the import statements above is quite a long line, we inserted a line break \\ to ease readability.\nWe will also use some functions written for the labs in this book in the ISLP package.\n\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n\n\n\n\nThe function dir() provides a list of objects in a namespace.\n\ndir()\n\n['In',\n 'MS',\n 'Out',\n 'VIF',\n '_',\n '__',\n '___',\n '__builtin__',\n '__builtins__',\n '__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_dh',\n '_i',\n '_i1',\n '_i2',\n '_i3',\n '_i4',\n '_i5',\n '_ih',\n '_ii',\n '_iii',\n '_oh',\n 'anova_lm',\n 'exit',\n 'get_ipython',\n 'load_data',\n 'np',\n 'open',\n 'pd',\n 'poly',\n 'quit',\n 'sm',\n 'subplots',\n 'summarize']\n\n\nThis shows you everything that Python can find at the top level. There are certain objects like __builtins__ that contain references to built-in functions like print().\nEvery python object has its own notion of namespace, also accessible with dir(). This will include both the attributes of the object as well as any methods associated with it. For instance, we see 'sum' in the listing for an array.\n\nA = np.array([3,5,11])\ndir(A)\n\n['T',\n '__abs__',\n '__add__',\n '__and__',\n '__array__',\n '__array_finalize__',\n '__array_function__',\n '__array_interface__',\n '__array_prepare__',\n '__array_priority__',\n '__array_struct__',\n '__array_ufunc__',\n '__array_wrap__',\n '__bool__',\n '__buffer__',\n '__class__',\n '__class_getitem__',\n '__complex__',\n '__contains__',\n '__copy__',\n '__deepcopy__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__divmod__',\n '__dlpack__',\n '__dlpack_device__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__iand__',\n '__ifloordiv__',\n '__ilshift__',\n '__imatmul__',\n '__imod__',\n '__imul__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__ior__',\n '__ipow__',\n '__irshift__',\n '__isub__',\n '__iter__',\n '__itruediv__',\n '__ixor__',\n '__le__',\n '__len__',\n '__lshift__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__setitem__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__xor__',\n 'all',\n 'any',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'astype',\n 'base',\n 'byteswap',\n 'choose',\n 'clip',\n 'compress',\n 'conj',\n 'conjugate',\n 'copy',\n 'ctypes',\n 'cumprod',\n 'cumsum',\n 'data',\n 'diagonal',\n 'dot',\n 'dtype',\n 'dump',\n 'dumps',\n 'fill',\n 'flags',\n 'flat',\n 'flatten',\n 'getfield',\n 'imag',\n 'item',\n 'itemset',\n 'itemsize',\n 'max',\n 'mean',\n 'min',\n 'nbytes',\n 'ndim',\n 'newbyteorder',\n 'nonzero',\n 'partition',\n 'prod',\n 'ptp',\n 'put',\n 'ravel',\n 'real',\n 'repeat',\n 'reshape',\n 'resize',\n 'round',\n 'searchsorted',\n 'setfield',\n 'setflags',\n 'shape',\n 'size',\n 'sort',\n 'squeeze',\n 'std',\n 'strides',\n 'sum',\n 'swapaxes',\n 'take',\n 'tobytes',\n 'tofile',\n 'tolist',\n 'tostring',\n 'trace',\n 'transpose',\n 'var',\n 'view']\n\n\nThis indicates that the object A.sum exists. In this case it is a method that can be used to compute the sum of the array A as can be seen by typing A.sum?.\n\nA.sum()\n\n19\n\n\n\n\n\n\nIn this section we will construct model matrices (also called design matrices) using the ModelSpec() transform from ISLP.models.\nWe will use the Boston housing data set, which is contained in the ISLP package. The Boston dataset records medv (median house value) for \\(506\\) neighborhoods around Boston. We will build a regression model to predict medv using \\(13\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940), and lstat (percent of households with low socioeconomic status). We will use statsmodels for this task, a Python package that implements several commonly used regression methods.\nWe have included a simple loading function load_data() in the ISLP package:\n\nBoston = load_data(\"Boston\")\nBoston.columns\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')\n\n\nType Boston? to find out more about these data.\nWe start by using the sm.OLS() function to fit a simple linear regression model. Our response will be medv and lstat will be the single predictor. For this model, we can create the model matrix by hand.\n\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']})\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nWe extract the response, and fit the model.\n\ny = Boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nNote that sm.OLS() does not fit the model; it specifies the model, and then model.fit() does the actual fitting.\nOur ISLP function summarize() produces a simple table of the parameter estimates, their standard errors, t-statistics and p-values. The function takes a single argument, such as the object results returned here by the fit method, and returns such a summary.\n\nsummarize(results)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n34.5538\n0.563\n61.415\n0.0\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.0\n\n\n\n\n\n\n\nBefore we describe other methods for working with fitted models, we outline a more useful and general framework for constructing a model matrix~X. ### Using Transformations: Fit and Transform Our model above has a single predictor, and constructing X was straightforward. In practice we often fit models with more than one predictor, typically selected from an array or data frame. We may wish to introduce transformations to the variables before fitting the model, specify interactions between variables, and expand some particular variables into sets of variables (e.g. polynomials). The sklearn package has a particular notion for this type of task: a transform. A transform is an object that is created with some parameters as arguments. The object has two main methods: fit() and transform().\nWe provide a general approach for specifying models and constructing the model matrix through the transform ModelSpec() in the ISLP library. ModelSpec() (renamed MS() in the preamble) creates a transform object, and then a pair of methods transform() and fit() are used to construct a corresponding model matrix.\nWe first describe this process for our simple regression model using a single predictor lstat in the Boston data frame, but will use it repeatedly in more complex tasks in this and other labs in this book. In our case the transform is created by the expression design = MS(['lstat']).\nThe fit() method takes the original array and may do some initial computations on it, as specified in the transform object. For example, it may compute means and standard deviations for centering and scaling. The transform() method applies the fitted transformation to the array of data, and produces the model matrix.\n\ndesign = MS(['lstat'])\ndesign = design.fit(Boston)\nX = design.transform(Boston)\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nIn this simple case, the fit() method does very little; it simply checks that the variable 'lstat' specified in design exists in Boston. Then transform() constructs the model matrix with two columns: an intercept and the variable lstat.\nThese two operations can be combined with the fit_transform() method.\n\ndesign = MS(['lstat'])\nX = design.fit_transform(Boston)\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nNote that, as in the previous code chunk when the two steps were done separately, the design object is changed as a result of the fit() operation. The power of this pipeline will become clearer when we fit more complex models that involve interactions and transformations.\nLet’s return to our fitted regression model. The object results has several methods that can be used for inference. We already presented a function summarize() for showing the essentials of the fit. For a full and somewhat exhaustive summary of the fit, we can use the summary() method.\n\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nTue, 04 Jun 2024\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n16:19:08\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n\n\n\nOmnibus:\n137.043\nDurbin-Watson:\n0.892\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n291.373\n\n\nSkew:\n1.453\nProb(JB):\n5.36e-64\n\n\nKurtosis:\n5.319\nCond. No.\n29.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe fitted coefficients can also be retrieved as the params attribute of results.\n\nresults.params\n\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n\n\nThe get_prediction() method can be used to obtain predictions, and produce confidence intervals and prediction intervals for the prediction of medv for given values of lstat.\nWe first create a new data frame, in this case containing only the variable lstat, with the values for this variable at which we wish to make predictions. We then use the transform() method of design to create the corresponding model matrix.\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\nnewX = design.transform(new_df)\nnewX\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n5\n\n\n1\n1.0\n10\n\n\n2\n1.0\n15\n\n\n\n\n\n\n\nNext we compute the predictions at newX, and view them by extracting the predicted_mean attribute.\n\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean\n\narray([29.80359411, 25.05334734, 20.30310057])\n\n\nWe can produce confidence intervals for the predicted values.\n\nnew_predictions.conf_int(alpha=0.05)\n\narray([[29.00741194, 30.59977628],\n       [24.47413202, 25.63256267],\n       [19.73158815, 20.87461299]])\n\n\nPrediction intervals are computing by setting obs=True:\n\nnew_predictions.conf_int(obs=True, alpha=0.05)\n\narray([[17.56567478, 42.04151344],\n       [12.82762635, 37.27906833],\n       [ 8.0777421 , 32.52845905]])\n\n\nFor instance, the 95% confidence interval associated with an lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.82, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when lstat equals 10), but the latter are substantially wider.\nNext we will plot medv and lstat using DataFrame.plot.scatter(), and wish to add the regression line to the resulting plot.\n\n\nWhile there is a function within the ISLP package that adds a line to an existing plot, we take this opportunity to define our first function to do so.\n\ndef abline(ax, b, m):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim)\n\nA few things are illustrated above. First we see the syntax for defining a function: def funcname(...). The function has arguments ax, b, m where ax is an axis object for an exisiting plot, b is the intercept and m is the slope of the desired line. Other plotting options can be passed on to ax.plot by including additional optional arguments as follows:\n\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n\nThe addition of *args allows any number of non-named arguments to abline, while *kwargs allows any number of named arguments (such as linewidth=3) to abline. In our function, we pass these arguments verbatim to ax.plot above. Readers interested in learning more about functions are referred to the section on defining functions in docs.python.org/tutorial.\nLet’s use our new function to add this regression line to a plot of medv vs. lstat.\n\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n\n\n\n\n\n\n\n\nThus, the final call to ax.plot() is ax.plot(xlim, ylim, 'r--', linewidth=3). We have used the argument 'r--' to produce a red dashed line, and added an argument to make it of width 3. There is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nAs mentioned above, there is an existing function to add a line to a plot — ax.axline() — but knowing how to write such functions empowers us to create more expressive displays.\nNext we examine some diagnostic plots, several of which were discussed in Section~\\(\\ref{Ch3:problems.sec}\\). We can find the fitted values and residuals of the fit as attributes of the results object. Various influence measures describing the regression model are computed with the get_influence() method. As we will not use the fig component returned as the first value from subplots(), we simply capture the second returned value in ax below.\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n\n\n\n\n\n\n\nWe add a horizontal line at 0 for reference using the ax.axhline() method, indicating it should be black (c='k') and have a dashed linestyle (ls='--').\nOn the basis of the residual plot, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hat_matrix_diag attribute of the value returned by the get_influence() method.\n\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n\n374\n\n\n\n\n\n\n\n\n\nThe np.argmax() function identifies the index of the largest element of an array, optionally computed over an axis of the array. In this case, we maximized over the entire array to determine which observation has the largest leverage statistic.\n\n\n\n\nIn order to fit a multiple linear regression model using least squares, we again use the ModelSpec() transform to construct the required model matrix and response. The arguments to ModelSpec() can be quite general, but in this case a list of column names suffice. We consider a fit here with the two variables lstat and age.\n\nX = MS(['lstat', 'age']).fit_transform(Boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nsummarize(results1)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n33.2228\n0.731\n45.458\n0.000\n\n\nlstat\n-1.0321\n0.048\n-21.416\n0.000\n\n\nage\n0.0345\n0.012\n2.826\n0.005\n\n\n\n\n\n\n\nNotice how we have compacted the first line into a succinct expression describing the construction of X.\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nterms = Boston.columns.drop('medv')\nterms\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat'],\n      dtype='object')\n\n\nWe can now fit the model with all the variables in terms using the same model matrix builder.\n\nX = MS(terms).fit_transform(Boston)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nsummarize(results)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n41.6173\n4.936\n8.431\n0.000\n\n\ncrim\n-0.1214\n0.033\n-3.678\n0.000\n\n\nzn\n0.0470\n0.014\n3.384\n0.001\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n\n\nchas\n2.8400\n0.870\n3.264\n0.001\n\n\nnox\n-18.7580\n3.851\n-4.870\n0.000\n\n\nrm\n3.6581\n0.420\n8.705\n0.000\n\n\nage\n0.0036\n0.013\n0.271\n0.787\n\n\ndis\n-1.4908\n0.202\n-7.394\n0.000\n\n\nrad\n0.2894\n0.067\n4.325\n0.000\n\n\ntax\n-0.0127\n0.004\n-3.337\n0.001\n\n\nptratio\n-0.9375\n0.132\n-7.091\n0.000\n\n\nlstat\n-0.5520\n0.051\n-10.897\n0.000\n\n\n\n\n\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nminus_age = Boston.columns.drop(['medv', 'age']) \nXma = MS(minus_age).fit_transform(Boston)\nmodel1 = sm.OLS(y, Xma)\nsummarize(model1.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n41.5251\n4.920\n8.441\n0.000\n\n\ncrim\n-0.1214\n0.033\n-3.683\n0.000\n\n\nzn\n0.0465\n0.014\n3.379\n0.001\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n\n\nchas\n2.8528\n0.868\n3.287\n0.001\n\n\nnox\n-18.4851\n3.714\n-4.978\n0.000\n\n\nrm\n3.6811\n0.411\n8.951\n0.000\n\n\ndis\n-1.5068\n0.193\n-7.825\n0.000\n\n\nrad\n0.2879\n0.067\n4.322\n0.000\n\n\ntax\n-0.0127\n0.004\n-3.333\n0.001\n\n\nptratio\n-0.9346\n0.132\n-7.099\n0.000\n\n\nlstat\n-0.5474\n0.048\n-11.483\n0.000\n\n\n\n\n\n\n\n\n\n\nWe can access the individual components of results by name (dir(results) shows us what is available). Hence results.rsquared gives us the \\(R^2\\), and np.sqrt(results.scale) gives us the RSE.\nVariance inflation factors (section~\\(\\ref{Ch3:problems.sec}\\)) are sometimes useful to assess the effect of collinearity in the model matrix of a regression model. We will compute the VIFs in our multiple regression fit, and use the opportunity to introduce the idea of list comprehension.\n\n\nOften we encounter a sequence of objects which we would like to transform for some other task. Below, we compute the VIF for each feature in our X matrix and produce a data frame whose index agrees with the columns of X. The notion of list comprehension can often make such a task easier.\nList comprehensions are simple and powerful ways to form lists of Python objects. The language also supports dictionary and generator comprehension, though these are beyond our scope here. Let’s look at an example. We compute the VIF for each of the variables in the model matrix X, using the function variance_inflation_factor().\n\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])]\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif\n\n\n\n\n\n\n\n\nvif\n\n\n\n\ncrim\n1.767486\n\n\nzn\n2.298459\n\n\nindus\n3.987181\n\n\nchas\n1.071168\n\n\nnox\n4.369093\n\n\nrm\n1.912532\n\n\nage\n3.088232\n\n\ndis\n3.954037\n\n\nrad\n7.445301\n\n\ntax\n9.002158\n\n\nptratio\n1.797060\n\n\nlstat\n2.870777\n\n\n\n\n\n\n\nThe function VIF() takes two arguments: a dataframe or array, and a variable column index. In the code above we call VIF() on the fly for all columns in X.\nWe have excluded column 0 above (the intercept), which is not of interest. In this case the VIFs are not that exciting.\nThe object vals above could have been constructed with the following for loop:\n\nvals = []\nfor i in range(1, X.values.shape[1]):\n    vals.append(VIF(X.values, i))\n\nList comprehension allows us to perform such repetitive operations in a more straightforward way. ## Interaction Terms It is easy to include interaction terms in a linear model using ModelSpec(). Including a tuple (\"lstat\",\"age\") tells the model matrix builder to include an interaction term between lstat and age.\n\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston)\nmodel2 = sm.OLS(y, X)\nsummarize(model2.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n36.0885\n1.470\n24.553\n0.000\n\n\nlstat\n-1.3921\n0.167\n-8.313\n0.000\n\n\nage\n-0.0007\n0.020\n-0.036\n0.971\n\n\nlstat:age\n0.0042\n0.002\n2.244\n0.025\n\n\n\n\n\n\n\n\n\n\n\nThe model matrix builder can include terms beyond just column names and interactions. For instance, the poly() function supplied in ISLP specifies that columns representing polynomial functions of its first argument are added to the model matrix.\n\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\nmodel3 = sm.OLS(y, X)\nresults3 = model3.fit()\nsummarize(results3)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n17.7151\n0.781\n22.681\n0.0\n\n\npoly(lstat, degree=2)[0]\n-179.2279\n6.733\n-26.620\n0.0\n\n\npoly(lstat, degree=2)[1]\n72.9908\n5.482\n13.315\n0.0\n\n\nage\n0.0703\n0.011\n6.471\n0.0\n\n\n\n\n\n\n\nThe effectively zero p-value associated with the quadratic term (i.e. the third row above) suggests that it leads to an improved model.\nBy default, poly() creates a basis matrix for inclusion in the model matrix whose columns are orthogonal polynomials, which are designed for stable least squares computations. {Actually, poly() is a wrapper for the workhorse and standalone function Poly() that does the work in building the model matrix.} Alternatively, had we included an argument raw=True in the above call to poly(), the basis matrix would consist simply of lstat and lstat**2. Since either of these bases represent quadratic polynomials, the fitted values would not change in this case, just the polynomial coefficients. Also by default, the columns created by poly() do not include an intercept column as that is automatically added by MS().\nWe use the anova_lm() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nanova_lm(results1, results3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n503.0\n19168.128609\n0.0\nNaN\nNaN\nNaN\n\n\n1\n502.0\n14165.613251\n1.0\n5002.515357\n177.278785\n7.468491e-35\n\n\n\n\n\n\n\nHere results1 represents the linear submodel containing predictors lstat and age, while results3 corresponds to the larger model above with a quadratic term in lstat. The anova_lm() function performs a hypothesis test comparing the two models. The null hypothesis is that the quadratic term in the bigger model is not needed, and the alternative hypothesis is that the bigger model is superior. Here the F-statistic is 177.28 and the associated p-value is zero. In this case the F-statistic is the square of the t-statistic for the quadratic term in the linear model summary for results3 — a consequence of the fact that these nested models differ by one degree of freedom. This provides very clear evidence that the quadratic polynomial in lstat improves the linear model. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat.\nThe function anova_lm() can take more than two nested models as input, in which case it compares every successive pair of models. That also explains why their are NaNs in the first row above, since there is no previous model with which to compare the first.\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results3.fittedvalues, results3.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n\n\n\n\n\n\n\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly().\n\n\n\nHere we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\n\nCarseats = load_data('Carseats')\nCarseats.columns\n\nIndex(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],\n      dtype='object')\n\n\nThe Carseats\ndata includes qualitative predictors such as ShelveLoc, an indicator of the quality of the shelving location — that is, the space within a store in which the car seat is displayed. The predictor ShelveLoc takes on three possible values, Bad, Medium, and Good. Given a qualitative variable such as ShelveLoc, ModelSpec() generates dummy variables automatically. These variables are often referred to as a one-hot encoding of the categorical feature. Their columns sum to one, so to avoid collinearity with an intercept, the first column is dropped. Below we see the column ShelveLoc[Bad] has been dropped, since Bad is the first level of ShelveLoc. Below we fit a multiple regression model that includes some interaction terms.\n\nallvars = list(Carseats.columns.drop('Sales'))\ny = Carseats['Sales']\nfinal = allvars + [('Income', 'Advertising'),\n                   ('Price', 'Age')]\nX = MS(final).fit_transform(Carseats)\nmodel = sm.OLS(y, X)\nsummarize(model.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n6.5756\n1.009\n6.519\n0.000\n\n\nCompPrice\n0.0929\n0.004\n22.567\n0.000\n\n\nIncome\n0.0109\n0.003\n4.183\n0.000\n\n\nAdvertising\n0.0702\n0.023\n3.107\n0.002\n\n\nPopulation\n0.0002\n0.000\n0.433\n0.665\n\n\nPrice\n-0.1008\n0.007\n-13.549\n0.000\n\n\nShelveLoc[Good]\n4.8487\n0.153\n31.724\n0.000\n\n\nShelveLoc[Medium]\n1.9533\n0.126\n15.531\n0.000\n\n\nAge\n-0.0579\n0.016\n-3.633\n0.000\n\n\nEducation\n-0.0209\n0.020\n-1.063\n0.288\n\n\nUrban[Yes]\n0.1402\n0.112\n1.247\n0.213\n\n\nUS[Yes]\n-0.1576\n0.149\n-1.058\n0.291\n\n\nIncome:Advertising\n0.0008\n0.000\n2.698\n0.007\n\n\nPrice:Age\n0.0001\n0.000\n0.801\n0.424\n\n\n\n\n\n\n\nIn the first line above, we made allvars a list, so that we could add the interaction terms two lines down. Our model-matrix builder has created a ShelveLoc[Good] dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLoc[Medium] dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLoc[Good] in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLoc[Medium] has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location, but lower sales than a good shelving location."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#importing-packages",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#importing-packages",
    "title": "Linear Regression",
    "section": "",
    "text": "We import our standard libraries at this top level.\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\n\n\n\nThroughout this lab we will introduce new functions and libraries. However, we will import them here to emphasize these are the new code objects in this lab. Keeping imports near the top of a notebook makes the code more readable, since scanning the first few lines tells us what libraries are used.\n\nimport statsmodels.api as sm\n\nWe will provide relevant details about the functions below as they are needed.\nBesides importing whole modules, it is also possible to import only a few items from a given module. This will help keep the namespace clean. We will use a few specific objects from the statsmodels package which we import here.\n\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\nAs one of the import statements above is quite a long line, we inserted a line break \\ to ease readability.\nWe will also use some functions written for the labs in this book in the ISLP package.\n\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n\n\n\n\nThe function dir() provides a list of objects in a namespace.\n\ndir()\n\n['In',\n 'MS',\n 'Out',\n 'VIF',\n '_',\n '__',\n '___',\n '__builtin__',\n '__builtins__',\n '__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_dh',\n '_i',\n '_i1',\n '_i2',\n '_i3',\n '_i4',\n '_i5',\n '_ih',\n '_ii',\n '_iii',\n '_oh',\n 'anova_lm',\n 'exit',\n 'get_ipython',\n 'load_data',\n 'np',\n 'open',\n 'pd',\n 'poly',\n 'quit',\n 'sm',\n 'subplots',\n 'summarize']\n\n\nThis shows you everything that Python can find at the top level. There are certain objects like __builtins__ that contain references to built-in functions like print().\nEvery python object has its own notion of namespace, also accessible with dir(). This will include both the attributes of the object as well as any methods associated with it. For instance, we see 'sum' in the listing for an array.\n\nA = np.array([3,5,11])\ndir(A)\n\n['T',\n '__abs__',\n '__add__',\n '__and__',\n '__array__',\n '__array_finalize__',\n '__array_function__',\n '__array_interface__',\n '__array_prepare__',\n '__array_priority__',\n '__array_struct__',\n '__array_ufunc__',\n '__array_wrap__',\n '__bool__',\n '__buffer__',\n '__class__',\n '__class_getitem__',\n '__complex__',\n '__contains__',\n '__copy__',\n '__deepcopy__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__divmod__',\n '__dlpack__',\n '__dlpack_device__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__iand__',\n '__ifloordiv__',\n '__ilshift__',\n '__imatmul__',\n '__imod__',\n '__imul__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__ior__',\n '__ipow__',\n '__irshift__',\n '__isub__',\n '__iter__',\n '__itruediv__',\n '__ixor__',\n '__le__',\n '__len__',\n '__lshift__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__setitem__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__xor__',\n 'all',\n 'any',\n 'argmax',\n 'argmin',\n 'argpartition',\n 'argsort',\n 'astype',\n 'base',\n 'byteswap',\n 'choose',\n 'clip',\n 'compress',\n 'conj',\n 'conjugate',\n 'copy',\n 'ctypes',\n 'cumprod',\n 'cumsum',\n 'data',\n 'diagonal',\n 'dot',\n 'dtype',\n 'dump',\n 'dumps',\n 'fill',\n 'flags',\n 'flat',\n 'flatten',\n 'getfield',\n 'imag',\n 'item',\n 'itemset',\n 'itemsize',\n 'max',\n 'mean',\n 'min',\n 'nbytes',\n 'ndim',\n 'newbyteorder',\n 'nonzero',\n 'partition',\n 'prod',\n 'ptp',\n 'put',\n 'ravel',\n 'real',\n 'repeat',\n 'reshape',\n 'resize',\n 'round',\n 'searchsorted',\n 'setfield',\n 'setflags',\n 'shape',\n 'size',\n 'sort',\n 'squeeze',\n 'std',\n 'strides',\n 'sum',\n 'swapaxes',\n 'take',\n 'tobytes',\n 'tofile',\n 'tolist',\n 'tostring',\n 'trace',\n 'transpose',\n 'var',\n 'view']\n\n\nThis indicates that the object A.sum exists. In this case it is a method that can be used to compute the sum of the array A as can be seen by typing A.sum?.\n\nA.sum()\n\n19"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#simple-linear-regression",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#simple-linear-regression",
    "title": "Linear Regression",
    "section": "",
    "text": "In this section we will construct model matrices (also called design matrices) using the ModelSpec() transform from ISLP.models.\nWe will use the Boston housing data set, which is contained in the ISLP package. The Boston dataset records medv (median house value) for \\(506\\) neighborhoods around Boston. We will build a regression model to predict medv using \\(13\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940), and lstat (percent of households with low socioeconomic status). We will use statsmodels for this task, a Python package that implements several commonly used regression methods.\nWe have included a simple loading function load_data() in the ISLP package:\n\nBoston = load_data(\"Boston\")\nBoston.columns\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')\n\n\nType Boston? to find out more about these data.\nWe start by using the sm.OLS() function to fit a simple linear regression model. Our response will be medv and lstat will be the single predictor. For this model, we can create the model matrix by hand.\n\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']})\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nWe extract the response, and fit the model.\n\ny = Boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\nNote that sm.OLS() does not fit the model; it specifies the model, and then model.fit() does the actual fitting.\nOur ISLP function summarize() produces a simple table of the parameter estimates, their standard errors, t-statistics and p-values. The function takes a single argument, such as the object results returned here by the fit method, and returns such a summary.\n\nsummarize(results)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n34.5538\n0.563\n61.415\n0.0\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.0\n\n\n\n\n\n\n\nBefore we describe other methods for working with fitted models, we outline a more useful and general framework for constructing a model matrix~X. ### Using Transformations: Fit and Transform Our model above has a single predictor, and constructing X was straightforward. In practice we often fit models with more than one predictor, typically selected from an array or data frame. We may wish to introduce transformations to the variables before fitting the model, specify interactions between variables, and expand some particular variables into sets of variables (e.g. polynomials). The sklearn package has a particular notion for this type of task: a transform. A transform is an object that is created with some parameters as arguments. The object has two main methods: fit() and transform().\nWe provide a general approach for specifying models and constructing the model matrix through the transform ModelSpec() in the ISLP library. ModelSpec() (renamed MS() in the preamble) creates a transform object, and then a pair of methods transform() and fit() are used to construct a corresponding model matrix.\nWe first describe this process for our simple regression model using a single predictor lstat in the Boston data frame, but will use it repeatedly in more complex tasks in this and other labs in this book. In our case the transform is created by the expression design = MS(['lstat']).\nThe fit() method takes the original array and may do some initial computations on it, as specified in the transform object. For example, it may compute means and standard deviations for centering and scaling. The transform() method applies the fitted transformation to the array of data, and produces the model matrix.\n\ndesign = MS(['lstat'])\ndesign = design.fit(Boston)\nX = design.transform(Boston)\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nIn this simple case, the fit() method does very little; it simply checks that the variable 'lstat' specified in design exists in Boston. Then transform() constructs the model matrix with two columns: an intercept and the variable lstat.\nThese two operations can be combined with the fit_transform() method.\n\ndesign = MS(['lstat'])\nX = design.fit_transform(Boston)\nX[:4]\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n\n\n\n\n\nNote that, as in the previous code chunk when the two steps were done separately, the design object is changed as a result of the fit() operation. The power of this pipeline will become clearer when we fit more complex models that involve interactions and transformations.\nLet’s return to our fitted regression model. The object results has several methods that can be used for inference. We already presented a function summarize() for showing the essentials of the fit. For a full and somewhat exhaustive summary of the fit, we can use the summary() method.\n\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nTue, 04 Jun 2024\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n16:19:08\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n\n\n\nOmnibus:\n137.043\nDurbin-Watson:\n0.892\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n291.373\n\n\nSkew:\n1.453\nProb(JB):\n5.36e-64\n\n\nKurtosis:\n5.319\nCond. No.\n29.7\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe fitted coefficients can also be retrieved as the params attribute of results.\n\nresults.params\n\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n\n\nThe get_prediction() method can be used to obtain predictions, and produce confidence intervals and prediction intervals for the prediction of medv for given values of lstat.\nWe first create a new data frame, in this case containing only the variable lstat, with the values for this variable at which we wish to make predictions. We then use the transform() method of design to create the corresponding model matrix.\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\nnewX = design.transform(new_df)\nnewX\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n5\n\n\n1\n1.0\n10\n\n\n2\n1.0\n15\n\n\n\n\n\n\n\nNext we compute the predictions at newX, and view them by extracting the predicted_mean attribute.\n\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean\n\narray([29.80359411, 25.05334734, 20.30310057])\n\n\nWe can produce confidence intervals for the predicted values.\n\nnew_predictions.conf_int(alpha=0.05)\n\narray([[29.00741194, 30.59977628],\n       [24.47413202, 25.63256267],\n       [19.73158815, 20.87461299]])\n\n\nPrediction intervals are computing by setting obs=True:\n\nnew_predictions.conf_int(obs=True, alpha=0.05)\n\narray([[17.56567478, 42.04151344],\n       [12.82762635, 37.27906833],\n       [ 8.0777421 , 32.52845905]])\n\n\nFor instance, the 95% confidence interval associated with an lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.82, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when lstat equals 10), but the latter are substantially wider.\nNext we will plot medv and lstat using DataFrame.plot.scatter(), and wish to add the regression line to the resulting plot.\n\n\nWhile there is a function within the ISLP package that adds a line to an existing plot, we take this opportunity to define our first function to do so.\n\ndef abline(ax, b, m):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim)\n\nA few things are illustrated above. First we see the syntax for defining a function: def funcname(...). The function has arguments ax, b, m where ax is an axis object for an exisiting plot, b is the intercept and m is the slope of the desired line. Other plotting options can be passed on to ax.plot by including additional optional arguments as follows:\n\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n\nThe addition of *args allows any number of non-named arguments to abline, while *kwargs allows any number of named arguments (such as linewidth=3) to abline. In our function, we pass these arguments verbatim to ax.plot above. Readers interested in learning more about functions are referred to the section on defining functions in docs.python.org/tutorial.\nLet’s use our new function to add this regression line to a plot of medv vs. lstat.\n\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n\n\n\n\n\n\n\n\nThus, the final call to ax.plot() is ax.plot(xlim, ylim, 'r--', linewidth=3). We have used the argument 'r--' to produce a red dashed line, and added an argument to make it of width 3. There is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nAs mentioned above, there is an existing function to add a line to a plot — ax.axline() — but knowing how to write such functions empowers us to create more expressive displays.\nNext we examine some diagnostic plots, several of which were discussed in Section~\\(\\ref{Ch3:problems.sec}\\). We can find the fitted values and residuals of the fit as attributes of the results object. Various influence measures describing the regression model are computed with the get_influence() method. As we will not use the fig component returned as the first value from subplots(), we simply capture the second returned value in ax below.\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n\n\n\n\n\n\n\nWe add a horizontal line at 0 for reference using the ax.axhline() method, indicating it should be black (c='k') and have a dashed linestyle (ls='--').\nOn the basis of the residual plot, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hat_matrix_diag attribute of the value returned by the get_influence() method.\n\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n\n374\n\n\n\n\n\n\n\n\n\nThe np.argmax() function identifies the index of the largest element of an array, optionally computed over an axis of the array. In this case, we maximized over the entire array to determine which observation has the largest leverage statistic."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#multiple-linear-regression",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#multiple-linear-regression",
    "title": "Linear Regression",
    "section": "",
    "text": "In order to fit a multiple linear regression model using least squares, we again use the ModelSpec() transform to construct the required model matrix and response. The arguments to ModelSpec() can be quite general, but in this case a list of column names suffice. We consider a fit here with the two variables lstat and age.\n\nX = MS(['lstat', 'age']).fit_transform(Boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nsummarize(results1)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n33.2228\n0.731\n45.458\n0.000\n\n\nlstat\n-1.0321\n0.048\n-21.416\n0.000\n\n\nage\n0.0345\n0.012\n2.826\n0.005\n\n\n\n\n\n\n\nNotice how we have compacted the first line into a succinct expression describing the construction of X.\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nterms = Boston.columns.drop('medv')\nterms\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat'],\n      dtype='object')\n\n\nWe can now fit the model with all the variables in terms using the same model matrix builder.\n\nX = MS(terms).fit_transform(Boston)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nsummarize(results)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n41.6173\n4.936\n8.431\n0.000\n\n\ncrim\n-0.1214\n0.033\n-3.678\n0.000\n\n\nzn\n0.0470\n0.014\n3.384\n0.001\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n\n\nchas\n2.8400\n0.870\n3.264\n0.001\n\n\nnox\n-18.7580\n3.851\n-4.870\n0.000\n\n\nrm\n3.6581\n0.420\n8.705\n0.000\n\n\nage\n0.0036\n0.013\n0.271\n0.787\n\n\ndis\n-1.4908\n0.202\n-7.394\n0.000\n\n\nrad\n0.2894\n0.067\n4.325\n0.000\n\n\ntax\n-0.0127\n0.004\n-3.337\n0.001\n\n\nptratio\n-0.9375\n0.132\n-7.091\n0.000\n\n\nlstat\n-0.5520\n0.051\n-10.897\n0.000\n\n\n\n\n\n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nminus_age = Boston.columns.drop(['medv', 'age']) \nXma = MS(minus_age).fit_transform(Boston)\nmodel1 = sm.OLS(y, Xma)\nsummarize(model1.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n41.5251\n4.920\n8.441\n0.000\n\n\ncrim\n-0.1214\n0.033\n-3.683\n0.000\n\n\nzn\n0.0465\n0.014\n3.379\n0.001\n\n\nindus\n0.0135\n0.062\n0.217\n0.829\n\n\nchas\n2.8528\n0.868\n3.287\n0.001\n\n\nnox\n-18.4851\n3.714\n-4.978\n0.000\n\n\nrm\n3.6811\n0.411\n8.951\n0.000\n\n\ndis\n-1.5068\n0.193\n-7.825\n0.000\n\n\nrad\n0.2879\n0.067\n4.322\n0.000\n\n\ntax\n-0.0127\n0.004\n-3.333\n0.001\n\n\nptratio\n-0.9346\n0.132\n-7.099\n0.000\n\n\nlstat\n-0.5474\n0.048\n-11.483\n0.000"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#multivariate-goodness-of-fit",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#multivariate-goodness-of-fit",
    "title": "Linear Regression",
    "section": "",
    "text": "We can access the individual components of results by name (dir(results) shows us what is available). Hence results.rsquared gives us the \\(R^2\\), and np.sqrt(results.scale) gives us the RSE.\nVariance inflation factors (section~\\(\\ref{Ch3:problems.sec}\\)) are sometimes useful to assess the effect of collinearity in the model matrix of a regression model. We will compute the VIFs in our multiple regression fit, and use the opportunity to introduce the idea of list comprehension.\n\n\nOften we encounter a sequence of objects which we would like to transform for some other task. Below, we compute the VIF for each feature in our X matrix and produce a data frame whose index agrees with the columns of X. The notion of list comprehension can often make such a task easier.\nList comprehensions are simple and powerful ways to form lists of Python objects. The language also supports dictionary and generator comprehension, though these are beyond our scope here. Let’s look at an example. We compute the VIF for each of the variables in the model matrix X, using the function variance_inflation_factor().\n\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])]\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif\n\n\n\n\n\n\n\n\nvif\n\n\n\n\ncrim\n1.767486\n\n\nzn\n2.298459\n\n\nindus\n3.987181\n\n\nchas\n1.071168\n\n\nnox\n4.369093\n\n\nrm\n1.912532\n\n\nage\n3.088232\n\n\ndis\n3.954037\n\n\nrad\n7.445301\n\n\ntax\n9.002158\n\n\nptratio\n1.797060\n\n\nlstat\n2.870777\n\n\n\n\n\n\n\nThe function VIF() takes two arguments: a dataframe or array, and a variable column index. In the code above we call VIF() on the fly for all columns in X.\nWe have excluded column 0 above (the intercept), which is not of interest. In this case the VIFs are not that exciting.\nThe object vals above could have been constructed with the following for loop:\n\nvals = []\nfor i in range(1, X.values.shape[1]):\n    vals.append(VIF(X.values, i))\n\nList comprehension allows us to perform such repetitive operations in a more straightforward way. ## Interaction Terms It is easy to include interaction terms in a linear model using ModelSpec(). Including a tuple (\"lstat\",\"age\") tells the model matrix builder to include an interaction term between lstat and age.\n\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston)\nmodel2 = sm.OLS(y, X)\nsummarize(model2.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n36.0885\n1.470\n24.553\n0.000\n\n\nlstat\n-1.3921\n0.167\n-8.313\n0.000\n\n\nage\n-0.0007\n0.020\n-0.036\n0.971\n\n\nlstat:age\n0.0042\n0.002\n2.244\n0.025"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#non-linear-transformations-of-the-predictors",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#non-linear-transformations-of-the-predictors",
    "title": "Linear Regression",
    "section": "",
    "text": "The model matrix builder can include terms beyond just column names and interactions. For instance, the poly() function supplied in ISLP specifies that columns representing polynomial functions of its first argument are added to the model matrix.\n\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\nmodel3 = sm.OLS(y, X)\nresults3 = model3.fit()\nsummarize(results3)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n17.7151\n0.781\n22.681\n0.0\n\n\npoly(lstat, degree=2)[0]\n-179.2279\n6.733\n-26.620\n0.0\n\n\npoly(lstat, degree=2)[1]\n72.9908\n5.482\n13.315\n0.0\n\n\nage\n0.0703\n0.011\n6.471\n0.0\n\n\n\n\n\n\n\nThe effectively zero p-value associated with the quadratic term (i.e. the third row above) suggests that it leads to an improved model.\nBy default, poly() creates a basis matrix for inclusion in the model matrix whose columns are orthogonal polynomials, which are designed for stable least squares computations. {Actually, poly() is a wrapper for the workhorse and standalone function Poly() that does the work in building the model matrix.} Alternatively, had we included an argument raw=True in the above call to poly(), the basis matrix would consist simply of lstat and lstat**2. Since either of these bases represent quadratic polynomials, the fitted values would not change in this case, just the polynomial coefficients. Also by default, the columns created by poly() do not include an intercept column as that is automatically added by MS().\nWe use the anova_lm() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nanova_lm(results1, results3)\n\n\n\n\n\n\n\n\ndf_resid\nssr\ndf_diff\nss_diff\nF\nPr(&gt;F)\n\n\n\n\n0\n503.0\n19168.128609\n0.0\nNaN\nNaN\nNaN\n\n\n1\n502.0\n14165.613251\n1.0\n5002.515357\n177.278785\n7.468491e-35\n\n\n\n\n\n\n\nHere results1 represents the linear submodel containing predictors lstat and age, while results3 corresponds to the larger model above with a quadratic term in lstat. The anova_lm() function performs a hypothesis test comparing the two models. The null hypothesis is that the quadratic term in the bigger model is not needed, and the alternative hypothesis is that the bigger model is superior. Here the F-statistic is 177.28 and the associated p-value is zero. In this case the F-statistic is the square of the t-statistic for the quadratic term in the linear model summary for results3 — a consequence of the fact that these nested models differ by one degree of freedom. This provides very clear evidence that the quadratic polynomial in lstat improves the linear model. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat.\nThe function anova_lm() can take more than two nested models as input, in which case it compares every successive pair of models. That also explains why their are NaNs in the first row above, since there is no previous model with which to compare the first.\n\nax = subplots(figsize=(8,8))[1]\nax.scatter(results3.fittedvalues, results3.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n\n\n\n\n\n\n\n\nWe see that when the quadratic term is included in the model, there is little discernible pattern in the residuals. In order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument to poly()."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03-linreg-lab.html#qualitative-predictors",
    "href": "lecture_slides/03_linear_regression/03-linreg-lab.html#qualitative-predictors",
    "title": "Linear Regression",
    "section": "",
    "text": "Here we use the Carseats data, which is included in the ISLP package. We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.\n\nCarseats = load_data('Carseats')\nCarseats.columns\n\nIndex(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],\n      dtype='object')\n\n\nThe Carseats\ndata includes qualitative predictors such as ShelveLoc, an indicator of the quality of the shelving location — that is, the space within a store in which the car seat is displayed. The predictor ShelveLoc takes on three possible values, Bad, Medium, and Good. Given a qualitative variable such as ShelveLoc, ModelSpec() generates dummy variables automatically. These variables are often referred to as a one-hot encoding of the categorical feature. Their columns sum to one, so to avoid collinearity with an intercept, the first column is dropped. Below we see the column ShelveLoc[Bad] has been dropped, since Bad is the first level of ShelveLoc. Below we fit a multiple regression model that includes some interaction terms.\n\nallvars = list(Carseats.columns.drop('Sales'))\ny = Carseats['Sales']\nfinal = allvars + [('Income', 'Advertising'),\n                   ('Price', 'Age')]\nX = MS(final).fit_transform(Carseats)\nmodel = sm.OLS(y, X)\nsummarize(model.fit())\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n6.5756\n1.009\n6.519\n0.000\n\n\nCompPrice\n0.0929\n0.004\n22.567\n0.000\n\n\nIncome\n0.0109\n0.003\n4.183\n0.000\n\n\nAdvertising\n0.0702\n0.023\n3.107\n0.002\n\n\nPopulation\n0.0002\n0.000\n0.433\n0.665\n\n\nPrice\n-0.1008\n0.007\n-13.549\n0.000\n\n\nShelveLoc[Good]\n4.8487\n0.153\n31.724\n0.000\n\n\nShelveLoc[Medium]\n1.9533\n0.126\n15.531\n0.000\n\n\nAge\n-0.0579\n0.016\n-3.633\n0.000\n\n\nEducation\n-0.0209\n0.020\n-1.063\n0.288\n\n\nUrban[Yes]\n0.1402\n0.112\n1.247\n0.213\n\n\nUS[Yes]\n-0.1576\n0.149\n-1.058\n0.291\n\n\nIncome:Advertising\n0.0008\n0.000\n2.698\n0.007\n\n\nPrice:Age\n0.0001\n0.000\n0.801\n0.424\n\n\n\n\n\n\n\nIn the first line above, we made allvars a list, so that we could add the interaction terms two lines down. Our model-matrix builder has created a ShelveLoc[Good] dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLoc[Medium] dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLoc[Good] in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLoc[Medium] has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location, but lower sales than a good shelving location."
  }
]