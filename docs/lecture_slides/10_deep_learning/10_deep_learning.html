<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Professor: Davi Moreira">
  <title>MGMT 47400: Predictive Analytics â€“  MGMT 47400: Predictive Analytics </title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><span style="font-size: 100%;"> MGMT 47400: Predictive Analytics </span></h1>
  <p class="subtitle"><span style="font-size: 150%;"> Deep Learning </span></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Professor: Davi Moreira 
</div>
</div>
</div>

</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>
<div>
<div class="columns">
<div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>XXXX</li>
</ul>
</div><div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>XXX</li>
</ul>
</div></div>
</div>
</section>
<section>
<section id="xxx" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>XXX</h1>

</section>
<section id="deep-learning" class="slide level2 center">
<h2>Deep Learning</h2>
<ul>
<li class="fragment"><p>Neural networks became popular in the 1980s.<br>
Lots of successes, hype, and great conferences: NeurIPS, Snowbird.</p></li>
<li class="fragment"><p>Then along came SVMs, Random Forests, and Boosting in the 1990s, and Neural Networks took a back seat.</p></li>
<li class="fragment"><p>Re-emerged around 2010 as <em>Deep Learning</em>.<br>
By 2020s, very dominant and successful.</p></li>
<li class="fragment"><p>Part of success due to vast improvements in computing power, larger training sets, and software: TensorFlow and PyTorch.</p></li>
<li class="fragment"><p>Much of the credit goes to three pioneers and their students:</p>
<ul>
<li class="fragment"><strong>Yann LeCun</strong>, <strong>Geoffrey Hinton</strong>, and <strong>Yoshua Bengio</strong>,<br>
who received the 2019 ACM Turing Award for their work in Neural Networks.</li>
</ul></li>
</ul>

<img data-src="figs/10_1_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="single-layer-neural-network" class="slide level2 center">
<h2>Single Layer Neural Network</h2>
<p><span class="math display">\[
f(X) = \beta_0 + \sum_{k=1}^{K} \beta_k h_k(X) \\
= \beta_0 + \sum_{k=1}^{K} \beta_k g(w_{k0} + \sum_{j=1}^{p} w_{kj} X_j).
\]</span></p>
<p><strong>Diagram of Single Layer Neural Network</strong></p>

<img data-src="figs/10_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details" class="slide level2 center">
<h2>Details</h2>

<img data-src="figs/10_2-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p><span class="math inline">\(A_k = h_k(X) = g(w_{k0} + \sum_{j=1}^{p} w_{kj} X_j)\)</span> are called the <strong>activations</strong> in the <em>hidden layer</em>.</p></li>
<li class="fragment"><p><span class="math inline">\(g(z)\)</span> is called the <strong>activation function</strong>. Popular examples are the <strong>sigmoid</strong> and <strong>rectified linear</strong> (ReLU), shown in the figure.</p></li>
<li class="fragment"><p>Activation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.</p></li>
<li class="fragment"><p>So the activations are like derived features â€” nonlinear transformations of linear combinations of the features.</p></li>
<li class="fragment"><p>The model is fit by minimizing <span class="math inline">\(\sum_{i=1}^{n} (y_i - f(x_i))^2\)</span> (e.g., for regression).</p></li>
</ul>
</section>
<section id="example-mnist-digits" class="slide level2 center">
<h2>Example: MNIST Digits</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/10_3a-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/10_3b-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li class="fragment"><p><strong>Handwritten digits</strong></p>
<ul>
<li class="fragment"><p><span class="math inline">\(28 \times 28\)</span> grayscale images</p></li>
<li class="fragment"><p>60K train, 10K test images</p></li>
<li class="fragment"><p>Features are the 784 pixel grayscale values <span class="math inline">\(\in (0, 255)\)</span></p></li>
<li class="fragment"><p>Labels are the digit class <span class="math inline">\(0\text{â€“}9\)</span></p></li>
</ul></li>
<li class="fragment"><p><strong>Goal</strong>: Build a classifier to predict the image class.</p></li>
<li class="fragment"><p>We build a two-layer network with:</p>
<ul>
<li class="fragment"><p>256 units at the first layer,</p></li>
<li class="fragment"><p>128 units at the second layer, and</p></li>
<li class="fragment"><p>10 units at the output layer.</p></li>
</ul></li>
<li class="fragment"><p>Along with intercepts (called <em>biases</em>), there are 235,146 parameters (referred to as <em>weights</em>).</p></li>
</ul>
</section>
<section id="section" class="slide level2 center">
<h2></h2>

<img data-src="figs/10_4-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-output-layer" class="slide level2 center">
<h2>Details of Output Layer</h2>
<ul>
<li class="fragment"><p>Let <span class="math inline">\(Z_m = \beta_{m0} + \sum_{\ell=1}^{K_2} \beta_{m\ell} A_\ell^{(2)}\)</span>, <span class="math inline">\(m = 0, 1, \ldots, 9\)</span>, be 10 linear combinations of activations at the second layer.</p></li>
<li class="fragment"><p>Output activation function encodes the <strong>softmax</strong> function:</p></li>
</ul>
<p><span class="math display">\[
f_m(X) = \Pr(Y = m \mid X) = \frac{e^{Z_m}}{\sum_{\ell=0}^{9} e^{Z_\ell}}.
\]</span></p>
<ul>
<li class="fragment">We fit the model by minimizing the negative multinomial log-likelihood (or cross-entropy):</li>
</ul>
<p><span class="math display">\[
-\sum_{i=1}^{n} \sum_{m=0}^{9} y_{im} \log(f_m(x_i)).
\]</span></p>
<ul>
<li class="fragment"><span class="math inline">\(y_{im}\)</span> is 1 if the true class for observation <span class="math inline">\(i\)</span> is <span class="math inline">\(m\)</span>, else 0 â€” i.e., <em>one-hot encoded</em>.</li>
</ul>
</section>
<section id="results" class="slide level2 center">
<h2>Results</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Method</th>
<th>Test Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neural Network + Ridge Regularization</td>
<td>2.3%</td>
</tr>
<tr class="even">
<td>Neural Network + Dropout Regularization</td>
<td>1.8%</td>
</tr>
<tr class="odd">
<td>Multinomial Logistic Regression</td>
<td>7.2%</td>
</tr>
<tr class="even">
<td>Linear Discriminant Analysis</td>
<td>12.7%</td>
</tr>
</tbody>
</table>
<ul>
<li class="fragment"><p>Early success for neural networks in the 1990s.</p></li>
<li class="fragment"><p>With so many parameters, regularization is essential.</p></li>
<li class="fragment"><p>Some details of regularization and fitting will come later.</p></li>
<li class="fragment"><p>Very overworked problem â€” best reported rates are <span class="math inline">\(&lt; 0.5\%\)</span>!</p></li>
<li class="fragment"><p>Human error rate is reported to be around <span class="math inline">\(0.2\%\)</span>, or 20 of the 10K test images.</p></li>
</ul>
</section>
<section id="convolutional-neural-network-cnn" class="slide level2 center">
<h2>Convolutional Neural Network â€” CNN</h2>
<ul>
<li class="fragment"><p><strong>Major success story for classifying images.</strong></p></li>
<li class="fragment"><p>Shown are samples from <strong>CIFAR100</strong> database: <span class="math inline">\(32 \times 32\)</span> color natural images, with 100 classes.</p></li>
<li class="fragment"><p><span class="math inline">\(50K\)</span> training images, <span class="math inline">\(10K\)</span> test images.</p></li>
<li class="fragment"><p>Each image is a three-dimensional array or <em>feature map</em>:<br>
<span class="math inline">\(32 \times 32 \times 3\)</span> array of 8-bit numbers.<br>
The last dimension represents the three color channels for red, green, and blue.</p></li>
</ul>
</section>
<section id="how-cnns-work" class="slide level2 center">
<h2>How CNNs Work</h2>

<img data-src="figs/10_1_2-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>The CNN builds up an image in a hierarchical fashion.</p></li>
<li class="fragment"><p>Edges and shapes are recognized and pieced together to form more complex shapes, eventually assembling the target image.</p></li>
<li class="fragment"><p>This hierarchical construction is achieved using <strong>convolution</strong> and <em>pooling</em> layers.</p></li>
</ul>
</section>
<section id="convolution-filter" class="slide level2 center">
<h2>Convolution Filter</h2>
<p><span class="math display">\[
\text{Input Image} =
\begin{bmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
g &amp; h &amp; i \\
j &amp; k &amp; l
\end{bmatrix}
\quad \text{Convolution Filter} =
\begin{bmatrix}
\alpha &amp; \beta \\
\gamma &amp; \delta
\end{bmatrix}.
\]</span></p>
<p><span class="math display">\[
\text{Convolved Image} =
\begin{bmatrix}
a\alpha + b\beta + d\gamma + e\delta &amp; b\alpha + c\beta + e\gamma + f\delta \\
d\alpha + e\beta + g\gamma + h\delta &amp; e\alpha + f\beta + h\gamma + i\delta \\
g\alpha + h\beta + j\gamma + k\delta &amp; h\alpha + i\beta + k\gamma + l\delta
\end{bmatrix}.
\]</span></p>
<ul>
<li class="fragment"><p>The filter is itself an image and represents a small shape, edge, etc.</p></li>
<li class="fragment"><p>We slide it around the input image, scoring for matches.</p></li>
<li class="fragment"><p>The scoring is done via <strong>dot-products</strong>, illustrated above.</p></li>
<li class="fragment"><p>If the subimage of the input image is similar to the filter, the score is high; otherwise, it is low.</p></li>
<li class="fragment"><p>The filters are <strong>learned</strong> during training.</p></li>
</ul>
</section>
<section id="convolution-example" class="slide level2 center">
<h2>Convolution Example</h2>

<img data-src="figs/10_7-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>The idea of convolution with a filter is to find common patterns that occur in different parts of the image.</p></li>
<li class="fragment"><p>The two filters shown here highlight vertical and horizontal stripes.</p></li>
<li class="fragment"><p>The result of the convolution is a new feature map.</p></li>
<li class="fragment"><p>Since images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.</p></li>
<li class="fragment"><p>The weights in the filters are <strong>learned</strong> by the network.</p></li>
</ul>
</section>
<section id="pooling" class="slide level2 center">
<h2>Pooling</h2>
<p><span class="math display">\[
\text{Max pool}
\begin{bmatrix}
1 &amp; 2 &amp; 5 &amp; 3 \\
3 &amp; 0 &amp; 1 &amp; 2 \\
2 &amp; 1 &amp; 3 &amp; 4 \\
1 &amp; 1 &amp; 2 &amp; 0
\end{bmatrix}
\rightarrow
\begin{bmatrix}
3 &amp; 5 \\
2 &amp; 4
\end{bmatrix}
\]</span></p>
<ul>
<li class="fragment"><p>Each non-overlapping <span class="math inline">\(2 \times 2\)</span> block is replaced by its maximum.</p></li>
<li class="fragment"><p>This sharpens the feature identification.</p></li>
<li class="fragment"><p>Allows for locational invariance.</p></li>
<li class="fragment"><p>Reduces the dimension by a factor of 4 â€” i.e., factor of 2 in each dimension.</p></li>
</ul>
</section>
<section id="architecture-of-a-cnn" class="slide level2 center">
<h2>Architecture of a CNN</h2>

<img data-src="figs/10_8-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>Many convolve + pool layers.</p></li>
<li class="fragment"><p>Filters are typically small, e.g., each channel <span class="math inline">\(3 \times 3\)</span>.</p></li>
<li class="fragment"><p>Each filter creates a new channel in the convolution layer.</p></li>
<li class="fragment"><p>As pooling reduces size, the number of filters/channels is typically increased.</p></li>
<li class="fragment"><p>Number of layers can be very large.<br>
E.g., <strong>resnet50</strong> trained on <strong>imagenet</strong> 1000-class image database has 50 layers!</p></li>
</ul>
</section>
<section id="using-pretrained-networks-to-classify-images" class="slide level2 center">
<h2>Using Pretrained Networks to Classify Images</h2>

<img data-src="figs/10_1_3-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p>Here we use the 50-layer <strong>resnet50</strong> network trained on the 1000-class <strong>imagenet</strong> corpus to classify some photographs.</p>
</section>
<section id="document-classification-imdb-movie-reviews" class="slide level2 center">
<h2>Document Classification: IMDB Movie Reviews</h2>
<p>The <strong>IMDB</strong> corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for <strong>sentiment</strong> as <strong>positive</strong> or <strong>negative</strong>. Here is the beginning of a negative review:</p>
<blockquote>
<p><em>This has to be one of the worst films of the 1990s. When my friends &amp; I were watching this film (being the target audience it was aimed at) we just sat &amp; watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn â€¦</em></p>
</blockquote>
<p>We have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.</p>
<p>We wish to build a classifier to predict the sentiment of a review.</p>
</section>
<section id="featurization-bag-of-words" class="slide level2 center">
<h2>Featurization: Bag-of-Words</h2>
<p>Documents have different lengths and consist of sequences of words. How do we create features <span class="math inline">\(X\)</span> to characterize a document?</p>
<ul>
<li class="fragment"><p>From a dictionary, identify the 10K most frequently occurring words.</p></li>
<li class="fragment"><p>Create a binary vector of length <span class="math inline">\(p = 10K\)</span> for each document, and score a 1 in every position that the corresponding word occurred.</p></li>
<li class="fragment"><p>With <span class="math inline">\(n\)</span> documents, we now have an <span class="math inline">\(n \times p\)</span> <strong>sparse</strong> feature matrix <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li class="fragment"><p>We compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)</p></li>
<li class="fragment"><p>Bag-of-words are <strong>unigrams</strong>. We can instead use <strong>bigrams</strong> (occurrences of adjacent word pairs) and, in general, <strong>m-grams</strong>.</p></li>
</ul>
</section>
<section id="lasso-versus-neural-network-imdb-reviews" class="slide level2 center">
<h2>Lasso versus Neural Network â€” IMDB Reviews</h2>

<img data-src="figs/10_11-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>Simpler lasso logistic regression model works as well as neural network in this case.</p></li>
<li class="fragment"><p><strong>glmnet</strong> was used to fit the lasso model, and is very effective because it can exploit sparsity in the <span class="math inline">\(\mathbf{X}\)</span> matrix.</p></li>
</ul>
</section>
<section id="recurrent-neural-networks" class="slide level2 center">
<h2>Recurrent Neural Networks</h2>
<p>Often data arise as sequences:</p>
<ul>
<li class="fragment"><p><strong>Documents</strong> are sequences of words, and their relative positions have meaning.</p></li>
<li class="fragment"><p><strong>Time-series</strong> such as weather data or financial indices.</p></li>
<li class="fragment"><p><strong>Recorded speech or music.</strong></p></li>
<li class="fragment"><p><strong>Handwriting,</strong> such as doctorâ€™s notes.</p></li>
</ul>
<p>RNNs build models that take into account this sequential nature of the data and build a memory of the past.</p>
<ul>
<li class="fragment"><p>The feature for each observation is a <strong>sequence</strong> of vectors <span class="math inline">\(X = \{X_1, X_2, \ldots, X_L\}\)</span>.</p></li>
<li class="fragment"><p>The target <span class="math inline">\(Y\)</span> is often of the usual kind â€” e.g., a single variable such as <strong>Sentiment</strong>, or a one-hot vector for multiclass.</p></li>
<li class="fragment"><p>However, <span class="math inline">\(Y\)</span> can also be a sequence, such as the same document in a different language.</p></li>
</ul>
</section>
<section id="simple-recurrent-neural-network-architecture" class="slide level2 center">
<h2>Simple Recurrent Neural Network Architecture</h2>

<img data-src="figs/10_12-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>The hidden layer is a sequence of vectors <span class="math inline">\(A_\ell\)</span>, receiving as input <span class="math inline">\(X_\ell\)</span> as well as <span class="math inline">\(A_{\ell-1}\)</span>. <span class="math inline">\(A_\ell\)</span> produces an output <span class="math inline">\(O_\ell\)</span>.</p></li>
<li class="fragment"><p>The <strong>same</strong> weights <span class="math inline">\(\mathbf{W}\)</span>, <span class="math inline">\(\mathbf{U}\)</span>, and <span class="math inline">\(\mathbf{B}\)</span> are used at each step in the sequence â€” hence the term <strong>recurrent</strong>.</p></li>
<li class="fragment"><p>The <span class="math inline">\(A_\ell\)</span> sequence represents an evolving model for the response that is updated as each element <span class="math inline">\(X_\ell\)</span> is processed.</p></li>
</ul>
</section>
<section id="rnn-in-detail" class="slide level2 center">
<h2>RNN in Detail</h2>
<p>Suppose <span class="math inline">\(X_\ell = (X_{\ell1}, X_{\ell2}, \ldots, X_{\ell p})\)</span> has <span class="math inline">\(p\)</span> components, and <span class="math inline">\(A_\ell = (A_{\ell1}, A_{\ell2}, \ldots, A_{\ell K})\)</span> has <span class="math inline">\(K\)</span> components. Then the computation at the <span class="math inline">\(k\)</span>-th components of hidden unit <span class="math inline">\(A_\ell\)</span> is:</p>
<p><span class="math display">\[
A_{\ell k} = g\left(w_{k0} + \sum_{j=1}^{p} w_{kj} X_{\ell j} + \sum_{s=1}^{K} u_{ks} A_{\ell-1,s}\right)
\]</span></p>
<p><span class="math display">\[
O_\ell = \beta_0 + \sum_{k=1}^{K} \beta_k A_{\ell k}
\]</span></p>
<p>Often we are concerned only with the prediction <span class="math inline">\(O_L\)</span> at the last unit. For squared error loss, and <span class="math inline">\(n\)</span> sequence/response pairs, we would minimize:</p>
<p><span class="math display">\[
\sum_{i=1}^{n} (y_i - o_{iL})^2 = \sum_{i=1}^{n} \left(y_i - \left(\beta_0 + \sum_{k=1}^{K} \beta_k g\left(w_{k0} + \sum_{j=1}^{p} w_{kj} x_{iL,j} + \sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\right)\right)\right)^2
\]</span></p>
</section>
<section id="rnn-and-imdb-reviews" class="slide level2 center">
<h2>RNN and IMDB Reviews</h2>
<ul>
<li class="fragment"><p>The document feature is a sequence of words <span class="math inline">\(\{\mathcal{W}_\ell\}_{1}^{L}\)</span>. We typically truncate/pad the documents to the same number <span class="math inline">\(L\)</span> of words (we use <span class="math inline">\(L = 500\)</span>).</p></li>
<li class="fragment"><p>Each word <span class="math inline">\(\mathcal{W}_\ell\)</span> is represented as a <strong>one-hot encoded</strong> binary vector <span class="math inline">\(X_\ell\)</span> (dummy variable) of length <span class="math inline">\(10K\)</span>, with all zeros and a single one in the position for that word in the dictionary.</p></li>
<li class="fragment"><p>This results in an extremely sparse feature representation and would not work well.</p></li>
<li class="fragment"><p>Instead, we use a lower-dimensional pretrained <strong>word embedding</strong> matrix <span class="math inline">\(\mathbf{E}\)</span> (<span class="math inline">\(m \times 10K\)</span>, next slide).</p></li>
<li class="fragment"><p>This reduces the binary feature vector of length <span class="math inline">\(10K\)</span> to a real feature vector of dimension <span class="math inline">\(m \ll 10K\)</span> (e.g., <span class="math inline">\(m\)</span> in the low hundreds).</p></li>
</ul>
</section>
<section id="word-embedding" class="slide level2 center">
<h2>Word Embedding</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/10_13a-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/10_13b-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<p>Embeddings are pretrained on very large corpora of documents, using methods similar to principal components. <strong>word2vec</strong> and <strong>GloVe</strong> are popular.</p>
</section>
<section id="rnn-on-imdb-reviews" class="slide level2 center">
<h2>RNN on IMDB Reviews</h2>
<ul>
<li class="fragment"><p>After a lot of work, the results are a disappointing 76% accuracy.</p></li>
<li class="fragment"><p>We then fit a more exotic RNN than the one displayed â€” a <strong>LSTM</strong> with <em>long and short term memory</em>. Here <span class="math inline">\(A_\ell\)</span> receives input from <span class="math inline">\(A_{\ell-1}\)</span> (short term memory) as well as from a version that reaches further back in time (long term memory). Now we get 87% accuracy, slightly less than the 88% achieved by <strong>glmnet</strong>.</p></li>
<li class="fragment"><p>These data have been used as a benchmark for new RNN architectures. The best reported result found at the time of writing (2020) was around 95%. We point to a <strong>leaderboard</strong> in Section 10.5.1.</p></li>
</ul>
</section>
<section id="time-series-forecasting" class="slide level2 center">
<h2>Time Series Forecasting</h2>

<img data-src="figs/10_14-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="new-york-stock-exchange-data" class="slide level2 center">
<h2>New-York Stock Exchange Data</h2>
<p>Shown in the previous slide are three daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):</p>
<ul>
<li class="fragment"><p><strong>Log trading volume.</strong> This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.</p></li>
<li class="fragment"><p><strong>Dow Jones return.</strong> This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.</p></li>
<li class="fragment"><p><strong>Log volatility.</strong> This is based on the absolute values of daily price movements.</p></li>
</ul>
<p>Goal: predict <strong>Log trading volume</strong> tomorrow, given its observed values up to today, as well as those of <strong>Dow Jones return</strong> and <strong>Log volatility</strong>.</p>
<p><em>These data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213â€“220.</em></p>
</section>
<section id="autocorrelation" class="slide level2 center">
<h2>Autocorrelation</h2>

<img data-src="figs/10_15-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>The <strong>autocorrelation</strong> at lag <span class="math inline">\(\ell\)</span> is the correlation of all pairs <span class="math inline">\((v_t, v_{t-\ell})\)</span> that are <span class="math inline">\(\ell\)</span> trading days apart.</p></li>
<li class="fragment"><p>These sizable correlations give us confidence that past values will be helpful in predicting the future.</p></li>
<li class="fragment"><p>This is a curious prediction problem: the response <span class="math inline">\(v_t\)</span> is also a feature <span class="math inline">\(v_{t-\ell}\)</span>!</p></li>
</ul>
</section>
<section id="rnn-forecaster" class="slide level2 center">
<h2>RNN Forecaster</h2>
<p>We only have one series of data! How do we set up for an RNN?</p>
<p>We extract many short mini-series of input sequences<br>
<span class="math inline">\(\mathbf{X} = \{ X_1, X_2, \ldots, X_L \}\)</span> with a predefined length <span class="math inline">\(L\)</span> known as the <strong>lag</strong>:</p>
<p><span class="math display">\[
X_1 = \begin{pmatrix}
v_{t-L} \\
r_{t-L} \\
z_{t-L}
\end{pmatrix}, \quad
X_2 = \begin{pmatrix}
v_{t-L+1} \\
r_{t-L+1} \\
z_{t-L+1}
\end{pmatrix}, \quad
\cdots, \quad
X_L = \begin{pmatrix}
v_{t-1} \\
r_{t-1} \\
z_{t-1}
\end{pmatrix}, \quad \text{and} \quad Y = v_t.
\]</span></p>
<p>Since <span class="math inline">\(T = 6,051\)</span>, with <span class="math inline">\(L = 5\)</span>, we can create 6,046 such <span class="math inline">\((X, Y)\)</span> pairs.<br>
We use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per <span class="math inline">\(A_\ell\)</span>).</p>
</section>
<section id="rnn-results-for-nyse-data" class="slide level2 center">
<h2>RNN Results for NYSE Data</h2>

<img data-src="figs/10_16-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p>Figure shows predictions and truth for the test period.</p>
<p><span class="math display">\[
R^2 = 0.42 \text{ for RNN}
\]</span> <span class="math display">\[
R^2 = 0.18 \text{ for straw man â€” use yesterdayâ€™s value of Log trading volume to predict that of today.}
\]</span></p>
</section>
<section id="autoregression-forecaster" class="slide level2 center">
<h2>Autoregression Forecaster</h2>
<p>The RNN forecaster is similar in structure to a traditional <strong>autoregression</strong> procedure.</p>
<p><span class="math display">\[
\mathbf{y} =
\begin{bmatrix}
v_{L+1} \\
v_{L+2} \\
v_{L+3} \\
\vdots \\
v_T
\end{bmatrix}, \quad
\mathbf{M} =
\begin{bmatrix}
1 &amp; v_L &amp; v_{L-1} &amp; \cdots &amp; v_1 \\
1 &amp; v_{L+1} &amp; v_L &amp; \cdots &amp; v_2 \\
1 &amp; v_{L+2} &amp; v_{L+1} &amp; \cdots &amp; v_3 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; v_{T-1} &amp; v_{T-2} &amp; \cdots &amp; v_{T-L}
\end{bmatrix}.
\]</span></p>
<p>Fit an OLS regression of <span class="math inline">\(\mathbf{y}\)</span> on <span class="math inline">\(\mathbf{M}\)</span>, giving:</p>
<p><span class="math display">\[
\hat{v}_t = \hat{\beta}_0 + \hat{\beta}_1 v_{t-1} + \hat{\beta}_2 v_{t-2} + \cdots + \hat{\beta}_L v_{t-L}.
\]</span></p>
<p>Known as an <strong>order-<span class="math inline">\(L\)</span> autoregression</strong> model or <span class="math inline">\(AR(L)\)</span>.<br>
For the <strong>NYSE</strong> data, we can include lagged versions of <strong>DJ_return</strong> and <strong>log_volatility</strong> in matrix <span class="math inline">\(\mathbf{M}\)</span>, resulting in <span class="math inline">\(3L + 1\)</span> columns.</p>
</section>
<section id="autoregression-results-for-nyse-data" class="slide level2 center">
<h2>Autoregression Results for NYSE Data</h2>
<p><span class="math display">\[
R^2 = 0.41 \text{ for } AR(5) \text{ model (16 parameters)}
\]</span></p>
<p><span class="math display">\[
R^2 = 0.42 \text{ for RNN model (205 parameters)}
\]</span></p>
<p><span class="math display">\[
R^2 = 0.42 \text{ for } AR(5) \text{ model fit by neural network.}
\]</span></p>
<p><span class="math display">\[
R^2 = 0.46 \text{ for all models if we include } \textbf{day\_of\_week} \text{ of day being predicted.}
\]</span></p>
</section>
<section id="summary-of-rnns" class="slide level2 center">
<h2>Summary of RNNs</h2>
<ul>
<li class="fragment"><p>We have presented the simplest of RNNs. Many more complex variations exist.</p></li>
<li class="fragment"><p>One variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.</p></li>
<li class="fragment"><p>Can have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.</p></li>
<li class="fragment"><p>Can have output also be a sequence, and input and output share the hidden units. So called <strong>seq2seq</strong> learning are used for language translation.</p></li>
</ul>
</section>
<section id="when-to-use-deep-learning" class="slide level2 center">
<h2>When to Use Deep Learning</h2>
<ul>
<li class="fragment"><p><strong>CNNs</strong> have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.</p></li>
<li class="fragment"><p><strong>RNNs</strong> have had big wins in speech modeling, language translation, and forecasting.</p></li>
</ul>
<p>Should we always use deep learning models?</p>
<ul>
<li class="fragment"><p>Often the big successes occur when the <strong>signal to noise ratio</strong> is high â€” e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.</p></li>
<li class="fragment"><p>For noisier data, simpler models can often work better:</p>
<ul>
<li class="fragment"><p>On the <strong>NYSE</strong> data, the <strong>AR(5)</strong> model is much simpler than an RNN, and performed as well.</p></li>
<li class="fragment"><p>On the <strong>IMDB</strong> review data, the linear model fit by <strong>glmnet</strong> did as well as the neural network, and better than the RNN.</p></li>
</ul></li>
<li class="fragment"><p>We endorse the <strong>Occamâ€™s razor</strong> principle â€” we prefer simpler models if they work as well. More interpretable!</p></li>
</ul>
</section>
<section id="fitting-neural-networks" class="slide level2 center">
<h2>Fitting Neural Networks</h2>

<img data-src="figs/10_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p><span class="math display">\[
\min_{\{w_k\}_{1}^K, \beta} \frac{1}{2} \sum_{i=1}^n \left(y_i - f(x_i)\right)^2, \quad \text{where}
\]</span></p>
<p><span class="math display">\[
f(x_i) = \beta_0 + \sum_{k=1}^K \beta_k g\left(w_{k0} + \sum_{j=1}^p w_{kj} x_{ij}\right).
\]</span></p>
<p>This problem is difficult because the objective is <strong>non-convex</strong>.</p>
<p>Despite this, effective algorithms have evolved that can optimize complex neural network problems efficiently.</p>
</section>
<section id="non-convex-functions-and-gradient-descent" class="slide level2 center">
<h2>Non Convex Functions and Gradient Descent</h2>
<p>Let <span class="math inline">\(R(\theta) = \frac{1}{2} \sum_{i=1}^n (y_i - f_\theta(x_i))^2\)</span> with <span class="math inline">\(\theta = (\{w_k\}_{1}^K, \beta)\)</span>.</p>

<img data-src="figs/10_17-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ol type="1">
<li class="fragment"><p>Start with a guess <span class="math inline">\(\theta^0\)</span> for all the parameters in <span class="math inline">\(\theta\)</span>, and set <span class="math inline">\(t = 0\)</span>.</p></li>
<li class="fragment"><p>Iterate until the objective <span class="math inline">\(R(\theta)\)</span> fails to decrease:</p>
<ol type="a">
<li class="fragment"><p>Find a vector <span class="math inline">\(\delta\)</span> that reflects a small change in <span class="math inline">\(\theta\)</span>, such that <span class="math inline">\(\theta^{t+1} = \theta^t + \delta\)</span> <strong>reduces</strong> the objective; i.e., <span class="math inline">\(R(\theta^{t+1}) &lt; R(\theta^t)\)</span>.</p></li>
<li class="fragment"><p>Set <span class="math inline">\(t \gets t + 1\)</span>.</p></li>
</ol></li>
</ol>
</section>
<section id="gradient-descent-continued" class="slide level2 center">
<h2>Gradient Descent Continued</h2>
<ul>
<li class="fragment"><p>In this simple example, we reached the <strong>global minimum</strong>.</p></li>
<li class="fragment"><p>If we had started a little to the left of <span class="math inline">\(\theta^0\)</span>, we would have gone in the other direction and ended up in a <strong>local minimum</strong>.</p></li>
<li class="fragment"><p>Although <span class="math inline">\(\theta\)</span> is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.</p></li>
</ul>
<p>How to find a direction <span class="math inline">\(\delta\)</span> that points downhill? We compute the <strong>gradient vector</strong>:</p>
<p><span class="math display">\[
\nabla R(\theta^t) = \frac{\partial R(\theta)}{\partial \theta} \bigg|_{\theta = \theta^t}
\]</span></p>
<p>i.e., the vector of <strong>partial derivatives</strong> at the current guess <span class="math inline">\(\theta^t\)</span>.<br>
The gradient points uphill, so our update is <span class="math inline">\(\delta = - \rho \nabla R(\theta^t)\)</span> or</p>
<p><span class="math display">\[
\theta^{t+1} \gets \theta^t - \rho \nabla R(\theta^t),
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the <strong>learning rate</strong> (typically small, e.g., <span class="math inline">\(\rho = 0.001\)</span>).</p>
</section>
<section id="gradients-and-backpropagation" class="slide level2 center">
<h2>Gradients and Backpropagation</h2>
<p><span class="math display">\[
R(\theta) = \sum_{i=1}^n R_i(\theta) \text{ is a sum, so gradient is sum of gradients.}
\]</span></p>
<p><span class="math display">\[
R_i(\theta) = \frac{1}{2}(y_i - f_\theta(x_i))^2 = \frac{1}{2} \left( y_i - \beta_0 - \sum_{k=1}^K \beta_k g\left( w_{k0} + \sum_{j=1}^p w_{kj} x_{ij} \right) \right)^2
\]</span></p>
<p>For ease of notation, let</p>
<p><span class="math display">\[
z_{ik} = w_{k0} + \sum_{j=1}^p w_{kj} x_{ij}.
\]</span></p>
<p>Backpropagation uses the <strong>chain rule for differentiation</strong>:</p>
<p><span class="math display">\[
\frac{\partial R_i(\theta)}{\partial \beta_k} = \frac{\partial R_i(\theta)}{\partial f_\theta(x_i)} \cdot \frac{\partial f_\theta(x_i)}{\partial \beta_k}
= -(y_i - f_\theta(x_i)) \cdot g(z_{ik}).
\]</span></p>
<p><span class="math display">\[
\frac{\partial R_i(\theta)}{\partial w_{kj}} = \frac{\partial R_i(\theta)}{\partial f_\theta(x_i)} \cdot \frac{\partial f_\theta(x_i)}{\partial g(z_{ik})} \cdot \frac{\partial g(z_{ik})}{\partial z_{ik}} \cdot \frac{\partial z_{ik}}{\partial w_{kj}}
= -(y_i - f_\theta(x_i)) \cdot \beta_k \cdot g'(z_{ik}) \cdot x_{ij}.
\]</span></p>
</section>
<section id="tricks-of-the-trade" class="slide level2 center">
<h2>Tricks of the Trade</h2>
<ul>
<li class="fragment"><strong>Slow learning.</strong> Gradient descent is slow, and a small learning rate <span class="math inline">\(\rho\)</span> slows it even further. With <em>early stopping</em>, this is a form of regularization.</li>
<li class="fragment"><strong>Stochastic gradient descent.</strong> Rather than compute the gradient using <em>all</em> the data, use a small <em>minibatch</em> drawn at random at each step. E.g. for <em>MNIST</em> data, with <span class="math inline">\(n = 60K\)</span>, we use minibatches of 128 observations.</li>
<li class="fragment"><strong>An epoch</strong> is a count of iterations and amounts to the number of minibatch updates such that <span class="math inline">\(n\)</span> samples in total have been processed; i.e.&nbsp;<span class="math inline">\(60K/128 \approx 469\)</span> for <em>MNIST</em>.</li>
<li class="fragment"><strong>Regularization.</strong> Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are <em>dropout</em> and <em>augmentation</em>, discussed next.</li>
</ul>
</section>
<section id="dropout-learning" class="slide level2 center">
<h2>Dropout Learning</h2>

<img data-src="figs/10_1_4-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p><strong>At each SGD update</strong>, randomly remove units with probability <span class="math inline">\(\phi\)</span>, and scale up the weights of those retained by <span class="math inline">\(1/(1-\phi)\)</span> to compensate.</p></li>
<li class="fragment"><p>In simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.</p></li>
<li class="fragment"><p>As in ridge, the other units <em>stand in</em> for those temporarily removed, and their weights are drawn closer together.</p></li>
<li class="fragment"><p>Similar to randomly omitting variables when growing trees in random forests (<em>Chapter 8</em>).</p></li>
</ul>
</section>
<section id="ridge-and-data-augmentation" class="slide level2 center">
<h2>Ridge and Data Augmentation</h2>

<img data-src="figs/10_1_5-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p><strong>Make many copies of each</strong> <span class="math inline">\((x_i, y_i)\)</span> and add a small amount of Gaussian noise to the <span class="math inline">\(x_i\)</span> â€” a little cloud around each observation â€” but <em>leave the copies of <span class="math inline">\(y_i\)</span> alone</em>!</p></li>
<li class="fragment"><p>This makes the fit robust to small perturbations in <span class="math inline">\(x_i\)</span>, and is equivalent to ridge regularization in an OLS setting.</p></li>
</ul>
</section>
<section id="data-augmentation-on-the-fly" class="slide level2 center">
<h2>Data Augmentation on the Fly</h2>

<img data-src="figs/10_1_6-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p><strong>Data augmentation</strong> is especially effective with <strong>SGD</strong>, here demonstrated for a CNN and image classification.</p></li>
<li class="fragment"><p>Natural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.</p></li>
<li class="fragment"><p>The label is left unchanged â€” in each case still <strong>tiger</strong>.</p></li>
<li class="fragment"><p>Improves performance of CNN and is similar to ridge.</p></li>
</ul>
</section>
<section id="double-descent" class="slide level2 center">
<h2>Double Descent</h2>
<ul>
<li class="fragment"><p><strong>With neural networks</strong>, it seems better to have too many hidden units than too few.</p></li>
<li class="fragment"><p>Likewise more hidden layers better than few.</p></li>
<li class="fragment"><p>Running stochastic gradient descent till zero training error often gives good out-of-sample error.</p></li>
<li class="fragment"><p>Increasing the number of units or layers and again training till zero error sometimes gives <strong>even better</strong> out-of-sample error.</p></li>
</ul>
<p>What happened to overfitting and the usual bias-variance trade-off?</p>
<p><em>Belkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off.</em></p>
</section>
<section id="the-double-descent-error-curve" class="slide level2 center">
<h2>The Double-Descent Error Curve</h2>

<img data-src="figs/10_20-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>When <span class="math inline">\(d \leq 20\)</span>, model is OLS, and we see usual bias-variance trade-off.</p></li>
<li class="fragment"><p>When <span class="math inline">\(d &gt; 20\)</span>, we revert to minimum-norm. As <span class="math inline">\(d\)</span> increases above 20, <span class="math inline">\(\sum_{j=1}^d \hat{\beta}_j^2\)</span> <strong>decreases</strong> since it is easier to achieve zero error, and hence less wiggly solutions.</p></li>
</ul>
</section>
<section id="less-wiggly-solutions" class="slide level2 center">
<h2>Less Wiggly Solutions</h2>

<img data-src="figs/10_21-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p>To achieve a zero-residual solution with <span class="math inline">\(d = 20\)</span> is a real stretch!</p></li>
<li class="fragment"><p>Easier for larger <span class="math inline">\(d\)</span>.</p></li>
</ul>
</section>
<section id="some-facts" class="slide level2 center">
<h2>Some Facts</h2>
<ul>
<li class="fragment"><p>In a wide linear model (<span class="math inline">\(p \gg n\)</span>) fit by least squares, SGD with a small step size leads to a <em>minimum norm</em> zero-residual solution.</p></li>
<li class="fragment"><p>Stochastic gradient <em>flow</em> â€” i.e.&nbsp;the entire path of SGD solutions â€” is somewhat similar to ridge path.</p></li>
<li class="fragment"><p>By analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.</p></li>
<li class="fragment"><p>In particular cases with <em>high signal-to-noise ratio</em> â€” e.g.&nbsp;image recognition â€” are less prone to overfitting; the zero-error solution is mostly signal!</p></li>
</ul>
</section>
<section id="software" class="slide level2 center">
<h2>Software</h2>
<ul>
<li class="fragment"><p>Wonderful software available for neural networks and deep learning. <strong>Tensorflow</strong> from Google and <strong>PyTorch</strong> from Facebook. Both are <strong>Python</strong> packages.</p></li>
<li class="fragment"><p>In the Chapter 10 lab we demonstrate <strong>tensorflow</strong> and <strong>keras</strong> packages in <strong>R</strong>, which interface to Python. See textbook and online resources for <strong>Rmarkdown</strong> and <strong>Jupyter</strong> notebooks for these and all labs for the second edition of ISLR book.</p></li>
<li class="fragment"><p>The <strong>torch</strong> package in <strong>R</strong> is available as well, and implements the <strong>PyTorch</strong> dialect. The Chapter 10 lab will be available in this dialect as well; watch the resources page at <a href="http://www.statlearning.com">www.statlearning.com</a>.</p></li>
</ul>
</section>
<section id="summary" class="slide level2 center">
<h2>Summary</h2>
<div>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 80%;">
<ul>
<li>XXXX</li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 80%;">
<ul>
<li>XXXX</li>
</ul>
</div>
</div></div>
</div>
</section></section>
<section id="thank-you" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Thank you!</h1>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">
<p>Predictive Analytics</p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>