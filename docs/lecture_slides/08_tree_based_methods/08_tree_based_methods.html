<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Professor: Davi Moreira">
  <title>MGMT 47400: Predictive Analytics â€“  MGMT 47400: Predictive Analytics </title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><span style="font-size: 100%;"> MGMT 47400: Predictive Analytics </span></h1>
  <p class="subtitle"><span style="font-size: 150%;"> Tree Based Methods </span></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Professor: Davi Moreira 
</div>
</div>
</div>

</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>
<div>
<div class="columns">
<div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>XXXX</li>
</ul>
</div><div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>XXX</li>
</ul>
</div></div>
</div>
</section>
<section>
<section id="xxx" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>XXX</h1>

</section>
<section id="tree-based-methods" class="slide level2 center">
<h2>Tree-based Methods</h2>
<ul>
<li class="fragment"><p>Here we describe <em>tree-based</em> methods for regression and classification.</p></li>
<li class="fragment"><p>These involve <em>stratifying</em> or <em>segmenting</em> the predictor space into a number of simple regions.</p></li>
<li class="fragment"><p>Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as <em>decision-tree</em> methods.</p></li>
</ul>
</section>
<section id="pros-and-cons" class="slide level2 center">
<h2>Pros and Cons</h2>
<ul>
<li class="fragment"><p>Tree-based methods are simple and useful for interpretation.</p></li>
<li class="fragment"><p>However, they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.</p></li>
<li class="fragment"><p>Hence we also discuss <em>bagging</em>, <em>random forests</em>, and <em>boosting</em>. These methods grow multiple trees which are then combined to yield a single consensus prediction.</p></li>
<li class="fragment"><p>Combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.</p></li>
</ul>
</section>
<section id="the-basics-of-decision-trees" class="slide level2 center">
<h2>The Basics of Decision Trees</h2>
<ul>
<li class="fragment"><p>Decision trees can be applied to both regression and classification problems.</p></li>
<li class="fragment"><p>We first consider regression problems, and then move on to classification.</p></li>
</ul>
</section>
<section id="baseball-salary-data-how-would-you-stratify-it" class="slide level2 center">
<h2>Baseball salary data: how would you stratify it?</h2>
<p>Salary is color-coded from low (<em>blue, green</em>) to high (<em>yellow, red</em>).</p>

<img data-src="figs/8_1_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="decision-tree-for-these-data" class="slide level2 center">
<h2>Decision tree for these data</h2>

<img data-src="figs/8_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-previous-figure" class="slide level2 center">
<h2>Details of previous figure</h2>
<ul>
<li class="fragment"><p>For the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year.</p></li>
<li class="fragment"><p>At a given internal node, the label (of the form <span class="math inline">\(X_j &lt; t_k\)</span>) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to <span class="math inline">\(X_j \geq t_k\)</span>. For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to <span class="math inline">\(\text{Years} &lt; 4.5\)</span>, and the right-hand branch corresponds to <span class="math inline">\(\text{Years} \geq 4.5\)</span>.</p></li>
<li class="fragment"><p>The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there.</p></li>
</ul>
</section>
<section id="results" class="slide level2 center">
<h2>Results</h2>
<ul>
<li class="fragment">Overall, the tree stratifies or segments the players into three regions of predictor space: <span class="math inline">\(R_1 = \{X \ | \ \text{Years} &lt; 4.5\}\)</span>, <span class="math inline">\(R_2 = \{X \ | \ \text{Years} \geq 4.5, \text{Hits} &lt; 117.5\}\)</span>, and <span class="math inline">\(R_3 = \{X \ | \ \text{Years} \geq 4.5, \text{Hits} \geq 117.5\}\)</span>.</li>
</ul>

<img data-src="figs/8_2-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="terminology-for-trees" class="slide level2 center">
<h2>Terminology for Trees</h2>
<ul>
<li class="fragment"><p>In keeping with the <em>tree</em> analogy, the regions <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span>, and <span class="math inline">\(R_3\)</span> are known as <em>terminal nodes</em>.</p></li>
<li class="fragment"><p>Decision trees are typically drawn <em>upside down</em>, in the sense that the leaves are at the bottom of the tree.</p></li>
<li class="fragment"><p>The points along the tree where the predictor space is split are referred to as <em>internal nodes</em>.</p></li>
<li class="fragment"><p>In the Hitters tree, the two internal nodes are indicated by the text <span class="math inline">\(\text{Years} &lt; 4.5\)</span> and <span class="math inline">\(\text{Hits} &lt; 117.5\)</span>.</p></li>
</ul>
</section>
<section id="interpretation-of-results" class="slide level2 center">
<h2>Interpretation of Results</h2>
<ul>
<li class="fragment"><p><em>Years</em> is the most important factor in determining <em>Salary</em>, and players with less experience earn lower salaries than more experienced players.</p></li>
<li class="fragment"><p>Given that a player is less experienced, the number of <em>Hits</em> that he made in the previous year seems to play little role in his <em>Salary</em>.</p></li>
<li class="fragment"><p>But among players who have been in the major leagues for five or more years, the number of <em>Hits</em> made in the previous year does affect <em>Salary</em>, and players who made more <em>Hits</em> last year tend to have higher salaries.</p></li>
<li class="fragment"><p>Surely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain.</p></li>
</ul>
</section>
<section id="details-of-the-tree-building-process" class="slide level2 center">
<h2>Details of the tree-building process</h2>
<ol type="1">
<li class="fragment"><p>We divide the predictor space â€” that is, the set of possible values for <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> â€” into <span class="math inline">\(J\)</span> distinct and non-overlapping regions, <span class="math inline">\(R_1, R_2, \dots, R_J\)</span>.</p></li>
<li class="fragment"><p>For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>.</p></li>
</ol>
</section>
<section id="more-details-of-the-tree-building-process" class="slide level2 center">
<h2>More details of the tree-building process</h2>
<ul>
<li class="fragment"><p>In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or <em>boxes</em>, for simplicity and for ease of interpretation of the resulting predictive model.</p></li>
<li class="fragment"><p>The goal is to find boxes <span class="math inline">\(R_1, \dots, R_J\)</span> that minimize the RSS, given by</p></li>
</ul>
<p><span class="math display">\[
\sum_{j=1}^{J} \sum_{i \in R_j} \left( y_i - \hat{y}_{R_j} \right)^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the mean response for the training observations within the <span class="math inline">\(j\)</span>-th box.</p>
</section>
<section id="more-details-of-the-tree-building-process-1" class="slide level2 center">
<h2>More details of the tree-building process</h2>
<ul>
<li class="fragment"><p>Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes.</p></li>
<li class="fragment"><p>For this reason, we take a <em>top-down, greedy</em> approach that is known as recursive binary splitting.</p></li>
<li class="fragment"><p>The approach is <em>top-down</em> because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.</p></li>
<li class="fragment"><p>It is <em>greedy</em> because at each step of the tree-building process, the <em>best</em> split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.</p></li>
</ul>
</section>
<section id="details-continued" class="slide level2 center">
<h2>Detailsâ€” Continued</h2>
<ul>
<li class="fragment"><p>We first select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\(\{X | X_j &lt; s\}\)</span> and <span class="math inline">\(\{X | X_j \geq s\}\)</span> leads to the greatest possible reduction in RSS.</p></li>
<li class="fragment"><p>Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.</p></li>
<li class="fragment"><p>However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.</p></li>
<li class="fragment"><p>Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.</p></li>
</ul>
</section>
<section id="predictions" class="slide level2 center">
<h2>Predictions</h2>
<ul>
<li class="fragment"><p>We predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.</p></li>
<li class="fragment"><p>A five-region example of this approach is shown in the next slide.</p></li>
</ul>
</section>
<section id="predictions-1" class="slide level2 center">
<h2>Predictions</h2>

<img data-src="figs/8_3-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-previous-figure-1" class="slide level2 center">
<h2>Details of previous figure</h2>
<p><em>Top Left:</em> A partition of two-dimensional feature space that could not result from recursive binary splitting.</p>
<p><em>Top Right:</em> The output of recursive binary splitting on a two-dimensional example.</p>
<p><em>Bottom Left:</em> A tree corresponding to the partition in the top right panel.</p>
<p><em>Bottom Right:</em> A perspective plot of the prediction surface corresponding to that tree.</p>
</section>
<section id="pruning-a-tree" class="slide level2 center">
<h2>Pruning a tree</h2>
<ul>
<li class="fragment"><p>The process described above may produce good predictions on the training set, but is likely to <em>overfit</em> the data, leading to poor test set performance. <em>Why?</em></p></li>
<li class="fragment"><p>A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1, \dots, R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias.</p></li>
<li class="fragment"><p>One possible alternative to the process described above is to grow the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.</p></li>
<li class="fragment"><p>This strategy will result in smaller trees, but is too <em>short-sighted</em>: a seemingly worthless split early on in the tree might be followed by a very good split â€” that is, a split that leads to a large reduction in RSS later on.</p></li>
</ul>
</section>
<section id="pruning-a-tree-continued" class="slide level2 center">
<h2>Pruning a treeâ€” continued</h2>
<ul>
<li class="fragment"><p>A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span>, and then <em>prune</em> it back in order to obtain a <em>subtree</em>.</p></li>
<li class="fragment"><p><em>Cost complexity pruning</em> â€” also known as <em>weakest link pruning</em> â€” is used to do this.</p></li>
<li class="fragment"><p>We consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span>. For each value of <span class="math inline">\(\alpha\)</span>, there corresponds a subtree <span class="math inline">\(T \subset T_0\)</span> such that</p></li>
</ul>
<p><span class="math display">\[
\sum_{m=1}^{|T|} \sum_{i : x_i \in R_m} \left( y_i - \hat{y}_{R_m} \right)^2 + \alpha |T|
\]</span></p>
<p>is as small as possible. Here <span class="math inline">\(|T|\)</span> indicates the number of terminal nodes of the tree <span class="math inline">\(T\)</span>, <span class="math inline">\(R_m\)</span> is the rectangle (i.e., the subset of predictor space) corresponding to the <span class="math inline">\(m\)</span>-th terminal node, and <span class="math inline">\(\hat{y}_{R_m}\)</span> is the mean of the training observations in <span class="math inline">\(R_m\)</span>.</p>
</section>
<section id="choosing-the-best-subtree" class="slide level2 center">
<h2>Choosing the best subtree</h2>
<ul>
<li class="fragment"><p>The tuning parameter <span class="math inline">\(\alpha\)</span> controls a trade-off between the subtreeâ€™s complexity and its fit to the training data.</p></li>
<li class="fragment"><p>We select an optimal value <span class="math inline">\(\hat{\alpha}\)</span> using cross-validation.</p></li>
<li class="fragment"><p>We then return to the full data set and obtain the subtree corresponding to <span class="math inline">\(\hat{\alpha}\)</span>.</p></li>
</ul>
</section>
<section id="summary-tree-algorithm" class="slide level2 center">
<h2>Summary: tree algorithm</h2>
<ol type="1">
<li class="fragment"><p>Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.</p></li>
<li class="fragment"><p>Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(\alpha\)</span>.</p></li>
<li class="fragment"><p>Use K-fold cross-validation to choose <span class="math inline">\(\alpha\)</span>. For each <span class="math inline">\(k = 1, \dots, K\)</span>:</p>
<p>3.1 Repeat Steps 1 and 2 on the <span class="math inline">\(\frac{K-1}{K}\)</span>-th fraction of the training data, excluding the <span class="math inline">\(k\)</span>-th fold. 3.2 Evaluate the mean squared prediction error on the data in the left-out <span class="math inline">\(k\)</span>-th fold, as a function of <span class="math inline">\(\alpha\)</span>.</p>
<p>Average the results, and pick <span class="math inline">\(\alpha\)</span> to minimize the average error.</p></li>
<li class="fragment"><p>Return the subtree from Step 2 that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>.</p></li>
</ol>
</section>
<section id="baseball-example-continued" class="slide level2 center">
<h2>Baseball example continued</h2>
<ul>
<li class="fragment"><p>First, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set.</p></li>
<li class="fragment"><p>We then built a large regression tree on the training data and varied <span class="math inline">\(\alpha\)</span> in order to create subtrees with different numbers of terminal nodes.</p></li>
<li class="fragment"><p>Finally, we performed six-fold cross-validation in order to estimate the cross-validated MSE of the trees as a function of <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
</section>
<section id="baseball-example-continued-1" class="slide level2 center">
<h2>Baseball example continued</h2>

<img data-src="figs/8_4-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="baseball-example-continued-2" class="slide level2 center">
<h2>Baseball example continued</h2>

<img data-src="figs/8_5-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="classification-trees" class="slide level2 center">
<h2>Classification Trees</h2>
<ul>
<li class="fragment"><p>Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.</p></li>
<li class="fragment"><p>For a classification tree, we predict that each observation belongs to the <em>most commonly occurring class</em> of training observations in the region to which it belongs.</p></li>
</ul>
</section>
<section id="details-of-classification-trees" class="slide level2 center">
<h2>Details of classification trees</h2>
<ul>
<li class="fragment"><p>Just as in the regression setting, we use recursive binary splitting to grow a classification tree.</p></li>
<li class="fragment"><p>In the classification setting, RSS cannot be used as a criterion for making the binary splits.</p></li>
<li class="fragment"><p>A natural alternative to RSS is the <em>classification error rate</em>. This is simply the fraction of the training observations in that region that do not belong to the most common class:</p></li>
</ul>
<p><span class="math display">\[
E = 1 - \max_k(\hat{p}_{mk}).
\]</span></p>
<p>Here <span class="math inline">\(\hat{p}_{mk}\)</span> represents the proportion of training observations in the <span class="math inline">\(m\)</span>-th region that are from the <span class="math inline">\(k\)</span>-th class.</p>
<ul>
<li class="fragment">However, classification error is not sufficiently sensitive for tree-growing, and in practice, two other measures are preferable.</li>
</ul>
</section>
<section id="gini-index-and-deviance" class="slide level2 center">
<h2>Gini index and Deviance</h2>
<ul>
<li class="fragment">The <em>Gini index</em> is defined by</li>
</ul>
<p><span class="math display">\[
G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk}),
\]</span></p>
<p>a measure of total variance across the <span class="math inline">\(K\)</span> classes. The Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span>â€™s are close to zero or one.</p>
<ul>
<li class="fragment"><p>For this reason, the Gini index is referred to as a measure of node <em>purity</em> â€” a small value indicates that a node contains predominantly observations from a single class.</p></li>
<li class="fragment"><p>An alternative to the Gini index is <em>cross-entropy</em>, given by</p></li>
</ul>
<p><span class="math display">\[
D = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}.
\]</span></p>
<ul>
<li class="fragment">It turns out that the Gini index and the cross-entropy are very similar numerically.</li>
</ul>
</section>
<section id="example-heart-data" class="slide level2 center">
<h2>Example: heart data</h2>
<ul>
<li class="fragment"><p>These data contain a binary outcome <em>HD</em> for 303 patients who presented with chest pain.</p></li>
<li class="fragment"><p>An outcome value of <em>Yes</em> indicates the presence of heart disease based on an angiographic test, while <em>No</em> means no heart disease.</p></li>
<li class="fragment"><p>There are 13 predictors including <em>Age</em>, <em>Sex</em>, <em>Chol</em> (a cholesterol measurement), and other heart and lung function measurements.</p></li>
<li class="fragment"><p>Cross-validation yields a tree with six terminal nodes. See next figure.</p></li>
</ul>
</section>
<section id="section" class="slide level2 center">
<h2></h2>

<img data-src="figs/8_6-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="trees-versus-linear-models" class="slide level2 center">
<h2>Trees Versus Linear Models</h2>

<img data-src="figs/8_7-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p><strong>Top Row:</strong> True linear boundary; Bottom row: true non-linear boundary.<br>
<strong>Left column:</strong> Linear model; <strong>Right column:</strong> Tree-based model.</p>
</section>
<section id="advantages-and-disadvantages-of-trees" class="slide level2 center">
<h2>Advantages and Disadvantages of Trees</h2>
<p>XXX Check original slide for arrows XXX</p>
<ul>
<li class="fragment"><p>Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!</p></li>
<li class="fragment"><p>Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.</p></li>
<li class="fragment"><p>Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).</p></li>
<li class="fragment"><p>Trees can easily handle qualitative predictors without the need to create dummy variables.</p></li>
<li class="fragment"><p>Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.</p></li>
</ul>
<p>However, by aggregating many decision trees, the predictive performance of trees can be substantially improved. We introduce these concepts next.</p>
</section>
<section id="bagging" class="slide level2 center">
<h2>Bagging</h2>
<ul>
<li class="fragment"><p><em>Bootstrap aggregation</em>, or <em>bagging</em>, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.</p></li>
<li class="fragment"><p>Recall that given a set of <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(Z_1, \dots, Z_n\)</span>, each with variance <span class="math inline">\(\sigma^2\)</span>, the variance of the mean <span class="math inline">\(\bar{Z}\)</span> of the observations is given by <span class="math inline">\(\sigma^2 / n\)</span>.</p></li>
<li class="fragment"><p>In other words, <em>averaging a set of observations reduces variance</em>. Of course, this is not practical because we generally do not have access to multiple training sets.</p></li>
</ul>
</section>
<section id="bagging-continued" class="slide level2 center">
<h2>Baggingâ€” continued</h2>
<ul>
<li class="fragment"><p>Instead, we can bootstrap, by taking repeated samples from the (single) training data set.</p></li>
<li class="fragment"><p>In this approach, we generate <span class="math inline">\(B\)</span> different bootstrapped training data sets. We then train our method on the <span class="math inline">\(b\)</span>-th bootstrapped training set in order to get <span class="math inline">\(\hat{f}^*_b(x)\)</span>, the prediction at a point <span class="math inline">\(x\)</span>. We then average all the predictions to obtain</p></li>
</ul>
<p><span class="math display">\[
\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^*_b(x).
\]</span></p>
<p>This is called <em>bagging</em>.</p>
</section>
<section id="bagging-classification-trees" class="slide level2 center">
<h2>Bagging classification trees</h2>
<ul>
<li class="fragment"><p>The above prescription applied to regression trees.</p></li>
<li class="fragment"><p>For classification trees: for each test observation, we record the class predicted by each of the <span class="math inline">\(B\)</span> trees, and take a <em>majority vote</em>: the overall prediction is the most commonly occurring class among the <span class="math inline">\(B\)</span> predictions.</p></li>
</ul>
</section>
<section id="bagging-the-heart-data" class="slide level2 center">
<h2>Bagging the heart data</h2>

<img data-src="figs/8_8-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-previous-figure-2" class="slide level2 center">
<h2>Details of previous figure</h2>
<p>Bagging and random forest results for the Heart data.</p>
<ul>
<li class="fragment"><p>The test error (black and orange) is shown as a function of <span class="math inline">\(B\)</span>, the number of bootstrapped training sets used.</p></li>
<li class="fragment"><p>Random forests were applied with <span class="math inline">\(m = \sqrt{p}\)</span>.</p></li>
<li class="fragment"><p>The dashed line indicates the test error resulting from a single classification tree.</p></li>
<li class="fragment"><p>The green and blue traces show the OOB error, which in this case is considerably lower.</p></li>
</ul>
</section>
<section id="out-of-bag-error-estimation" class="slide level2 center">
<h2>Out-of-Bag Error Estimation</h2>
<ul>
<li class="fragment"><p>It turns out that there is a very straightforward way to estimate the test error of a bagged model.</p></li>
<li class="fragment"><p>Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.</p></li>
<li class="fragment"><p>The remaining one-third of the observations not used to fit a given bagged tree are referred to as the <em>out-of-bag</em> (OOB) observations.</p></li>
<li class="fragment"><p>We can predict the response for the <span class="math inline">\(i\)</span>th observation using each of the trees in which that observation was OOB. This will yield around <span class="math inline">\(B/3\)</span> predictions for the <span class="math inline">\(i\)</span>th observation, which we average.</p></li>
<li class="fragment"><p>This estimate is essentially the LOO cross-validation error for bagging, if <span class="math inline">\(B\)</span> is large.</p></li>
</ul>
</section>
<section id="random-forests" class="slide level2 center">
<h2>Random Forests</h2>
<ul>
<li class="fragment"><p><em>Random forests</em> provide an improvement over bagged trees by way of a small tweak that <em>decorrelates</em> the trees. This reduces the variance when we average the trees.</p></li>
<li class="fragment"><p>As in bagging, we build a number of decision trees on bootstrapped training samples.</p></li>
<li class="fragment"><p>But when building these decision trees, each time a split in a tree is considered, <em>a random selection of <span class="math inline">\(m\)</span> predictors</em> is chosen as split candidates from the full set of <span class="math inline">\(p\)</span> predictors. The split is allowed to use only one of those <span class="math inline">\(m\)</span> predictors.</p></li>
<li class="fragment"><p>A fresh selection of <span class="math inline">\(m\)</span> predictors is taken at each split, and typically we choose <span class="math inline">\(m \approx \sqrt{p}\)</span> â€” that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data).</p></li>
</ul>
</section>
<section id="example-gene-expression-data" class="slide level2 center">
<h2>Example: Gene Expression Data</h2>
<ul>
<li class="fragment"><p>We applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients.</p></li>
<li class="fragment"><p>There are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions.</p></li>
<li class="fragment"><p>Each of the patient samples has a qualitative label with 15 different levels: either normal or one of 14 different types of cancer.</p></li>
<li class="fragment"><p>We use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.</p></li>
<li class="fragment"><p>We randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables <span class="math inline">\(m\)</span>.</p></li>
</ul>
</section>
<section id="results-gene-expression-data" class="slide level2 center">
<h2>Results: Gene Expression Data</h2>

<img data-src="figs/8_10-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-previous-figure-3" class="slide level2 center">
<h2>Details of Previous Figure</h2>
<ul>
<li class="fragment"><p>Results from random forests for the fifteen-class gene expression data set with <span class="math inline">\(p = 500\)</span> predictors.</p></li>
<li class="fragment"><p>The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of <span class="math inline">\(m\)</span>, the number of predictors available for splitting at each interior tree node.</p></li>
<li class="fragment"><p>Random forests (<span class="math inline">\(m &lt; p\)</span>) lead to a slight improvement over bagging (<span class="math inline">\(m = p\)</span>). A single classification tree has an error rate of 45.7%.</p></li>
</ul>
</section>
<section id="boosting" class="slide level2 center">
<h2>Boosting</h2>
<ul>
<li class="fragment"><p>Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. We only discuss boosting for decision trees.</p></li>
<li class="fragment"><p>Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.</p></li>
<li class="fragment"><p>Notably, each tree is built on a bootstrap data set, independent of the other trees.</p></li>
<li class="fragment"><p>Boosting works in a similar way, except that the trees are grown <em>sequentially</em>: each tree is grown using information from previously grown trees.</p></li>
</ul>
</section>
<section id="boosting-algorithm-for-regression-trees" class="slide level2 center">
<h2>Boosting Algorithm for Regression Trees</h2>
<ol type="1">
<li class="fragment"><p>Set <span class="math inline">\(\hat{f}(x) = 0\)</span> and <span class="math inline">\(r_i = y_i\)</span> for all <span class="math inline">\(i\)</span> in the training set.</p></li>
<li class="fragment"><p>For <span class="math inline">\(b = 1, 2, \dots, B\)</span>, repeat:</p>
<p>2.1 Fit a tree <span class="math inline">\(\hat{f}^b\)</span> with <span class="math inline">\(d\)</span> splits (<span class="math inline">\(d + 1\)</span> terminal nodes) to the training data <span class="math inline">\((X, r)\)</span>.</p>
<p>2.2 Update <span class="math inline">\(\hat{f}\)</span> by adding in a shrunken version of the new tree:</p></li>
</ol>
<p><span class="math display">\[
   \hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x).
\]</span></p>
<pre><code> 2.3 Update the residuals,</code></pre>
<p><span class="math display">\[
   r_i \leftarrow r_i - \lambda \hat{f}^b(x_i).
\]</span></p>
<ol start="3" type="1">
<li class="fragment">Output the boosted model,</li>
</ol>
<p><span class="math display">\[
\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x).
\]</span></p>
</section>
<section id="what-is-the-idea-behind-this-procedure" class="slide level2 center">
<h2>What is the idea behind this procedure?</h2>
<ul>
<li class="fragment"><p>Unlike fitting a single large decision tree to the data, which amounts to <em>fitting the data hard</em> and potentially overfitting, the boosting approach instead <em>learns slowly</em>.</p></li>
<li class="fragment"><p>Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.</p></li>
<li class="fragment"><p>Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter <span class="math inline">\(d\)</span> in the algorithm.</p></li>
<li class="fragment"><p>By fitting small trees to the residuals, we slowly improve <span class="math inline">\(\hat{f}\)</span> in areas where it does not perform well. The shrinkage parameter <span class="math inline">\(\lambda\)</span> slows the process down even further, allowing more and different shaped trees to attack the residuals.</p></li>
</ul>
</section>
<section id="boosting-for-classification" class="slide level2 center">
<h2>Boosting for classification</h2>
<ul>
<li class="fragment"><p>Boosting for classification is similar in spirit to boosting for regression, but is a bit more complex. We will not go into detail here, nor do we in the text book.</p></li>
<li class="fragment"><p>Students can learn about the details in <em>Elements of Statistical Learning</em>, chapter 10.</p></li>
<li class="fragment"><p>The R package <code>gbm</code> (gradient boosted models) handles a variety of regression and classification problems.</p></li>
</ul>
</section>
<section id="gene-expression-data-continued" class="slide level2 center">
<h2>Gene expression data continued</h2>

<img data-src="figs/8_11-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"></section>
<section id="details-of-previous-figure-4" class="slide level2 center">
<h2>Details of previous figure</h2>
<ul>
<li class="fragment"><p>Results from performing boosting and random forests on the fifteen-class gene expression data set in order to predict <em>cancer</em> versus <em>normal</em>.</p></li>
<li class="fragment"><p>The test error is displayed as a function of the number of trees. For the two boosted models, <span class="math inline">\(\lambda = 0.01\)</span>. Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.</p></li>
<li class="fragment"><p>The test error rate for a single tree is 24%.</p></li>
</ul>
</section>
<section id="tuning-parameters-for-boosting" class="slide level2 center">
<h2>Tuning Parameters for Boosting</h2>
<ol type="1">
<li class="fragment"><p><strong>The number of trees <span class="math inline">\(B\)</span>.</strong> Unlike bagging and random forests, boosting can overfit if <span class="math inline">\(B\)</span> is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select <span class="math inline">\(B\)</span>.</p></li>
<li class="fragment"><p><strong>The shrinkage parameter <span class="math inline">\(\lambda\)</span>.</strong> A small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small <span class="math inline">\(\lambda\)</span> can require using a very large value of <span class="math inline">\(B\)</span> in order to achieve good performance.</p></li>
<li class="fragment"><p><strong>The number of splits <span class="math inline">\(d\)</span> in each tree,</strong> which controls the complexity of the boosted ensemble. Often <span class="math inline">\(d = 1\)</span> works well, in which case each tree is a <em>stump</em>, consisting of a single split and resulting in an additive model. More generally <span class="math inline">\(d\)</span> is the <em>interaction depth</em>, and controls the interaction order of the boosted model, since <span class="math inline">\(d\)</span> splits can involve at most <span class="math inline">\(d\)</span> variables.</p></li>
</ol>
</section>
<section id="another-regression-example" class="slide level2 center">
<h2>Another Regression Example</h2>

<img data-src="figs/8_1_2-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p><em>From Elements of Statistical Learning, chapter 15.</em></p>
</section>
<section id="another-classification-example" class="slide level2 center">
<h2>Another Classification Example</h2>

<img data-src="figs/8_1_3-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p><em>From Elements of Statistical Learning, chapter 15.</em></p>
</section>
<section id="variable-importance-measure" class="slide level2 center">
<h2>Variable Importance Measure</h2>
<ul>
<li class="fragment"><p><strong>For bagged/RF regression trees</strong>:</p>
<ul>
<li class="fragment"><p>Record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all <span class="math inline">\(B\)</span> trees.</p></li>
<li class="fragment"><p>A large value indicates an important predictor.</p></li>
</ul></li>
<li class="fragment"><p><strong>For bagged/RF classification trees</strong>:</p>
<ul>
<li class="fragment">Add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all <span class="math inline">\(B\)</span> trees.</li>
</ul></li>
</ul>

<img data-src="figs/8_9-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p>Variable Importance Plot for the Heart Data</p>
</section>
<section id="summary" class="slide level2 center">
<h2>Summary</h2>
<ul>
<li class="fragment"><p><strong>Decision trees</strong> are simple and interpretable models for regression and classification.</p></li>
<li class="fragment"><p>However, they are often not competitive with other methods in terms of prediction accuracy.</p></li>
<li class="fragment"><p><strong>Bagging</strong>, <strong>random forests</strong>, and <strong>boosting</strong> are effective methods for improving the prediction accuracy of trees:</p>
<ul>
<li class="fragment">They work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees.</li>
</ul></li>
<li class="fragment"><p>Random forests and boosting are among the <strong>state-of-the-art methods for supervised learning</strong>, though their results can be difficult to interpret.</p></li>
</ul>
</section>
<section id="summary-1" class="slide level2 center">
<h2>Summary</h2>
<div>
<div class="columns">
<div class="column" style="width:50%;">
<div style="font-size: 80%;">
<ul>
<li>XXXX</li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div style="font-size: 80%;">
<ul>
<li>XXXX</li>
</ul>
</div>
</div></div>
</div>
</section></section>
<section id="thank-you" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Thank you!</h1>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">
<p>Predictive Analytics</p>
</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>